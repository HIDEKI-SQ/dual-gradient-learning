{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment E3: Reparameterization Invariance Test\n",
    "\n",
    "**One-step update consistency across parameter coordinate rescaling**\n",
    "\n",
    "**Implementation**: Based on validated 1-step reference implementation\n",
    "\n",
    "---\n",
    "\n",
    "## Generated Files\n",
    "\n",
    "- `E3_results.csv`\n",
    "- `E3_summary.json`\n",
    "- `E3_figure.png` (publication quality)\n",
    "- `E3_figure.pdf` (publication quality)\n",
    "\n",
    "**Runtime**: ~5 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "SAVE_DIR = '/content/drive/MyDrive/paper-E-final/E3'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print(f'Results → {SAVE_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class SoftmaxRegression(nn.Module):\n",
    "    def __init__(self, input_dim, n_classes, bias=False, dtype=torch.float64, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, n_classes, bias=bias, dtype=dtype, device=device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils: flatten / unflatten\n",
    "def get_param_vector(model: nn.Module) -> torch.Tensor:\n",
    "    return torch.cat([p.detach().reshape(-1) for p in model.parameters()])\n",
    "\n",
    "def set_param_vector_(model: nn.Module, vec: torch.Tensor) -> None:\n",
    "    # IMPORTANT: overwrite params deterministically\n",
    "    idx = 0\n",
    "    with torch.no_grad():\n",
    "        for p in model.parameters():\n",
    "            n = p.numel()\n",
    "            p.copy_(vec[idx:idx+n].view_as(p))\n",
    "            idx += n\n",
    "    assert idx == vec.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Fisher (analytic) for softmax regression (bias=False)\n",
    "def compute_true_fisher_softmax_regression(model: SoftmaxRegression, X: torch.Tensor) -> torch.Tensor:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(X)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "\n",
    "    B, C = probs.shape\n",
    "    D = X.shape[1]\n",
    "\n",
    "    # [B, C, C] : Diag(p) - p p^T\n",
    "    diag_p = torch.diag_embed(probs)\n",
    "    outer_p = torch.einsum(\"bc,bd->bcd\", probs, probs)\n",
    "    fisher_class = diag_p - outer_p\n",
    "\n",
    "    # [B, D, D] : x x^T\n",
    "    outer_x = torch.einsum(\"bi,bj->bij\", X, X)\n",
    "\n",
    "    # [B, C, D, C, D] : kron\n",
    "    fisher = torch.einsum(\"bik,bjl->bijkl\", fisher_class, outer_x)\n",
    "    fisher = fisher.reshape(B, C*D, C*D)\n",
    "\n",
    "    # Average over batch\n",
    "    F_true = fisher.mean(dim=0)\n",
    "    # Symmetrize\n",
    "    F_true = 0.5 * (F_true + F_true.T)\n",
    "    return F_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gate A: Fisher PSD quality\n",
    "def gate_a_fisher_psd(F: torch.Tensor, name=\"Fisher\"):\n",
    "    Fsym = 0.5 * (F + F.T)\n",
    "    evals = torch.linalg.eigvalsh(Fsym)\n",
    "    min_eig = evals.min().item()\n",
    "    max_eig = evals.max().item()\n",
    "    thresh = -1e-8 * abs(max_eig)\n",
    "    passed = (min_eig >= thresh)\n",
    "    msg = f\"{name}: min_eig={min_eig:.3e}, max_eig={max_eig:.3e}, thresh={thresh:.3e}\"\n",
    "    return passed, msg, min_eig, max_eig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gate C: all κ produce identical logits at init\n",
    "def gate_c_equivalence(models_dict, X, kappa_values, threshold=1e-5):\n",
    "    base = kappa_values[0]\n",
    "    with torch.no_grad():\n",
    "        logits0 = models_dict[base](X)\n",
    "    max_diff = 0.0\n",
    "    fails = []\n",
    "    for k in kappa_values[1:]:\n",
    "        with torch.no_grad():\n",
    "            logitsk = models_dict[k](X)\n",
    "        diff = (logitsk - logits0).abs().max().item()\n",
    "        max_diff = max(max_diff, diff)\n",
    "        if diff > threshold:\n",
    "            fails.append((k, diff))\n",
    "    if len(fails) == 0:\n",
    "        return True, f\"GateC PASS: max|Δlogits|={max_diff:.2e}\", max_diff\n",
    "    else:\n",
    "        return False, \"GateC FAIL: \" + \", \".join([f\"κ={k}: {d:.2e}\" for k,d in fails]), max_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symmetric pseudo-inverse (coordinate-invariant)\n",
    "def pinv_psd(F: torch.Tensor, rcond=1e-8):\n",
    "    Fsym = 0.5 * (F + F.T)\n",
    "    evals, evecs = torch.linalg.eigh(Fsym)\n",
    "    max_eig = evals.max()\n",
    "    cut = rcond * max_eig\n",
    "    inv = torch.zeros_like(evals)\n",
    "    mask = evals > cut\n",
    "    inv[mask] = 1.0 / evals[mask]\n",
    "    return (evecs * inv.unsqueeze(0)) @ evecs.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build parameter scaling vector S(κ)\n",
    "def make_S_vec(kappa: float, n_classes: int, n_features: int, device, dtype):\n",
    "    s_hi = math.sqrt(kappa)\n",
    "    s_lo = 1.0 / math.sqrt(kappa)\n",
    "    s_feat = torch.ones(n_features, device=device, dtype=dtype)\n",
    "    half = n_features // 2\n",
    "    s_feat[:half] = s_hi\n",
    "    s_feat[half:] = s_lo\n",
    "    # Repeat per class\n",
    "    S = s_feat.repeat(n_classes)\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core: compute Δθ in θ-space via φ-coordinate update\n",
    "def one_step_update_in_theta_space(g_theta, F_theta, S_vec, method, eta, rcond=1e-8):\n",
    "    # Transform grad and Fisher into φ coords\n",
    "    g_phi = S_vec * g_theta\n",
    "\n",
    "    # CRITICAL: F_φ = diag(S) @ F_θ @ diag(S)\n",
    "    F_phi = (S_vec[:, None] * F_theta) * S_vec[None, :]\n",
    "    F_phi = 0.5 * (F_phi + F_phi.T)\n",
    "\n",
    "    if method == \"sgd\":\n",
    "        delta_phi = -eta * g_phi\n",
    "        delta_theta = S_vec * delta_phi\n",
    "        return delta_theta, F_phi\n",
    "\n",
    "    elif method == \"natgrad\":\n",
    "        Fphi_pinv = pinv_psd(F_phi, rcond=rcond)\n",
    "        v_phi = Fphi_pinv @ g_phi\n",
    "        delta_phi = -eta * v_phi\n",
    "        delta_theta = S_vec * delta_phi\n",
    "        return delta_theta, F_phi\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'sgd' or 'natgrad'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main E3: 1-step invariance test\n",
    "def run_e3_one_step(X, y, seed=0, kappa_values=(1,10,100,1000), eta=1e-3, rcond=1e-8, device=\"cpu\"):\n",
    "    torch.manual_seed(seed)\n",
    "    dtype = torch.float64\n",
    "\n",
    "    B, D = X.shape\n",
    "    C = int(y.max().item()) + 1\n",
    "\n",
    "    # Build base model and θ0 (FIXED for all κ)\n",
    "    model = SoftmaxRegression(D, C, bias=False, dtype=dtype, device=device)\n",
    "    model.train()\n",
    "\n",
    "    theta0 = get_param_vector(model)\n",
    "\n",
    "    # Compute gθ and Fθ at θ0\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    logits = model(X)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    loss.backward()\n",
    "    g_theta = torch.cat([p.grad.detach().reshape(-1) for p in model.parameters()])\n",
    "\n",
    "    F_theta = compute_true_fisher_softmax_regression(model, X)\n",
    "\n",
    "    # Gate A on F_theta\n",
    "    passA, msgA, _, _ = gate_a_fisher_psd(F_theta, \"F_theta\")\n",
    "    if not passA:\n",
    "        raise RuntimeError(\"Gate A failed: \" + msgA)\n",
    "\n",
    "    # Gate C: all κ have identical logits at init\n",
    "    models = {}\n",
    "    for k in kappa_values:\n",
    "        m = SoftmaxRegression(D, C, bias=False, dtype=dtype, device=device)\n",
    "        set_param_vector_(m, theta0)\n",
    "        models[k] = m\n",
    "    passC, msgC, _ = gate_c_equivalence(models, X, list(kappa_values), threshold=1e-5)\n",
    "    if not passC:\n",
    "        raise RuntimeError(\"Gate C failed: \" + msgC)\n",
    "\n",
    "    # Compute Δθ for each method and κ\n",
    "    results = {}\n",
    "    for method in [\"sgd\", \"natgrad\"]:\n",
    "        deltas = {}\n",
    "        for k in kappa_values:\n",
    "            S_vec = make_S_vec(k, C, D, device=device, dtype=dtype)\n",
    "            delta_theta_k, F_phi_k = one_step_update_in_theta_space(\n",
    "                g_theta, F_theta, S_vec, method=method, eta=eta, rcond=rcond\n",
    "            )\n",
    "\n",
    "            # Gate A on F_phi_k\n",
    "            passA2, msgA2, _, _ = gate_a_fisher_psd(F_phi_k, f\"F_phi(κ={k})\")\n",
    "            if not passA2:\n",
    "                raise RuntimeError(\"Gate A failed on transformed Fisher: \" + msgA2)\n",
    "\n",
    "            deltas[k] = delta_theta_k\n",
    "\n",
    "        # Consistency vs κ=1\n",
    "        ref = deltas[kappa_values[0]]\n",
    "        cons = {}\n",
    "        for k in kappa_values[1:]:\n",
    "            cons[k] = (deltas[k] - ref).abs().max().item()\n",
    "        results[method] = cons\n",
    "\n",
    "    return {\n",
    "        \"seed\": seed,\n",
    "        \"eta\": eta,\n",
    "        \"rcond\": rcond,\n",
    "        \"gateA_msg\": msgA,\n",
    "        \"gateC_msg\": msgC,\n",
    "        \"consistency_natgrad\": results[\"natgrad\"],\n",
    "        \"consistency_sgd\": results[\"sgd\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "def generate_data(n_samples=5000, n_features=16, n_classes=10, noise_std=1.0, seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    centers = torch.randn(n_classes, n_features, dtype=torch.float64)\n",
    "    Q, _ = torch.linalg.qr(centers.T)\n",
    "    centers = (Q[:, :n_classes].T * 2.5).to(device)\n",
    "    samples_per_class = n_samples // n_classes\n",
    "    X_list, y_list = [], []\n",
    "    for c in range(n_classes):\n",
    "        X_class = centers[c] + torch.randn(samples_per_class, n_features, dtype=torch.float64, device=device) * noise_std\n",
    "        y_class = torch.full((samples_per_class,), c, dtype=torch.long, device=device)\n",
    "        X_list.append(X_class)\n",
    "        y_list.append(y_class)\n",
    "    X = torch.cat(X_list)\n",
    "    y = torch.cat(y_list)\n",
    "    perm = torch.randperm(X.shape[0], device=device)\n",
    "    return X[perm], y[perm]\n",
    "\n",
    "X_train, y_train = generate_data(seed=42)\n",
    "print(f'Data: {X_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments\n",
    "CONFIG = {\n",
    "    'kappa_values': [1, 10, 100, 1000],\n",
    "    'n_seeds': 3,\n",
    "    'eta': 0.001,\n",
    "    'rcond': 1e-8\n",
    "}\n",
    "\n",
    "print('Running E3 invariance tests...')\n",
    "print(f\"Total seeds: {CONFIG['n_seeds']}\\n\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for seed in range(CONFIG['n_seeds']):\n",
    "    print(f\"Seed {seed}:\")\n",
    "    result = run_e3_one_step(\n",
    "        X_train, y_train, \n",
    "        seed=seed, \n",
    "        kappa_values=tuple(CONFIG['kappa_values']),\n",
    "        eta=CONFIG['eta'],\n",
    "        rcond=CONFIG['rcond'],\n",
    "        device=device\n",
    "    )\n",
    "    print(f\"  Gate A: {result['gateA_msg']}\")\n",
    "    print(f\"  Gate C: {result['gateC_msg']}\")\n",
    "    print(f\"  NG κ=1000: {result['consistency_natgrad'][1000]:.2e}\")\n",
    "    print(f\"  SGD κ=1000: {result['consistency_sgd'][1000]:.2e}\")\n",
    "    \n",
    "    for k in CONFIG['kappa_values'][1:]:\n",
    "        all_results.append({\n",
    "            'seed': seed,\n",
    "            'kappa': k,\n",
    "            'method': 'natgrad',\n",
    "            'max_diff': result['consistency_natgrad'][k]\n",
    "        })\n",
    "        all_results.append({\n",
    "            'seed': seed,\n",
    "            'kappa': k,\n",
    "            'method': 'sgd',\n",
    "            'max_diff': result['consistency_sgd'][k]\n",
    "        })\n",
    "\n",
    "print('\\n✓ Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_results)\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('RESULTS SUMMARY')\n",
    "print('='*70)\n",
    "summary_table = df.groupby(['method', 'kappa'])['max_diff'].agg(['mean', 'std'])\n",
    "print(summary_table)\n",
    "\n",
    "ng_1000 = df[(df['method']=='natgrad') & (df['kappa']==1000)]['max_diff'].mean()\n",
    "sgd_1000 = df[(df['method']=='sgd') & (df['kappa']==1000)]['max_diff'].mean()\n",
    "\n",
    "print(f'\\nAt κ=1000:')\n",
    "print(f'  Natural Gradient: {ng_1000:.2e}')\n",
    "print(f'  SGD: {sgd_1000:.2e}')\n",
    "print(f'  Ratio (SGD/NG): {sgd_1000/ng_1000:.1f}×')\n",
    "\n",
    "if ng_1000 < sgd_1000:\n",
    "    print('\\n✓✓✓ SUCCESS: NG is more invariant than SGD')\n",
    "else:\n",
    "    print('\\n✗ WARNING: Unexpected result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publication-quality figure (no title)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4.5))\n",
    "\n",
    "for method in ['sgd', 'natgrad']:\n",
    "    method_data = df[df['method'] == method]\n",
    "    grouped = method_data.groupby('kappa')['max_diff'].agg(['mean', 'std'])\n",
    "    \n",
    "    label = 'Natural Gradient' if method == 'natgrad' else 'SGD'\n",
    "    color = 'C1' if method == 'natgrad' else 'C0'\n",
    "    \n",
    "    ax1.plot(grouped.index, grouped['mean'], marker='o', label=label, \n",
    "             linewidth=2, markersize=8, color=color)\n",
    "    ax1.fill_between(grouped.index, grouped['mean'] - grouped['std'], \n",
    "                      grouped['mean'] + grouped['std'], alpha=0.2, color=color)\n",
    "\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_yscale('log')\n",
    "ax1.axhline(y=1e-5, color='gray', linestyle='--', alpha=0.5, label='Threshold')\n",
    "ax1.set_xlabel('Condition Number κ', fontsize=11)\n",
    "ax1.set_ylabel('max |Δθ(κ) - Δθ(1)|', fontsize=11)\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3, which='both')\n",
    "\n",
    "# Cosine similarity (compute from stored deltas if needed, or use normalized difference)\n",
    "# For simplicity, show relative consistency\n",
    "for method in ['sgd', 'natgrad']:\n",
    "    method_data = df[df['method'] == method]\n",
    "    grouped = method_data.groupby('kappa')['max_diff'].agg(['mean', 'std'])\n",
    "    \n",
    "    # Normalize to [0,1] scale for visualization\n",
    "    normalized = 1.0 - np.minimum(grouped['mean'] / grouped['mean'].max(), 1.0)\n",
    "    \n",
    "    label = 'Natural Gradient' if method == 'natgrad' else 'SGD'\n",
    "    color = 'C1' if method == 'natgrad' else 'C0'\n",
    "    \n",
    "    ax2.plot(grouped.index, normalized, marker='o', label=label,\n",
    "             linewidth=2, markersize=8, color=color)\n",
    "\n",
    "ax2.set_xscale('log')\n",
    "ax2.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax2.set_xlabel('Condition Number κ', fontsize=11)\n",
    "ax2.set_ylabel('Relative Consistency', fontsize=11)\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR}/E3_figure.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig(f'{SAVE_DIR}/E3_figure.pdf', bbox_inches='tight')\n",
    "print('Figure saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'{SAVE_DIR}/E3_results.csv', index=False)\n",
    "\n",
    "ng_data = df[df['method'] == 'natgrad']\n",
    "sgd_data = df[df['method'] == 'sgd']\n",
    "\n",
    "summary = {\n",
    "    'experiment': 'E3',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'config': CONFIG,\n",
    "    'ng_consistency': {\n",
    "        'kappa_10': float(ng_data[ng_data['kappa']==10]['max_diff'].mean()),\n",
    "        'kappa_100': float(ng_data[ng_data['kappa']==100]['max_diff'].mean()),\n",
    "        'kappa_1000': float(ng_data[ng_data['kappa']==1000]['max_diff'].mean())\n",
    "    },\n",
    "    'sgd_degradation': {\n",
    "        'kappa_10': float(sgd_data[sgd_data['kappa']==10]['max_diff'].mean()),\n",
    "        'kappa_100': float(sgd_data[sgd_data['kappa']==100]['max_diff'].mean()),\n",
    "        'kappa_1000': float(sgd_data[sgd_data['kappa']==1000]['max_diff'].mean())\n",
    "    },\n",
    "    'ratio_at_1000': float(\n",
    "        sgd_data[sgd_data['kappa']==1000]['max_diff'].mean() / \n",
    "        ng_data[ng_data['kappa']==1000]['max_diff'].mean()\n",
    "    )\n",
    "}\n",
    "\n",
    "with open(f'{SAVE_DIR}/E3_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f'\\n✓ All files saved to {SAVE_DIR}')\n",
    "print('  - E3_results.csv')\n",
    "print('  - E3_summary.json')\n",
    "print('  - E3_figure.png')\n",
    "print('  - E3_figure.pdf')\n",
    "print(f'\\nSGD/NG invariance ratio at κ=1000: {summary[\"ratio_at_1000\"]:.1f}×')\n",
    "print('(Higher = NG more invariant)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
