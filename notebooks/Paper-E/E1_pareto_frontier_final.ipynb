{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment E1: Pareto Frontier\n",
    "\n",
    "**Natural Gradient local efficiency at matched per-step KL divergence**\n",
    "\n",
    "---\n",
    "\n",
    "## Generated Files\n",
    "\n",
    "This notebook generates:\n",
    "- `E1_results_raw.csv`\n",
    "- `E1_results_aggregated.csv`\n",
    "- `E1_gate_validation.csv`\n",
    "- `E1_summary.json`\n",
    "- `E1_figure.png` (publication quality)\n",
    "- `E1_figure.pdf` (publication quality)\n",
    "\n",
    "**Runtime**: ~5-10 minutes on GPU\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "SAVE_DIR = '/content/drive/MyDrive/paper-E-final/E1'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print(f'Results → {SAVE_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gate_a_fisher_quality(fisher, name=\"Fisher\", threshold=-1e-7):\n",
    "    fisher_sym = (fisher + fisher.T) / 2\n",
    "    eigenvalues = torch.linalg.eigvalsh(fisher_sym)\n",
    "    min_eig = eigenvalues.min().item()\n",
    "    max_eig = eigenvalues.max().item()\n",
    "    threshold_value = threshold * abs(max_eig)\n",
    "    \n",
    "    details = {\n",
    "        \"min_eigenvalue\": min_eig,\n",
    "        \"max_eigenvalue\": max_eig,\n",
    "        \"condition_number\": max_eig / min_eig if min_eig > 0 else float('inf')\n",
    "    }\n",
    "    \n",
    "    if min_eig < threshold_value:\n",
    "        return False, f\"FAIL: {name} negative eigenvalue {min_eig:.2e}\", details\n",
    "    return True, f\"PASS: {name} PSD (min={min_eig:.2e})\", details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, use_bias=False):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, num_classes, bias=use_bias)\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fisher_analytic(model, X):\n",
    "    with torch.no_grad():\n",
    "        logits = model(X)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        N, C = probs.shape\n",
    "        D = X.shape[1]\n",
    "        fisher = torch.zeros(C * D, C * D, dtype=X.dtype, device=X.device)\n",
    "        for i in range(N):\n",
    "            p = probs[i]\n",
    "            x = X[i]\n",
    "            H = torch.diag(p) - torch.outer(p, p)\n",
    "            F_sample = torch.kron(H, torch.outer(x, x))\n",
    "            fisher += F_sample\n",
    "        fisher /= N\n",
    "        fisher = (fisher + fisher.T) / 2\n",
    "        return fisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_samples=5000, n_features=16, n_classes=10, noise_std=1.0, seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    centers = torch.randn(n_classes, n_features, dtype=torch.float64)\n",
    "    Q, _ = torch.linalg.qr(centers.T)\n",
    "    centers = (Q[:, :n_classes].T * 2.5).to(device)\n",
    "    samples_per_class = n_samples // n_classes\n",
    "    X_list, y_list = [], []\n",
    "    for c in range(n_classes):\n",
    "        X_class = centers[c] + torch.randn(samples_per_class, n_features, dtype=torch.float64, device=device) * noise_std\n",
    "        y_class = torch.full((samples_per_class,), c, dtype=torch.long, device=device)\n",
    "        X_list.append(X_class)\n",
    "        y_list.append(y_class)\n",
    "    X = torch.cat(X_list)\n",
    "    y = torch.cat(y_list)\n",
    "    perm = torch.randperm(X.shape[0], device=device)\n",
    "    return X[perm], y[perm]\n",
    "\n",
    "X_train, y_train = generate_data(n_samples=5000, seed=42)\n",
    "print(f'Data: X={X_train.shape}, y={y_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters_flat(model):\n",
    "    return torch.cat([p.flatten() for p in model.parameters()])\n",
    "\n",
    "def set_parameters_flat(model, params):\n",
    "    offset = 0\n",
    "    for p in model.parameters():\n",
    "        numel = p.numel()\n",
    "        p.data.copy_(params[offset:offset+numel].view_as(p))\n",
    "        offset += numel\n",
    "\n",
    "def compute_loss_and_grad(model, X, y):\n",
    "    model.zero_grad()\n",
    "    logits = model(X)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    loss.backward()\n",
    "    grad = torch.cat([p.grad.flatten() for p in model.parameters()])\n",
    "    return loss.item(), grad\n",
    "\n",
    "def sgd_step(model, X, y, target_kl_step, fisher):\n",
    "    loss_before, grad = compute_loss_and_grad(model, X, y)\n",
    "    grad_F_grad = torch.dot(grad, fisher @ grad)\n",
    "    eta = torch.sqrt(2 * target_kl_step / grad_F_grad)\n",
    "    theta_before = get_parameters_flat(model)\n",
    "    theta_after = theta_before - eta * grad\n",
    "    set_parameters_flat(model, theta_after)\n",
    "    with torch.no_grad():\n",
    "        logits_before = model.forward(X)\n",
    "    set_parameters_flat(model, theta_after)\n",
    "    with torch.no_grad():\n",
    "        logits_after = model.forward(X)\n",
    "        probs_before = F.softmax(logits_before, dim=1)\n",
    "        probs_after = F.softmax(logits_after, dim=1)\n",
    "        kl_actual = -(probs_before * torch.log(probs_after / (probs_before + 1e-10) + 1e-10)).sum(1).mean().item()\n",
    "    loss_after = F.cross_entropy(logits_after, y).item()\n",
    "    delta_loss = loss_before - loss_after\n",
    "    return {\n",
    "        'loss_before': loss_before,\n",
    "        'loss_after': loss_after,\n",
    "        'delta_loss': delta_loss,\n",
    "        'kl_step': kl_actual,\n",
    "        'kl_ratio': kl_actual / target_kl_step,\n",
    "        'eta': eta.item(),\n",
    "        'direction': grad / torch.norm(grad)\n",
    "    }\n",
    "\n",
    "def natgrad_step(model, X, y, target_kl_step, fisher):\n",
    "    loss_before, grad = compute_loss_and_grad(model, X, y)\n",
    "    eigvals, eigvecs = torch.linalg.eigh(fisher)\n",
    "    eigvals_inv = torch.where(eigvals > 1e-8, 1.0 / eigvals, torch.zeros_like(eigvals))\n",
    "    fisher_inv = eigvecs @ torch.diag(eigvals_inv) @ eigvecs.T\n",
    "    natgrad = fisher_inv @ grad\n",
    "    natgrad_F_natgrad = torch.dot(natgrad, fisher @ natgrad)\n",
    "    eta = torch.sqrt(2 * target_kl_step / natgrad_F_natgrad)\n",
    "    theta_before = get_parameters_flat(model)\n",
    "    theta_after = theta_before - eta * natgrad\n",
    "    set_parameters_flat(model, theta_after)\n",
    "    with torch.no_grad():\n",
    "        logits_before = model.forward(X)\n",
    "    set_parameters_flat(model, theta_after)\n",
    "    with torch.no_grad():\n",
    "        logits_after = model.forward(X)\n",
    "        probs_before = F.softmax(logits_before, dim=1)\n",
    "        probs_after = F.softmax(logits_after, dim=1)\n",
    "        kl_actual = -(probs_before * torch.log(probs_after / (probs_before + 1e-10) + 1e-10)).sum(1).mean().item()\n",
    "    loss_after = F.cross_entropy(logits_after, y).item()\n",
    "    delta_loss = loss_before - loss_after\n",
    "    return {\n",
    "        'loss_before': loss_before,\n",
    "        'loss_after': loss_after,\n",
    "        'delta_loss': delta_loss,\n",
    "        'kl_step': kl_actual,\n",
    "        'kl_ratio': kl_actual / target_kl_step,\n",
    "        'eta': eta.item(),\n",
    "        'direction': natgrad / torch.norm(natgrad)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'n_features': 16,\n",
    "    'n_classes': 10,\n",
    "    'use_bias': False,\n",
    "    'epsilon_values': [1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2, 1e-1],\n",
    "    'n_seeds': 10\n",
    "}\n",
    "\n",
    "print(f\"Total experiments: {len(CONFIG['epsilon_values']) * CONFIG['n_seeds'] * 2}\")\n",
    "\n",
    "results = []\n",
    "gate_results = []\n",
    "\n",
    "for seed in range(CONFIG['n_seeds']):\n",
    "    torch.manual_seed(seed)\n",
    "    model = SoftmaxRegression(CONFIG['n_features'], CONFIG['n_classes'], use_bias=CONFIG['use_bias']).to(device)\n",
    "    fisher = compute_fisher_analytic(model, X_train)\n",
    "    passed, msg, details = gate_a_fisher_quality(fisher, name=f\"Seed {seed}\")\n",
    "    if seed == 0:\n",
    "        print(f\"Gate A: {msg}\")\n",
    "    gate_results.append({'seed': seed, 'gate_a_passed': passed, **details})\n",
    "    if not passed:\n",
    "        print(f\"⚠️ WARNING: Gate A failed for seed {seed}\")\n",
    "    theta_0 = get_parameters_flat(model).clone()\n",
    "    for eps in CONFIG['epsilon_values']:\n",
    "        set_parameters_flat(model, theta_0)\n",
    "        sgd_result = sgd_step(model, X_train, y_train, eps, fisher)\n",
    "        set_parameters_flat(model, theta_0)\n",
    "        ng_result = natgrad_step(model, X_train, y_train, eps, fisher)\n",
    "        cos_sim = torch.dot(sgd_result['direction'], ng_result['direction']).item()\n",
    "        results.append({'seed': seed, 'epsilon': eps, 'method': 'sgd', **sgd_result})\n",
    "        results.append({'seed': seed, 'epsilon': eps, 'method': 'natgrad', **ng_result, 'cos_g_v': cos_sim})\n",
    "\n",
    "print(f\"✓ Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gate_df = pd.DataFrame(gate_results)\n",
    "pass_rate = gate_df['gate_a_passed'].mean()\n",
    "print(f\"\\nGate A pass rate: {pass_rate:.1%}\")\n",
    "if pass_rate == 1.0:\n",
    "    print(\"✓✓✓ GATE A: ALL PASSED\")\n",
    "else:\n",
    "    print(\"⚠️ GATE A: SOME FAILED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "agg_df = df.groupby(['epsilon', 'method']).agg({\n",
    "    'delta_loss': ['mean', 'std'],\n",
    "    'kl_step': ['mean', 'std'],\n",
    "    'kl_ratio': ['mean', 'std']\n",
    "}).reset_index()\n",
    "agg_df.columns = ['epsilon', 'method', 'delta_loss_mean', 'delta_loss_std',\n",
    "                  'kl_step_mean', 'kl_step_std', 'kl_ratio_mean', 'kl_ratio_std']\n",
    "\n",
    "sgd_df = agg_df[agg_df['method'] == 'sgd'].set_index('epsilon')\n",
    "ng_df = agg_df[agg_df['method'] == 'natgrad'].set_index('epsilon')\n",
    "relative_advantage = (ng_df['delta_loss_mean'] - sgd_df['delta_loss_mean']) / sgd_df['delta_loss_mean'] * 100\n",
    "print(f\"\\nMean NG advantage: {relative_advantage.mean():+.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publication-quality figure (no title, no panel labels)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4.5))\n",
    "\n",
    "sgd_data = agg_df[agg_df['method'] == 'sgd']\n",
    "ng_data = agg_df[agg_df['method'] == 'natgrad']\n",
    "\n",
    "ax1.errorbar(sgd_data['kl_step_mean'], sgd_data['delta_loss_mean'],\n",
    "             yerr=sgd_data['delta_loss_std'], xerr=sgd_data['kl_step_std'],\n",
    "             marker='s', label='SGD', capsize=3, linewidth=2)\n",
    "ax1.errorbar(ng_data['kl_step_mean'], ng_data['delta_loss_mean'],\n",
    "             yerr=ng_data['delta_loss_std'], xerr=ng_data['kl_step_std'],\n",
    "             marker='o', label='Natural Gradient', capsize=3, linewidth=2)\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_xlabel('Per-Step KL Divergence', fontsize=11)\n",
    "ax1.set_ylabel('Loss Reduction', fontsize=11)\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "eps_values = relative_advantage.index.to_numpy()\n",
    "adv_values = relative_advantage.values\n",
    "ax2.plot(eps_values, adv_values, marker='o', linewidth=2, markersize=7, color='C2')\n",
    "ax2.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_xlabel('Per-Step KL Divergence', fontsize=11)\n",
    "ax2.set_ylabel('Relative Advantage (%)', fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR}/E1_figure.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig(f'{SAVE_DIR}/E1_figure.pdf', bbox_inches='tight')\n",
    "print(\"Figure saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'{SAVE_DIR}/E1_results_raw.csv', index=False)\n",
    "agg_df.to_csv(f'{SAVE_DIR}/E1_results_aggregated.csv', index=False)\n",
    "gate_df.to_csv(f'{SAVE_DIR}/E1_gate_validation.csv', index=False)\n",
    "\n",
    "summary = {\n",
    "    'experiment': 'E1',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'config': CONFIG,\n",
    "    'gate_a_pass_rate': float(pass_rate),\n",
    "    'mean_relative_advantage_pct': float(relative_advantage.mean()),\n",
    "    'min_advantage_pct': float(relative_advantage.min()),\n",
    "    'max_advantage_pct': float(relative_advantage.max())\n",
    "}\n",
    "\n",
    "with open(f'{SAVE_DIR}/E1_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ All files saved to {SAVE_DIR}\")\n",
    "print(f\"  - E1_results_raw.csv\")\n",
    "print(f\"  - E1_results_aggregated.csv\")\n",
    "print(f\"  - E1_gate_validation.csv\")\n",
    "print(f\"  - E1_summary.json\")\n",
    "print(f\"  - E1_figure.png\")\n",
    "print(f\"  - E1_figure.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
