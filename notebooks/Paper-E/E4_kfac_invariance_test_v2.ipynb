{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment E4: Approximation Invariance Test\n",
    "\n",
    "**Coordinate invariance degradation in Fisher matrix approximations**\n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "Quantify how much coordinate invariance degrades when using practical Fisher approximations:\n",
    "- **Exact Fisher** (ground truth from E3)\n",
    "- **K-FAC** (Kronecker-factored approximation)\n",
    "- **Empirical Fisher** (gradient outer product)\n",
    "- **SGD** (no Fisher, baseline)\n",
    "\n",
    "## Generated Files\n",
    "\n",
    "- `E4_results.csv`\n",
    "- `E4_summary.json`\n",
    "- `E4_figure.png` (publication quality)\n",
    "- `E4_figure.pdf` (publication quality)\n",
    "\n",
    "**Runtime**: ~10 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "SAVE_DIR = '/content/drive/MyDrive/paper-E-final/E4'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print(f'Results → {SAVE_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "\n",
    "# Deterministic execution\n",
    "torch.set_default_dtype(torch.float64)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression(nn.Module):\n",
    "    def __init__(self, input_dim, n_classes, bias=False, dtype=torch.float64, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, n_classes, bias=bias, dtype=dtype, device=device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_param_vector(model: nn.Module) -> torch.Tensor:\n",
    "    return torch.cat([p.detach().reshape(-1) for p in model.parameters()])\n",
    "\n",
    "def set_param_vector_(model: nn.Module, vec: torch.Tensor) -> None:\n",
    "    idx = 0\n",
    "    with torch.no_grad():\n",
    "        for p in model.parameters():\n",
    "            n = p.numel()\n",
    "            p.copy_(vec[idx:idx+n].view_as(p))\n",
    "            idx += n\n",
    "    assert idx == vec.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher Matrix Computations\n",
    "\n",
    "Three methods:\n",
    "1. **Exact Fisher** (analytic, from E3)\n",
    "2. **K-FAC** (Kronecker-factored approximation)\n",
    "3. **Empirical Fisher** (gradient outer product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_exact_fisher(model: SoftmaxRegression, X: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Exact (true) Fisher for softmax regression.\n",
    "    F = (1/N) Σ_i [diag(p_i) - p_i p_i^T] ⊗ [x_i x_i^T]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(X)\n",
    "        probs = Fn.softmax(logits, dim=1)\n",
    "\n",
    "    B, C = probs.shape\n",
    "    D = X.shape[1]\n",
    "\n",
    "    # [B, C, C] : Diag(p) - p p^T\n",
    "    diag_p = torch.diag_embed(probs)\n",
    "    outer_p = torch.einsum(\"bc,bd->bcd\", probs, probs)\n",
    "    fisher_class = diag_p - outer_p\n",
    "\n",
    "    # [B, D, D] : x x^T\n",
    "    outer_x = torch.einsum(\"bi,bj->bij\", X, X)\n",
    "\n",
    "    # [B, C, D, C, D] : kron\n",
    "    fisher = torch.einsum(\"bik,bjl->bijkl\", fisher_class, outer_x)\n",
    "    fisher = fisher.reshape(B, C*D, C*D)\n",
    "\n",
    "    # Average over batch\n",
    "    F_exact = fisher.mean(dim=0)\n",
    "    # Symmetrize\n",
    "    F_exact = 0.5 * (F_exact + F_exact.T)\n",
    "    return F_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kfac_fisher(model: SoftmaxRegression, X: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    K-FAC approximation: F ≈ S ⊗ A\n",
    "    \n",
    "    Where:\n",
    "    - A = (1/N) Σ_i x_i x_i^T  (input covariance, D×D)\n",
    "    - S = (1/N) Σ_i [diag(p_i) - p_i p_i^T]  (output Fisher, C×C)\n",
    "    \n",
    "    K-FAC assumes independence between input and output statistics,\n",
    "    which is exact for linear layers with certain distributions.\n",
    "    \n",
    "    Reference: Martens & Grosse (2015), Grosse & Martens (2016)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(X)\n",
    "        probs = Fn.softmax(logits, dim=1)\n",
    "        \n",
    "        B, C = probs.shape\n",
    "        D = X.shape[1]\n",
    "        \n",
    "        # A: Input covariance (D × D)\n",
    "        # A = (1/N) Σ x_i x_i^T\n",
    "        A = (X.T @ X) / B\n",
    "        A = 0.5 * (A + A.T)  # Symmetrize\n",
    "        \n",
    "        # S: Output Fisher (C × C)\n",
    "        # S = (1/N) Σ [diag(p_i) - p_i p_i^T]\n",
    "        # This is the expected Fisher over the output distribution\n",
    "        diag_p = torch.diag(probs.mean(dim=0))  # E[diag(p)]\n",
    "        outer_p = (probs.T @ probs) / B         # E[p p^T]\n",
    "        S = diag_p - outer_p\n",
    "        S = 0.5 * (S + S.T)  # Symmetrize\n",
    "        \n",
    "        # F_kfac = S ⊗ A (Kronecker product)\n",
    "        # Note: torch.kron computes A ⊗ B, we need S ⊗ A for correct layout\n",
    "        F_kfac = torch.kron(S, A)\n",
    "        F_kfac = 0.5 * (F_kfac + F_kfac.T)\n",
    "        \n",
    "    return F_kfac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_empirical_fisher(model: SoftmaxRegression, X: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Empirical Fisher: F_emp = (1/N) Σ_i g_i g_i^T\n",
    "    \n",
    "    Where g_i = ∇_θ log p(y_i | x_i, θ) is the gradient of the log-likelihood\n",
    "    for the *observed* label y_i (not the model's prediction).\n",
    "    \n",
    "    This is NOT the true Fisher, which uses the model's distribution.\n",
    "    The empirical Fisher is essentially free to compute but may not\n",
    "    preserve coordinate invariance.\n",
    "    \n",
    "    Reference: Kunstner et al. (2019) \"Limitations of the Empirical Fisher\"\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    B = X.shape[0]\n",
    "    param_dim = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    # Compute per-sample gradients\n",
    "    grads = []\n",
    "    for i in range(B):\n",
    "        model.zero_grad()\n",
    "        logits = model(X[i:i+1])\n",
    "        log_prob = Fn.log_softmax(logits, dim=1)\n",
    "        loss = -log_prob[0, y[i]]  # Negative log-likelihood for sample i\n",
    "        loss.backward()\n",
    "        \n",
    "        grad_i = torch.cat([p.grad.reshape(-1) for p in model.parameters()])\n",
    "        grads.append(grad_i)\n",
    "    \n",
    "    grads = torch.stack(grads)  # [B, param_dim]\n",
    "    \n",
    "    # F_emp = (1/N) Σ g_i g_i^T\n",
    "    F_emp = (grads.T @ grads) / B\n",
    "    F_emp = 0.5 * (F_emp + F_emp.T)\n",
    "    \n",
    "    return F_emp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gate Validation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gate_a_fisher_psd(F_mat: torch.Tensor, name=\"Fisher\"):\n",
    "    \"\"\"Gate A: Verify Fisher is positive semi-definite\"\"\"\n",
    "    Fsym = 0.5 * (F_mat + F_mat.T)\n",
    "    evals = torch.linalg.eigvalsh(Fsym)\n",
    "    min_eig = evals.min().item()\n",
    "    max_eig = evals.max().item()\n",
    "    thresh = -1e-8 * abs(max_eig)\n",
    "    passed = (min_eig >= thresh)\n",
    "    msg = f\"{name}: min_eig={min_eig:.3e}, max_eig={max_eig:.3e}\"\n",
    "    return passed, msg, min_eig, max_eig\n",
    "\n",
    "def gate_c_equivalence(models_dict, X, kappa_values, threshold=1e-5):\n",
    "    \"\"\"Gate C: Verify all κ produce identical initial logits\"\"\"\n",
    "    base = kappa_values[0]\n",
    "    with torch.no_grad():\n",
    "        logits0 = models_dict[base](X)\n",
    "    max_diff = 0.0\n",
    "    fails = []\n",
    "    for k in kappa_values[1:]:\n",
    "        with torch.no_grad():\n",
    "            logitsk = models_dict[k](X)\n",
    "        diff = (logitsk - logits0).abs().max().item()\n",
    "        max_diff = max(max_diff, diff)\n",
    "        if diff > threshold:\n",
    "            fails.append((k, diff))\n",
    "    if len(fails) == 0:\n",
    "        return True, f\"GateC PASS: max|Δlogits|={max_diff:.2e}\", max_diff\n",
    "    else:\n",
    "        return False, \"GateC FAIL: \" + \", \".join([f\"κ={k}: {d:.2e}\" for k,d in fails]), max_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo-Inverse and Coordinate Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinv_psd(F_mat: torch.Tensor, rcond=1e-8):\n",
    "    \"\"\"Symmetric pseudo-inverse for PSD matrices\"\"\"\n",
    "    Fsym = 0.5 * (F_mat + F_mat.T)\n",
    "    evals, evecs = torch.linalg.eigh(Fsym)\n",
    "    max_eig = evals.max()\n",
    "    cut = rcond * max_eig\n",
    "    inv = torch.zeros_like(evals)\n",
    "    mask = evals > cut\n",
    "    inv[mask] = 1.0 / evals[mask]\n",
    "    return (evecs * inv.unsqueeze(0)) @ evecs.T\n",
    "\n",
    "def make_S_vec(kappa: float, n_classes: int, n_features: int, device, dtype):\n",
    "    \"\"\"Build diagonal scaling vector S(κ) for coordinate transform\"\"\"\n",
    "    s_hi = math.sqrt(kappa)\n",
    "    s_lo = 1.0 / math.sqrt(kappa)\n",
    "    s_feat = torch.ones(n_features, device=device, dtype=dtype)\n",
    "    half = n_features // 2\n",
    "    s_feat[:half] = s_hi\n",
    "    s_feat[half:] = s_lo\n",
    "    # Repeat per class\n",
    "    S = s_feat.repeat(n_classes)\n",
    "    return S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core: One-Step Update with Different Fisher Approximations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_update_in_theta_space(g_theta, F_theta, S_vec, method, eta, rcond=1e-8):\n",
    "    \"\"\"\n",
    "    Compute parameter update Δθ in original θ-space,\n",
    "    after transforming to φ-coordinates and back.\n",
    "    \n",
    "    For coordinate invariance, the result should be independent of S_vec\n",
    "    when using exact Fisher. Approximations may show degradation.\n",
    "    \n",
    "    Methods:\n",
    "    - 'sgd': Δθ = -η * g (no Fisher)\n",
    "    - 'exact': Δθ = -η * F⁻¹ g (exact Natural Gradient)\n",
    "    - 'kfac': Δθ = -η * F_kfac⁻¹ g (K-FAC Natural Gradient)\n",
    "    - 'empirical': Δθ = -η * F_emp⁻¹ g (Empirical Fisher Natural Gradient)\n",
    "    \"\"\"\n",
    "    # Transform gradient to φ-coordinates: g_φ = S * g_θ\n",
    "    g_phi = S_vec * g_theta\n",
    "\n",
    "    # Transform Fisher to φ-coordinates: F_φ = diag(S) @ F_θ @ diag(S)\n",
    "    F_phi = (S_vec[:, None] * F_theta) * S_vec[None, :]\n",
    "    F_phi = 0.5 * (F_phi + F_phi.T)\n",
    "\n",
    "    if method == \"sgd\":\n",
    "        # SGD in φ-space: Δφ = -η * g_φ\n",
    "        delta_phi = -eta * g_phi\n",
    "    else:\n",
    "        # Natural gradient variants: Δφ = -η * F_φ⁻¹ g_φ\n",
    "        F_pinv = pinv_psd(F_phi, rcond=rcond)\n",
    "        delta_phi = -eta * F_pinv @ g_phi\n",
    "\n",
    "    # Transform back to θ-coordinates: Δθ = S * Δφ\n",
    "    delta_theta = S_vec * delta_phi\n",
    "    \n",
    "    return delta_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Experiment Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_e4_experiment(\n",
    "    X, y, seed,\n",
    "    kappa_values=(1, 10, 100, 1000),\n",
    "    eta=0.001,\n",
    "    rcond=1e-8,\n",
    "    n_features=16,\n",
    "    n_classes=10,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Run E4 experiment for one seed.\n",
    "    \n",
    "    Compares coordinate invariance across:\n",
    "    - SGD (baseline)\n",
    "    - Exact Natural Gradient\n",
    "    - K-FAC Natural Gradient  \n",
    "    - Empirical Fisher Natural Gradient\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    dtype = torch.float64\n",
    "    \n",
    "    # Create models for each κ (same initialization, different coordinates)\n",
    "    models = {}\n",
    "    for k in kappa_values:\n",
    "        models[k] = SoftmaxRegression(n_features, n_classes, bias=False, dtype=dtype, device=device)\n",
    "    \n",
    "    # Set identical initial parameters\n",
    "    theta_init = get_param_vector(models[kappa_values[0]]).clone()\n",
    "    for k in kappa_values:\n",
    "        set_param_vector_(models[k], theta_init.clone())\n",
    "    \n",
    "    # Gate C: Verify initial equivalence\n",
    "    passedC, msgC, _ = gate_c_equivalence(models, X, kappa_values)\n",
    "    if not passedC:\n",
    "        print(f\"⚠️ {msgC}\")\n",
    "    \n",
    "    # Use κ=1 model as reference\n",
    "    model_ref = models[kappa_values[0]]\n",
    "    \n",
    "    # Compute gradient (same for all methods)\n",
    "    model_ref.zero_grad()\n",
    "    logits = model_ref(X)\n",
    "    loss = Fn.cross_entropy(logits, y)\n",
    "    loss.backward()\n",
    "    g_theta = torch.cat([p.grad.reshape(-1) for p in model_ref.parameters()]).detach()\n",
    "    \n",
    "    # Compute different Fisher matrices\n",
    "    F_exact = compute_exact_fisher(model_ref, X)\n",
    "    F_kfac = compute_kfac_fisher(model_ref, X)\n",
    "    F_emp = compute_empirical_fisher(model_ref, X, y)\n",
    "    \n",
    "    # Gate A for all Fisher matrices\n",
    "    gate_results = {}\n",
    "    for name, F_mat in [('exact', F_exact), ('kfac', F_kfac), ('empirical', F_emp)]:\n",
    "        passed, msg, min_eig, max_eig = gate_a_fisher_psd(F_mat, name=name)\n",
    "        gate_results[name] = {'passed': passed, 'msg': msg, 'min_eig': min_eig, 'max_eig': max_eig}\n",
    "    \n",
    "    # Define methods and their Fisher matrices\n",
    "    methods_config = {\n",
    "        'sgd': F_exact,      # SGD doesn't use Fisher, but we pass it for consistent API\n",
    "        'exact': F_exact,\n",
    "        'kfac': F_kfac,\n",
    "        'empirical': F_emp\n",
    "    }\n",
    "    \n",
    "    # Compute Δθ for each method and each κ\n",
    "    results = {method: {} for method in methods_config}\n",
    "    \n",
    "    for method, F_theta in methods_config.items():\n",
    "        deltas = {}\n",
    "        for k in kappa_values:\n",
    "            S_vec = make_S_vec(k, n_classes, n_features, device, dtype)\n",
    "            delta_theta = one_step_update_in_theta_space(\n",
    "                g_theta, F_theta, S_vec, \n",
    "                method=method if method != 'sgd' else 'sgd',\n",
    "                eta=eta, rcond=rcond\n",
    "            )\n",
    "            deltas[k] = delta_theta\n",
    "        \n",
    "        # Compute consistency: max|Δθ(κ) - Δθ(1)|\n",
    "        ref_delta = deltas[kappa_values[0]]\n",
    "        for k in kappa_values:\n",
    "            diff = (deltas[k] - ref_delta).abs().max().item()\n",
    "            results[method][k] = diff\n",
    "    \n",
    "    return {\n",
    "        'seed': seed,\n",
    "        'gate_c': msgC,\n",
    "        'gate_a': gate_results,\n",
    "        'consistency': results\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_samples=5000, n_features=16, n_classes=10, noise_std=1.0, seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    centers = torch.randn(n_classes, n_features, dtype=torch.float64)\n",
    "    Q, _ = torch.linalg.qr(centers.T)\n",
    "    centers = (Q[:, :n_classes].T * 2.5).to(device)\n",
    "    samples_per_class = n_samples // n_classes\n",
    "    X_list, y_list = [], []\n",
    "    for c in range(n_classes):\n",
    "        X_class = centers[c] + torch.randn(samples_per_class, n_features, dtype=torch.float64, device=device) * noise_std\n",
    "        y_class = torch.full((samples_per_class,), c, dtype=torch.long, device=device)\n",
    "        X_list.append(X_class)\n",
    "        y_list.append(y_class)\n",
    "    X = torch.cat(X_list)\n",
    "    y = torch.cat(y_list)\n",
    "    perm = torch.randperm(X.shape[0], device=device)\n",
    "    return X[perm], y[perm]\n",
    "\n",
    "X_train, y_train = generate_data(seed=42)\n",
    "print(f'Data: X={X_train.shape}, y={y_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'kappa_values': [1, 10, 100, 1000],\n",
    "    'n_seeds': 3,\n",
    "    'eta': 0.001,\n",
    "    'rcond': 1e-8,\n",
    "    'n_features': 16,\n",
    "    'n_classes': 10\n",
    "}\n",
    "\n",
    "print('='*70)\n",
    "print('E4: Approximation Invariance Test')\n",
    "print('='*70)\n",
    "print(f\"Methods: SGD, Exact NG, K-FAC NG, Empirical Fisher NG\")\n",
    "print(f\"Condition numbers: {CONFIG['kappa_values']}\")\n",
    "print(f\"Seeds: {CONFIG['n_seeds']}\")\n",
    "print('='*70 + '\\n')\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for seed in range(CONFIG['n_seeds']):\n",
    "    print(f\"Seed {seed}:\")\n",
    "    result = run_e4_experiment(\n",
    "        X_train, y_train,\n",
    "        seed=seed,\n",
    "        kappa_values=tuple(CONFIG['kappa_values']),\n",
    "        eta=CONFIG['eta'],\n",
    "        rcond=CONFIG['rcond'],\n",
    "        n_features=CONFIG['n_features'],\n",
    "        n_classes=CONFIG['n_classes'],\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    print(f\"  Gate C: {result['gate_c']}\")\n",
    "    for name, ga in result['gate_a'].items():\n",
    "        status = '✓' if ga['passed'] else '✗'\n",
    "        print(f\"  Gate A ({name}): {status} min_eig={ga['min_eig']:.2e}\")\n",
    "    \n",
    "    print(f\"  At κ=1000:\")\n",
    "    for method in ['sgd', 'exact', 'kfac', 'empirical']:\n",
    "        val = result['consistency'][method][1000]\n",
    "        print(f\"    {method:12s}: {val:.2e}\")\n",
    "    \n",
    "    # Store results\n",
    "    for method in ['sgd', 'exact', 'kfac', 'empirical']:\n",
    "        for k in CONFIG['kappa_values'][1:]:  # Skip κ=1\n",
    "            all_results.append({\n",
    "                'seed': seed,\n",
    "                'kappa': k,\n",
    "                'method': method,\n",
    "                'max_diff': result['consistency'][method][k]\n",
    "            })\n",
    "    print()\n",
    "\n",
    "print('✓ All experiments completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_results)\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('RESULTS SUMMARY')\n",
    "print('='*70)\n",
    "\n",
    "# Summary table\n",
    "summary_table = df.groupby(['method', 'kappa'])['max_diff'].agg(['mean', 'std']).reset_index()\n",
    "summary_pivot = summary_table.pivot(index='kappa', columns='method', values='mean')\n",
    "summary_pivot = summary_pivot[['sgd', 'empirical', 'kfac', 'exact']]  # Order by expected invariance\n",
    "print('\\nMean deviation by method and κ:')\n",
    "print(summary_pivot.to_string())\n",
    "\n",
    "# Ratios at κ=1000\n",
    "print('\\n' + '-'*70)\n",
    "print('Invariance ratios at κ=1000 (relative to Exact NG):')\n",
    "print('-'*70)\n",
    "\n",
    "exact_1000 = df[(df['method']=='exact') & (df['kappa']==1000)]['max_diff'].mean()\n",
    "kfac_1000 = df[(df['method']=='kfac') & (df['kappa']==1000)]['max_diff'].mean()\n",
    "emp_1000 = df[(df['method']=='empirical') & (df['kappa']==1000)]['max_diff'].mean()\n",
    "sgd_1000 = df[(df['method']=='sgd') & (df['kappa']==1000)]['max_diff'].mean()\n",
    "\n",
    "print(f'  Exact NG:      {exact_1000:.2e} (reference)')\n",
    "print(f'  K-FAC NG:      {kfac_1000:.2e} ({kfac_1000/exact_1000:.1f}× worse)')\n",
    "print(f'  Empirical NG:  {emp_1000:.2e} ({emp_1000/exact_1000:.1f}× worse)')\n",
    "print(f'  SGD:           {sgd_1000:.2e} ({sgd_1000/exact_1000:.1f}× worse)')\n",
    "\n",
    "print('\\n' + '-'*70)\n",
    "print('Invariance spectrum (best to worst):')\n",
    "print('-'*70)\n",
    "methods_sorted = sorted(\n",
    "    [('exact', exact_1000), ('kfac', kfac_1000), ('empirical', emp_1000), ('sgd', sgd_1000)],\n",
    "    key=lambda x: x[1]\n",
    ")\n",
    "for i, (method, val) in enumerate(methods_sorted, 1):\n",
    "    print(f'  {i}. {method:12s}: {val:.2e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publication-Quality Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color scheme\n",
    "colors = {\n",
    "    'exact': '#2ecc71',      # Green (best)\n",
    "    'kfac': '#f39c12',       # Orange\n",
    "    'empirical': '#e74c3c',  # Red\n",
    "    'sgd': '#3498db'         # Blue (baseline)\n",
    "}\n",
    "\n",
    "labels = {\n",
    "    'exact': 'Exact NG',\n",
    "    'kfac': 'K-FAC NG',\n",
    "    'empirical': 'Empirical Fisher NG',\n",
    "    'sgd': 'SGD'\n",
    "}\n",
    "\n",
    "markers = {\n",
    "    'exact': 'o',\n",
    "    'kfac': 's',\n",
    "    'empirical': '^',\n",
    "    'sgd': 'd'\n",
    "}\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4.5))\n",
    "\n",
    "# Left panel: Deviation vs κ\n",
    "for method in ['sgd', 'empirical', 'kfac', 'exact']:\n",
    "    method_data = df[df['method'] == method]\n",
    "    grouped = method_data.groupby('kappa')['max_diff'].agg(['mean', 'std'])\n",
    "    \n",
    "    ax1.plot(grouped.index, grouped['mean'], \n",
    "             marker=markers[method], label=labels[method],\n",
    "             linewidth=2, markersize=8, color=colors[method])\n",
    "    ax1.fill_between(grouped.index, \n",
    "                     grouped['mean'] - grouped['std'],\n",
    "                     grouped['mean'] + grouped['std'],\n",
    "                     alpha=0.2, color=colors[method])\n",
    "\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_yscale('log')\n",
    "ax1.axhline(y=1e-5, color='gray', linestyle='--', alpha=0.5, label='Threshold')\n",
    "ax1.set_xlabel('Condition Number κ', fontsize=11)\n",
    "ax1.set_ylabel('max |Δθ(κ) - Δθ(1)|', fontsize=11)\n",
    "ax1.legend(fontsize=9, loc='upper left')\n",
    "ax1.grid(True, alpha=0.3, which='both')\n",
    "ax1.set_ylim(1e-17, 1)\n",
    "\n",
    "# Right panel: Bar chart at κ=1000\n",
    "methods_order = ['exact', 'kfac', 'empirical', 'sgd']\n",
    "values_1000 = [df[(df['method']==m) & (df['kappa']==1000)]['max_diff'].mean() for m in methods_order]\n",
    "stds_1000 = [df[(df['method']==m) & (df['kappa']==1000)]['max_diff'].std() for m in methods_order]\n",
    "\n",
    "x_pos = np.arange(len(methods_order))\n",
    "bars = ax2.bar(x_pos, values_1000, yerr=stds_1000, capsize=5,\n",
    "               color=[colors[m] for m in methods_order], alpha=0.8)\n",
    "ax2.set_yscale('log')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels([labels[m] for m in methods_order], fontsize=10)\n",
    "ax2.set_ylabel('max |Δθ(κ) - Δθ(1)| at κ=1000', fontsize=11)\n",
    "ax2.axhline(y=1e-5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (v, bar) in enumerate(zip(values_1000, bars)):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, v * 2, f'{v:.1e}',\n",
    "             ha='center', va='bottom', fontsize=8, rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR}/E4_figure.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig(f'{SAVE_DIR}/E4_figure.pdf', bbox_inches='tight')\n",
    "print('Figure saved')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CSV\n",
    "df.to_csv(f'{SAVE_DIR}/E4_results.csv', index=False)\n",
    "\n",
    "# Save summary JSON\n",
    "summary = {\n",
    "    'experiment': 'E4',\n",
    "    'title': 'Approximation Invariance Test',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'config': CONFIG,\n",
    "    'methods': ['sgd', 'exact', 'kfac', 'empirical'],\n",
    "    'results_at_kappa_1000': {\n",
    "        'exact_ng': float(exact_1000),\n",
    "        'kfac_ng': float(kfac_1000),\n",
    "        'empirical_ng': float(emp_1000),\n",
    "        'sgd': float(sgd_1000)\n",
    "    },\n",
    "    'ratios_vs_exact': {\n",
    "        'kfac_vs_exact': float(kfac_1000 / exact_1000),\n",
    "        'empirical_vs_exact': float(emp_1000 / exact_1000),\n",
    "        'sgd_vs_exact': float(sgd_1000 / exact_1000)\n",
    "    },\n",
    "    'invariance_ranking': [m for m, _ in methods_sorted]\n",
    "}\n",
    "\n",
    "with open(f'{SAVE_DIR}/E4_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f'\\n✓ All files saved to {SAVE_DIR}')\n",
    "print('  - E4_results.csv')\n",
    "print('  - E4_summary.json')\n",
    "print('  - E4_figure.png')\n",
    "print('  - E4_figure.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "**Invariance Spectrum** (expected order, best to worst):\n",
    "1. **Exact NG**: ~10⁻¹¹ (machine precision)\n",
    "2. **K-FAC NG**: ~10⁻? (partial degradation)\n",
    "3. **Empirical Fisher NG**: ~10⁻? (significant degradation)\n",
    "4. **SGD**: ~10⁻¹ (severe degradation)\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- **K-FAC** uses Kronecker factorization (F ≈ S ⊗ A), which is exact for independent input/output distributions but approximate in practice\n",
    "- **Empirical Fisher** uses observed gradients instead of expected gradients, fundamentally different from true Fisher\n",
    "- The degradation spectrum quantifies the \"cost\" of using approximations in terms of coordinate invariance\n",
    "\n",
    "### Implications for Practice\n",
    "\n",
    "- K-FAC may preserve most of Natural Gradient's invariance properties\n",
    "- Empirical Fisher should not be expected to preserve coordinate invariance\n",
    "- This benchmark can evaluate new approximation methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
