{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V2: Realistic Value Signal (Trusted Subset)\n",
    "\n",
    "**Purpose**: Show hysteresis exists even without perfect oracle (clean labels)\n",
    "\n",
    "**Protocol**:\n",
    "- Use only a small fraction (1%, 5%) of clean labels as trusted subset\n",
    "- g_value computed from trusted subset only\n",
    "- Compare with full oracle results\n",
    "\n",
    "**Key Question**: Does two-branch structure survive with realistic value signal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os, glob, json, time\n",
    "from datetime import datetime\n",
    "\n",
    "EXP_NAME = 'exp_V2_trusted_subset'\n",
    "NOTEBOOK_ID = 'V2'\n",
    "BASE_DIR = '/content/drive/MyDrive/dual-gradient-learning/Paper-A'\n",
    "\n",
    "existing = glob.glob(f'{BASE_DIR}/{EXP_NAME}_*')\n",
    "if existing:\n",
    "    SAVE_DIR = sorted(existing)[-1]\n",
    "    print(f'ðŸ”„ Resuming: {SAVE_DIR}')\n",
    "else:\n",
    "    TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    SAVE_DIR = f'{BASE_DIR}/{EXP_NAME}_{TIMESTAMP}'\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    print(f'ðŸ†• New: {SAVE_DIR}')\n",
    "\n",
    "os.makedirs(f'{SAVE_DIR}/checkpoints', exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR}/figures', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils import parameters_to_vector\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "NUM_WORKERS = 4\n",
    "LR = 0.1\n",
    "K = 16\n",
    "\n",
    "NOISE_RATE = 0.4\n",
    "TRUSTED_RATIOS = [0.01, 0.05]  # 1% and 5%\n",
    "\n",
    "ORDERED_LAMBDA = 0.35\n",
    "ORDERED_EPOCHS = 50\n",
    "COLLAPSE_LAMBDA = 0.60\n",
    "COLLAPSE_EPOCHS = 100\n",
    "\n",
    "LAMBDA_START = 0.30\n",
    "LAMBDA_END = 0.70\n",
    "LAMBDA_STEP = 0.04\n",
    "EPOCHS_PER_LAMBDA = 3\n",
    "\n",
    "LAMBDA_GRID_UP = np.round(np.arange(LAMBDA_START, LAMBDA_END + LAMBDA_STEP/2, LAMBDA_STEP), 2)\n",
    "LAMBDA_GRID_DOWN = np.round(np.arange(LAMBDA_END, LAMBDA_START - LAMBDA_STEP/2, -LAMBDA_STEP), 2)\n",
    "\n",
    "N_SEEDS = 3\n",
    "\n",
    "print(f'Trusted ratios: {TRUSTED_RATIOS}')\n",
    "print(f'Î» points: {len(LAMBDA_GRID_UP)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet18():\n",
    "    model = resnet18(weights=None, num_classes=10)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "    return model\n",
    "\n",
    "class IndexedDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        return img, label, idx\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def inject_label_noise(labels, noise_rate, seed):\n",
    "    np.random.seed(seed)\n",
    "    noisy = labels.copy()\n",
    "    n_noisy = int(noise_rate * len(labels))\n",
    "    idx = np.random.choice(len(labels), n_noisy, replace=False)\n",
    "    for i in idx:\n",
    "        noisy[i] = np.random.choice([l for l in range(10) if l != labels[i]])\n",
    "    return noisy\n",
    "\n",
    "def create_trusted_subset(n_total, trusted_ratio, seed):\n",
    "    np.random.seed(seed + 999)\n",
    "    n_trusted = int(n_total * trusted_ratio)\n",
    "    return set(np.random.choice(n_total, n_trusted, replace=False))\n",
    "\n",
    "def load_cifar10():\n",
    "    tr = transforms.Compose([transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(),\n",
    "                             transforms.ToTensor(), transforms.Normalize((0.4914,0.4822,0.4465),(0.2023,0.1994,0.2010))])\n",
    "    te = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914,0.4822,0.4465),(0.2023,0.1994,0.2010))])\n",
    "    return torchvision.datasets.CIFAR10('./data', True, tr, download=True), torchvision.datasets.CIFAR10('./data', False, te, download=True)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            correct += (model(x).argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch_trusted(model, train_loader, opt, clean_t, noisy_t, lam, cached_gv_ref, trusted_mask):\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    step = cached_gv_ref['step']\n",
    "    cached_gv = cached_gv_ref['gv']\n",
    "    \n",
    "    for x, _, idx in train_loader:\n",
    "        x, idx = x.to(device), idx.to(device)\n",
    "        bn = noisy_t[idx]\n",
    "        bc = clean_t[idx]\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss_s = crit(model(x), bn)\n",
    "        loss_s.backward(retain_graph=True)\n",
    "        gs = parameters_to_vector([p.grad for p in model.parameters()]).clone()\n",
    "        \n",
    "        if step % K == 0 or cached_gv is None:\n",
    "            idx_np = idx.cpu().numpy()\n",
    "            trusted_in_batch = [i for i, global_idx in enumerate(idx_np) if global_idx in trusted_mask]\n",
    "            \n",
    "            if len(trusted_in_batch) > 0:\n",
    "                trusted_idx = torch.tensor(trusted_in_batch, device=device)\n",
    "                x_trusted = x[trusted_idx]\n",
    "                bc_trusted = bc[trusted_idx]\n",
    "                \n",
    "                opt.zero_grad()\n",
    "                loss_v = crit(model(x_trusted), bc_trusted)\n",
    "                loss_v.backward()\n",
    "                cached_gv = parameters_to_vector([p.grad for p in model.parameters()]).clone()\n",
    "        \n",
    "        if cached_gv is None:\n",
    "            cached_gv = gs.clone()\n",
    "        \n",
    "        gs_n = gs / (gs.norm() + 1e-12)\n",
    "        gv_n = cached_gv / (cached_gv.norm() + 1e-12)\n",
    "        \n",
    "        g_mix = (1 - lam) * gs_n + lam * gv_n\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        i = 0\n",
    "        for p in model.parameters():\n",
    "            n = p.numel()\n",
    "            p.grad = g_mix[i:i+n].view(p.shape).clone()\n",
    "            i += n\n",
    "        opt.step()\n",
    "        step += 1\n",
    "    \n",
    "    cached_gv_ref['step'] = step\n",
    "    cached_gv_ref['gv'] = cached_gv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = load_cifar10()\n",
    "clean_labels = np.array(trainset.targets)\n",
    "train_loader = DataLoader(IndexedDataset(trainset), BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader = DataLoader(testset, BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "m = get_resnet18().to(device)\n",
    "for _ in range(10): _ = m(torch.randn(BATCH_SIZE,3,32,32,device=device))\n",
    "del m; torch.cuda.empty_cache()\n",
    "print('Ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_checkpoint(seed, target_state, trusted_ratio, train_loader, test_loader, clean_labels):\n",
    "    is_ordered = (target_state == 'ordered')\n",
    "    lam = ORDERED_LAMBDA if is_ordered else COLLAPSE_LAMBDA\n",
    "    epochs = ORDERED_EPOCHS if is_ordered else COLLAPSE_EPOCHS\n",
    "    \n",
    "    set_seed(seed)\n",
    "    noisy_labels = inject_label_noise(clean_labels, NOISE_RATE, seed)\n",
    "    trusted_mask = create_trusted_subset(len(clean_labels), trusted_ratio, seed)\n",
    "    \n",
    "    clean_t = torch.tensor(clean_labels, device=device)\n",
    "    noisy_t = torch.tensor(noisy_labels, device=device)\n",
    "    \n",
    "    model = get_resnet18().to(device)\n",
    "    opt = optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "    milestones = [30, 40] if is_ordered else [50, 80]\n",
    "    sched = optim.lr_scheduler.MultiStepLR(opt, milestones, gamma=0.1)\n",
    "    cached_gv_ref = {'step': 0, 'gv': None}\n",
    "    \n",
    "    print(f'  Training {target_state} at Î»={lam}, trusted={trusted_ratio*100:.0f}%...')\n",
    "    \n",
    "    for ep in range(epochs):\n",
    "        train_one_epoch_trusted(model, train_loader, opt, clean_t, noisy_t, lam, cached_gv_ref, trusted_mask)\n",
    "        sched.step()\n",
    "        if (ep + 1) % 20 == 0:\n",
    "            err = 1 - evaluate(model, test_loader)\n",
    "            print(f'    Epoch {ep+1}: error={err:.4f}')\n",
    "    \n",
    "    final_error = 1 - evaluate(model, test_loader)\n",
    "    \n",
    "    return {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': opt.state_dict(),\n",
    "        'seed': seed,\n",
    "        'trusted_ratio': trusted_ratio,\n",
    "        'target_state': target_state,\n",
    "        'final_error': final_error,\n",
    "        'lambda': lam,\n",
    "        'epochs': epochs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sweep_trusted(ckpt_data, train_loader, test_loader, clean_labels, direction='up'):\n",
    "    seed = ckpt_data['seed']\n",
    "    trusted_ratio = ckpt_data['trusted_ratio']\n",
    "    init_error = ckpt_data['final_error']\n",
    "    \n",
    "    lambda_grid = LAMBDA_GRID_UP if direction == 'up' else LAMBDA_GRID_DOWN\n",
    "    branch = 'ordered_up' if direction == 'up' else 'collapse_down'\n",
    "    \n",
    "    noisy_labels = inject_label_noise(clean_labels, NOISE_RATE, seed)\n",
    "    trusted_mask = create_trusted_subset(len(clean_labels), trusted_ratio, seed)\n",
    "    \n",
    "    clean_t = torch.tensor(clean_labels, device=device)\n",
    "    noisy_t = torch.tensor(noisy_labels, device=device)\n",
    "    \n",
    "    set_seed(seed + 7000)\n",
    "    model = get_resnet18().to(device)\n",
    "    model.load_state_dict(ckpt_data['model_state_dict'])\n",
    "    \n",
    "    opt = optim.SGD(model.parameters(), lr=LR * 0.01, momentum=0.9, weight_decay=5e-4)\n",
    "    cached_gv_ref = {'step': 0, 'gv': None}\n",
    "    trajectory = []\n",
    "    \n",
    "    for lam in lambda_grid:\n",
    "        for _ in range(EPOCHS_PER_LAMBDA):\n",
    "            train_one_epoch_trusted(model, train_loader, opt, clean_t, noisy_t, lam, cached_gv_ref, trusted_mask)\n",
    "        \n",
    "        err = 1 - evaluate(model, test_loader)\n",
    "        trajectory.append({'lambda': float(lam), 'error': err})\n",
    "        print(f'      Î»={lam:.2f}: err={err:.4f}')\n",
    "    \n",
    "    return {\n",
    "        'seed': seed,\n",
    "        'trusted_ratio': trusted_ratio,\n",
    "        'branch': branch,\n",
    "        'init_error': init_error,\n",
    "        'final_error': trajectory[-1]['error'],\n",
    "        'trajectory': trajectory\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "ckpt_file = f'{SAVE_DIR}/{NOTEBOOK_ID}_checkpoint.json'\n",
    "\n",
    "if os.path.exists(ckpt_file):\n",
    "    all_results = json.load(open(ckpt_file))\n",
    "    print(f'Loaded {len(all_results)} previous results')\n",
    "\n",
    "done_keys = {(r['trusted_ratio'], r['seed'], r['branch']) for r in all_results}\n",
    "\n",
    "for trusted_ratio in TRUSTED_RATIOS:\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'TRUSTED RATIO: {trusted_ratio*100:.0f}%')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    for seed in range(N_SEEDS):\n",
    "        if (trusted_ratio, seed, 'ordered_up') in done_keys and (trusted_ratio, seed, 'collapse_down') in done_keys:\n",
    "            print(f'\\nSeed {seed}: Already complete')\n",
    "            continue\n",
    "        \n",
    "        print(f'\\n--- Seed {seed} ---')\n",
    "        \n",
    "        if (trusted_ratio, seed, 'ordered_up') not in done_keys:\n",
    "            print('\\n[Ordered Checkpoint]')\n",
    "            t0 = time.time()\n",
    "            ckpt_ord = create_checkpoint(seed, 'ordered', trusted_ratio, train_loader, test_loader, clean_labels)\n",
    "            print(f'  Created in {(time.time()-t0)/60:.1f}min, error={ckpt_ord[\"final_error\"]:.4f}')\n",
    "            \n",
    "            print('\\n[Ordered Sweep (Î» â†‘)]')\n",
    "            result_ord = run_sweep_trusted(ckpt_ord, train_loader, test_loader, clean_labels, 'up')\n",
    "            result_ord['experiment_id'] = f'{NOTEBOOK_ID}-tr{int(trusted_ratio*100):02d}-ord-s{seed}'\n",
    "            all_results.append(result_ord)\n",
    "            done_keys.add((trusted_ratio, seed, 'ordered_up'))\n",
    "            json.dump(all_results, open(ckpt_file, 'w'), indent=2, default=str)\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        if (trusted_ratio, seed, 'collapse_down') not in done_keys:\n",
    "            print('\\n[Collapse Checkpoint]')\n",
    "            t0 = time.time()\n",
    "            ckpt_col = create_checkpoint(seed + 100, 'collapse', trusted_ratio, train_loader, test_loader, clean_labels)\n",
    "            print(f'  Created in {(time.time()-t0)/60:.1f}min, error={ckpt_col[\"final_error\"]:.4f}')\n",
    "            \n",
    "            print('\\n[Collapse Sweep (Î» â†“)]')\n",
    "            result_col = run_sweep_trusted(ckpt_col, train_loader, test_loader, clean_labels, 'down')\n",
    "            result_col['experiment_id'] = f'{NOTEBOOK_ID}-tr{int(trusted_ratio*100):02d}-col-s{seed}'\n",
    "            all_results.append(result_col)\n",
    "            done_keys.add((trusted_ratio, seed, 'collapse_down'))\n",
    "            json.dump(all_results, open(ckpt_file, 'w'), indent=2, default=str)\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print(f'{NOTEBOOK_ID} COMPLETE')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "json.dump(all_results, open(f'{SAVE_DIR}/{NOTEBOOK_ID}_results.json', 'w'), indent=2, default=str)\n",
    "\n",
    "all_data = []\n",
    "for r in all_results:\n",
    "    for t in r['trajectory']:\n",
    "        all_data.append({'seed': r['seed'], 'trusted_ratio': r['trusted_ratio'], 'branch': r['branch'], 'lambda': t['lambda'], 'error': t['error']})\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(f'{SAVE_DIR}/{NOTEBOOK_ID}_results.csv', index=False)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, len(TRUSTED_RATIOS), figsize=(6*len(TRUSTED_RATIOS), 5))\n",
    "if len(TRUSTED_RATIOS) == 1: axes = [axes]\n",
    "\n",
    "for i, tr in enumerate(TRUSTED_RATIOS):\n",
    "    ax = axes[i]\n",
    "    df_tr = df[df['trusted_ratio'] == tr]\n",
    "    df_ord = df_tr[df_tr['branch'] == 'ordered_up']\n",
    "    df_col = df_tr[df_tr['branch'] == 'collapse_down']\n",
    "    \n",
    "    if len(df_ord) > 0:\n",
    "        mean_ord = df_ord.groupby('lambda')['error'].agg(['mean', 'std']).reset_index()\n",
    "        ax.fill_between(mean_ord['lambda'], mean_ord['mean'] - mean_ord['std'], mean_ord['mean'] + mean_ord['std'], alpha=0.3, color='blue')\n",
    "        ax.plot(mean_ord['lambda'], mean_ord['mean'], 'b-o', linewidth=2, markersize=5, label='Ordered (Î»â†‘)')\n",
    "    \n",
    "    if len(df_col) > 0:\n",
    "        mean_col = df_col.groupby('lambda')['error'].agg(['mean', 'std']).reset_index()\n",
    "        ax.fill_between(mean_col['lambda'], mean_col['mean'] - mean_col['std'], mean_col['mean'] + mean_col['std'], alpha=0.3, color='red')\n",
    "        ax.plot(mean_col['lambda'], mean_col['mean'], 'r-s', linewidth=2, markersize=5, label='Collapse (Î»â†“)')\n",
    "    \n",
    "    ax.axhline(0.40, color='orange', linestyle='--', alpha=0.5)\n",
    "    ax.axhline(0.20, color='green', linestyle='--', alpha=0.5)\n",
    "    ax.set_xlabel('Î»', fontsize=12)\n",
    "    ax.set_ylabel('Test Error', fontsize=12)\n",
    "    ax.set_title(f'Trusted Subset: {tr*100:.0f}%', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(0.28, 0.72)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR}/figures/{NOTEBOOK_ID}_hysteresis_by_trusted.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print('\\n' + '='*60)\n",
    "print(f'{NOTEBOOK_ID} SUMMARY')\n",
    "print('='*60)\n",
    "for tr in TRUSTED_RATIOS:\n",
    "    df_tr = df[df['trusted_ratio'] == tr]\n",
    "    df_ord = df_tr[df_tr['branch'] == 'ordered_up']\n",
    "    df_col = df_tr[df_tr['branch'] == 'collapse_down']\n",
    "    if len(df_ord) > 0 and len(df_col) > 0:\n",
    "        gap = (df_col.groupby('lambda')['error'].mean() - df_ord.groupby('lambda')['error'].mean()).mean()\n",
    "        print(f'\\nðŸ“Š Trusted {tr*100:.0f}%: Mean gap = {gap*100:.1f}%, Two-branch = {\"YES\" if gap > 0.10 else \"WEAK\" if gap > 0.05 else \"NO\"}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
