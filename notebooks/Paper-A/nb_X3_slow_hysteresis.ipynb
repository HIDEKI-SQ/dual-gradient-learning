{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# X3-slow: Hysteresis Loop \u2014 Slow Sweep (10 ep/step)\n\n**Paper-A (P0009) \u2014 X Series: Cross-Domain Generality**\n\n| Item | Value |\n|------|-------|\n| Dataset | SST-2 (binary sentiment) |\n| Model | DistilBERT (pretrained) |\n| Noise | Symmetric, \u03b7 = 0.8 |\n| Trusted ratio | 5% |\n| \u03bb path | 0.20 \u2192 0.65 \u2192 0.20 (\u0394\u03bb = 0.05) |\n| Epochs per step | **10** (slow sweep) |\n| Seeds | 0\u20139 (n=10) |\n| Total steps per run | 19 (10 up + 9 down) |\n| Estimated time | ~8 hours (A100) |\n\n**Purpose**: Test whether ascending and descending \u03bb sweeps produce different outcomes  \nat the same \u03bb \u2014 the hallmark of history dependence (hysteresis).\n\n**Protocol**: One model is initialized, then trained continuously as \u03bb changes.  \nOptimizer state and model weights carry over between steps (no reinitialization).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install -q transformers datasets accelerate\n\nimport os\nimport json\nimport time\nimport copy\nimport random\nimport warnings\nimport logging\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import (\n    DistilBertTokenizer, DistilBertForSequenceClassification\n)\nfrom datasets import load_dataset\n\nwarnings.filterwarnings(\"ignore\")\nlogging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\nos.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\nos.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    gpu = torch.cuda.get_device_properties(0)\n    print(f\"GPU: {gpu.name} ({gpu.total_memory/1e9:.1f} GB)\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "CONFIG = {\n    \"experiment\": \"exp_X3_slow_hysteresis\",\n    \"series\": \"X\",\n    \"dataset\": \"SST-2\",\n    \"num_classes\": 2,\n    \"model_name\": \"distilbert-base-uncased\",\n    \"noise_type\": \"symmetric\",\n    \"noise_rate\": 0.8,\n    \"trusted_ratio\": 0.05,\n\n    # Hysteresis protocol\n    \"lambda_up\": [0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65],\n    \"lambda_down\": [0.60, 0.55, 0.50, 0.45, 0.40, 0.35, 0.30, 0.25, 0.20],\n    \"epochs_per_step\": 10,\n\n    \"seeds\": list(range(10)),\n    \"batch_size\": 64,\n    \"learning_rate\": 2e-5,\n    \"weight_decay\": 0.01,\n    \"warmup_ratio\": 0.1,\n    \"max_seq_length\": 128,\n    \"use_bf16\": True,\n    \"max_train_samples\": 16000,\n    \"csv_measure_every_n_steps\": 25,\n\n    \"output_dir\": \"X3_slow_results\",\n}\n\nos.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n\n# Full \u03bb path\nlambda_path = CONFIG[\"lambda_up\"] + CONFIG[\"lambda_down\"]\ntotal_steps_per_run = len(lambda_path)\nn_runs = len(CONFIG[\"seeds\"])\n\nn_used = min(int(67349 * 0.95), CONFIG[\"max_train_samples\"])\nbatches_per_epoch = n_used // CONFIG[\"batch_size\"]\nupdates_per_step = batches_per_epoch * CONFIG[\"epochs_per_step\"]\ntotal_updates = updates_per_step * total_steps_per_run\nest_per_run = total_updates * 25 / 1000 + total_steps_per_run * 5  # 25ms/update + 5s eval/step\n\nprint(f\"\u03bb path: {lambda_path}\")\nprint(f\"Steps per run: {total_steps_per_run}\")\nprint(f\"Epochs per step: {CONFIG['epochs_per_step']}\")\nprint(f\"Updates per step: ~{updates_per_step}\")\nprint(f\"Total updates per run: ~{total_updates}\")\nprint(f\"Seeds: {n_runs}\")\nprint(f\"Est. per run: ~{est_per_run/60:.1f} min\")\nprint(f\"Est. total: ~{n_runs * est_per_run / 3600:.1f} h\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Google Drive Mount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from google.colab import drive\ndrive.mount('/content/drive')\n\nDRIVE_BASE = \"/content/drive/MyDrive/Paper-A_X_series\"\nDRIVE_OUTPUT = f\"{DRIVE_BASE}/exp_X3_slow_hysteresis\"\nos.makedirs(DRIVE_OUTPUT, exist_ok=True)\nLOCAL_OUTPUT = CONFIG[\"output_dir\"]\n\ndef save_to_drive(filename, data):\n    for d in [LOCAL_OUTPUT, DRIVE_OUTPUT]:\n        with open(os.path.join(d, filename), \"w\") as f:\n            json.dump(data, f, indent=2)\n\ndef save_figure_to_drive(fig, filename, dpi=300):\n    for d in [LOCAL_OUTPUT, DRIVE_OUTPUT]:\n        fig.savefig(os.path.join(d, filename), dpi=dpi, bbox_inches=\"tight\", facecolor=\"white\")\n\nprint(f\"Drive: {DRIVE_OUTPUT}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Deterministic Seeding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Loading SST-2...\")\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\nraw_dataset = load_dataset(\"glue\", \"sst2\")\n\ntrain_texts = list(raw_dataset[\"train\"][\"sentence\"])\ntrain_labels = list(raw_dataset[\"train\"][\"label\"])\ntest_texts = list(raw_dataset[\"validation\"][\"sentence\"])\ntest_labels = list(raw_dataset[\"validation\"][\"label\"])\n\nprint(f\"Train: {len(train_texts)}, Test: {len(test_texts)}\")\n\nprint(\"Tokenizing...\")\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=\"max_length\",\n                            max_length=CONFIG[\"max_seq_length\"], return_tensors=\"pt\")\ntest_encodings = tokenizer(test_texts, truncation=True, padding=\"max_length\",\n                           max_length=CONFIG[\"max_seq_length\"], return_tensors=\"pt\")\nprint(\"Done.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class TextDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n    def __len__(self):\n        return len(self.labels)\n    def __getitem__(self, idx):\n        item = {k: v[idx] for k, v in self.encodings.items()}\n        item[\"labels\"] = self.labels[idx]\n        return item\n\ntest_dataset = TextDataset(test_encodings, torch.tensor(test_labels, dtype=torch.long))\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\nprint(f\"Test loader: {len(test_dataset)} samples\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Noise Injection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def prepare_noisy_and_trusted(train_labels, noise_rate, trusted_ratio,\n                              num_classes, seed, max_noisy_samples=None):\n    rng = np.random.RandomState(seed + 10000)\n    n = len(train_labels)\n    labels = np.array(train_labels)\n    indices = np.arange(n)\n\n    trusted_indices = []\n    for c in range(num_classes):\n        class_idx = indices[labels == c]\n        n_sel = max(1, int(len(class_idx) * trusted_ratio))\n        trusted_indices.extend(rng.choice(class_idx, size=n_sel, replace=False))\n    trusted_indices = np.array(sorted(trusted_indices))\n\n    noisy_mask = np.ones(n, dtype=bool)\n    noisy_mask[trusted_indices] = False\n    noisy_indices = indices[noisy_mask]\n\n    noisy_labels = labels.copy()\n    n_flip = int(len(noisy_indices) * noise_rate)\n    flip_idx = rng.choice(noisy_indices, size=n_flip, replace=False)\n    for idx in flip_idx:\n        candidates = [c for c in range(num_classes) if c != noisy_labels[idx]]\n        noisy_labels[idx] = rng.choice(candidates)\n\n    if max_noisy_samples and len(noisy_indices) > max_noisy_samples:\n        noisy_indices = np.sort(rng.choice(noisy_indices, size=max_noisy_samples, replace=False))\n\n    actual = np.mean(noisy_labels[noisy_indices] != labels[noisy_indices])\n    print(f\"  Trusted: {len(trusted_indices)} | Noisy: {len(noisy_indices)} (\u03b7={actual:.3f})\")\n    return torch.tensor(noisy_labels, dtype=torch.long), trusted_indices, noisy_indices\n\nprint(\"Test...\")\n_, ti, ni = prepare_noisy_and_trusted(train_labels, 0.8, 0.05, 2, 0, 16000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Dual-Gradient Core"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_head_param_names(model):\n    return [n for n, _ in model.named_parameters() if \"classifier\" in n or \"pre_classifier\" in n]\n\ndef extract_grads(model):\n    return {n: p.grad.clone() for n, p in model.named_parameters() if p.grad is not None}\n\ndef normalize_grad_dict(grads):\n    flat = torch.cat([g.flatten() for g in grads.values()])\n    norm = flat.norm()\n    if norm > 0:\n        return {n: g / norm for n, g in grads.items()}, norm.item()\n    return grads, 0.0\n\ndef cosine_sim(grads_a, grads_b, param_names=None):\n    keys = [n for n in (param_names or sorted(set(grads_a) & set(grads_b)))\n            if n in grads_a and n in grads_b]\n    if not keys: return 0.0\n    va = torch.cat([grads_a[n].flatten() for n in keys])\n    vb = torch.cat([grads_b[n].flatten() for n in keys])\n    return F.cosine_similarity(va.unsqueeze(0), vb.unsqueeze(0)).item()\n\ndef set_mixed_grad(model, g_struct_n, g_value_n, lam):\n    for n, p in model.named_parameters():\n        if n in g_struct_n and n in g_value_n:\n            p.grad = (1 - lam) * g_struct_n[n] + lam * g_value_n[n]\n        elif n in g_struct_n:\n            p.grad = (1 - lam) * g_struct_n[n]\n        elif n in g_value_n:\n            p.grad = lam * g_value_n[n]\n\n@torch.no_grad()\ndef evaluate(model, test_loader, device):\n    model.eval()\n    correct = total = 0\n    total_loss = 0.0\n    n = 0\n    for batch in test_loader:\n        ids = batch[\"input_ids\"].to(device)\n        mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n        out = model(input_ids=ids, attention_mask=mask, labels=labels)\n        preds = out.logits.argmax(dim=-1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n        total_loss += out.loss.item()\n        n += 1\n    model.train()\n    acc = correct / total\n    return acc, 1.0 - acc, total_loss / n\n\nprint(\"Core functions defined.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Hysteresis Sweep (Single Run)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def run_hysteresis_sweep(seed, config):\n    \"\"\"\n    Execute one full hysteresis sweep: \u03bb\u2191 then \u03bb\u2193.\n    \n    CRITICAL DESIGN:\n    - Model initialized ONCE at the start\n    - Optimizer state carries over between \u03bb steps (NO reset)\n    - At each step, train for epochs_per_step epochs at current \u03bb\n    - Record test error at end of each step\n    \"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    set_seed(seed)\n    t_start = time.time()\n\n    # --- Prepare data (fixed across the entire sweep) ---\n    noisy_labels, trusted_idx, noisy_idx = prepare_noisy_and_trusted(\n        train_labels, config[\"noise_rate\"], config[\"trusted_ratio\"],\n        config[\"num_classes\"], seed, config.get(\"max_train_samples\")\n    )\n\n    noisy_dataset = TextDataset(\n        {k: v[noisy_idx] for k, v in train_encodings.items()},\n        noisy_labels[noisy_idx]\n    )\n    trusted_dataset = TextDataset(\n        {k: v[trusted_idx] for k, v in train_encodings.items()},\n        noisy_labels[trusted_idx]\n    )\n\n    struct_loader = DataLoader(noisy_dataset, batch_size=config[\"batch_size\"],\n                               shuffle=True, drop_last=True)\n    value_loader = DataLoader(trusted_dataset,\n                              batch_size=min(config[\"batch_size\"], len(trusted_dataset)),\n                              shuffle=True, drop_last=False)\n\n    # --- Initialize model ONCE ---\n    set_seed(seed)\n    model = DistilBertForSequenceClassification.from_pretrained(\n        config[\"model_name\"], num_labels=config[\"num_classes\"]\n    ).to(device)\n    model.train()\n    head_params = get_head_param_names(model)\n\n    # --- Optimizer: persists across ALL \u03bb steps ---\n    optimizer = torch.optim.AdamW(\n        model.parameters(),\n        lr=config[\"learning_rate\"],\n        weight_decay=config[\"weight_decay\"]\n    )\n\n    use_amp = config.get(\"use_bf16\", False) and device.type == \"cuda\"\n    amp_dtype = torch.bfloat16 if use_amp else torch.float32\n\n    # --- Build full \u03bb path ---\n    lambda_path = config[\"lambda_up\"] + config[\"lambda_down\"]\n    phase_labels = [\"up\"] * len(config[\"lambda_up\"]) + [\"down\"] * len(config[\"lambda_down\"])\n\n    # --- Sweep ---\n    sweep_log = []\n    global_step = 0\n    measure_every = config.get(\"csv_measure_every_n_steps\", 25)\n\n    for step_idx, (lam, phase) in enumerate(zip(lambda_path, phase_labels)):\n        step_csv = []\n        step_loss_s = 0.0\n        step_loss_v = 0.0\n        n_updates = 0\n\n        for epoch in range(config[\"epochs_per_step\"]):\n            value_iter = iter(value_loader)\n\n            for struct_batch in struct_loader:\n                # Struct gradient\n                optimizer.zero_grad()\n                s_ids = struct_batch[\"input_ids\"].to(device)\n                s_mask = struct_batch[\"attention_mask\"].to(device)\n                s_labels = struct_batch[\"labels\"].to(device)\n                with torch.amp.autocast(\"cuda\", dtype=amp_dtype, enabled=use_amp):\n                    out_s = model(input_ids=s_ids, attention_mask=s_mask, labels=s_labels)\n                out_s.loss.backward()\n                g_struct = extract_grads(model)\n\n                # Value gradient\n                optimizer.zero_grad()\n                try:\n                    vb = next(value_iter)\n                except StopIteration:\n                    value_iter = iter(value_loader)\n                    vb = next(value_iter)\n                v_ids = vb[\"input_ids\"].to(device)\n                v_mask = vb[\"attention_mask\"].to(device)\n                v_labels = vb[\"labels\"].to(device)\n                with torch.amp.autocast(\"cuda\", dtype=amp_dtype, enabled=use_amp):\n                    out_v = model(input_ids=v_ids, attention_mask=v_mask, labels=v_labels)\n                out_v.loss.backward()\n                g_value = extract_grads(model)\n\n                # Mix and step\n                g_sn, _ = normalize_grad_dict(g_struct)\n                g_vn, _ = normalize_grad_dict(g_value)\n                set_mixed_grad(model, g_sn, g_vn, lam)\n                optimizer.step()\n\n                global_step += 1\n                n_updates += 1\n                step_loss_s += out_s.loss.item()\n                step_loss_v += out_v.loss.item()\n\n                if global_step % measure_every == 0:\n                    c = cosine_sim(g_struct, g_value, head_params)\n                    step_csv.append(c)\n\n        # Evaluate at end of this \u03bb step\n        test_acc, test_error, test_loss = evaluate(model, test_loader, device)\n        mean_csv = float(np.mean(step_csv)) if step_csv else 0.0\n\n        step_record = {\n            \"step_idx\": step_idx,\n            \"lambda\": lam,\n            \"phase\": phase,\n            \"test_acc\": round(test_acc, 4),\n            \"test_error\": round(test_error, 4),\n            \"test_loss\": round(test_loss, 4),\n            \"loss_struct\": round(step_loss_s / max(n_updates, 1), 4),\n            \"loss_value\": round(step_loss_v / max(n_updates, 1), 4),\n            \"c_sv_head\": round(mean_csv, 4),\n            \"n_updates\": n_updates,\n            \"global_step\": global_step,\n        }\n        sweep_log.append(step_record)\n\n        print(f\"    Step {step_idx+1}/{len(lambda_path)} [{phase:4s}] \"\n              f\"\u03bb={lam:.2f}: err={test_error:.4f} c_sv={mean_csv:.4f}\")\n\n    elapsed = time.time() - t_start\n\n    result = {\n        \"experiment_id\": f\"X3slow-{seed:03d}\",\n        \"experiment\": config[\"experiment\"],\n        \"seed\": seed,\n        \"lambda_path\": lambda_path,\n        \"phase_labels\": phase_labels,\n        \"epochs_per_step\": config[\"epochs_per_step\"],\n        \"sweep_log\": sweep_log,\n        \"total_updates\": global_step,\n        \"time_seconds\": round(elapsed, 1),\n    }\n\n    del model, optimizer\n    torch.cuda.empty_cache()\n    return result\n\nprint(\"Hysteresis sweep function defined.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Main Execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "all_results = []\nn_runs = len(CONFIG[\"seeds\"])\nt_total = time.time()\n\nprint(\"=\" * 70)\nprint(f\"X3-slow: Hysteresis Loop \u2014 {n_runs} seeds, {CONFIG['epochs_per_step']} ep/step\")\nprint(\"=\" * 70)\n\nfor i, seed in enumerate(CONFIG[\"seeds\"]):\n    print(f\"\\nSeed {seed} ({i+1}/{n_runs})\")\n    print(\"-\" * 50)\n    result = run_hysteresis_sweep(seed, CONFIG)\n    all_results.append(result)\n\n    elapsed = time.time() - t_total\n    eta = elapsed / (i+1) * (n_runs - i - 1)\n    print(f\"  Done: {result['time_seconds']:.0f}s [ETA: {eta/60:.0f}m]\")\n\n    save_to_drive(f\"X3slow_seed{seed:02d}.json\", result)\n\ntotal_time = time.time() - t_total\nsave_to_drive(\"X3_slow_results.json\", all_results)\nsave_to_drive(\"X3_slow_config.json\", CONFIG)\nprint(f\"\\n{'='*70}\")\nprint(f\"COMPLETE: {n_runs} runs in {total_time/60:.1f} min\")\nprint(f\"{'='*70}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Hysteresis Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Load results\npath = os.path.join(LOCAL_OUTPUT, \"X3_slow_results.json\")\nif not os.path.exists(path):\n    path = os.path.join(DRIVE_OUTPUT, \"X3_slow_results.json\")\nwith open(path) as f:\n    results = json.load(f)\n\n# Extract up/down curves per seed\nlambda_up = CONFIG[\"lambda_up\"]\nlambda_down = CONFIG[\"lambda_down\"]\nn_up = len(lambda_up)\nn_down = len(lambda_down)\n\nup_errors = {lam: [] for lam in lambda_up}\ndown_errors = {lam: [] for lam in lambda_down}\n\nfor r in results:\n    for rec in r[\"sweep_log\"]:\n        lam = rec[\"lambda\"]\n        if rec[\"phase\"] == \"up\" and lam in up_errors:\n            up_errors[lam].append(rec[\"test_error\"])\n        elif rec[\"phase\"] == \"down\" and lam in down_errors:\n            down_errors[lam].append(rec[\"test_error\"])\n\n# ============================================================\n# Fig X3-A: Hysteresis curve\n# ============================================================\nfig, axes = plt.subplots(1, 3, figsize=(20, 6))\n\nax = axes[0]\nup_means = [np.mean(up_errors[l]) for l in lambda_up]\nup_stds = [np.std(up_errors[l]) for l in lambda_up]\ndown_means = [np.mean(down_errors[l]) for l in lambda_down]\ndown_stds = [np.std(down_errors[l]) for l in lambda_down]\n\nax.errorbar(lambda_up, up_means, yerr=up_stds, fmt='o-', color=\"#D62728\",\n            capsize=4, linewidth=2.5, markersize=8, label=\"\u03bb\u2191 (ascending)\")\nax.fill_between(lambda_up, [m-s for m,s in zip(up_means,up_stds)],\n                [m+s for m,s in zip(up_means,up_stds)], alpha=0.1, color=\"#D62728\")\n\nax.errorbar(lambda_down, down_means, yerr=down_stds, fmt='s-', color=\"#1F77B4\",\n            capsize=4, linewidth=2.5, markersize=8, label=\"\u03bb\u2193 (descending)\")\nax.fill_between(lambda_down, [m-s for m,s in zip(down_means,down_stds)],\n                [m+s for m,s in zip(down_means,down_stds)], alpha=0.1, color=\"#1F77B4\")\n\n# Draw arrows for direction\nax.annotate(\"\", xy=(0.65, up_means[-1]+0.02), xytext=(0.20, up_means[0]+0.02),\n            arrowprops=dict(arrowstyle=\"->\", color=\"#D62728\", lw=1.5))\nax.annotate(\"\", xy=(0.20, down_means[-1]-0.02), xytext=(0.60, down_means[0]-0.02),\n            arrowprops=dict(arrowstyle=\"->\", color=\"#1F77B4\", lw=1.5))\n\nax.axhline(0.5, color=\"gray\", ls=\":\", alpha=0.4)\nax.set_xlabel(\"\u03bb\", fontsize=13)\nax.set_ylabel(\"Test Error\", fontsize=13)\nax.set_title(\"(A) Hysteresis Curve: \u03bb\u2191 vs \u03bb\u2193\", fontsize=14, fontweight=\"bold\")\nax.legend(fontsize=11)\nax.grid(True, alpha=0.3)\n\n# ============================================================\n# Fig X3-B: Gap at matched \u03bb\n# ============================================================\nax = axes[1]\nshared_lambdas = sorted(set(lambda_up) & set(lambda_down))\ngaps = []\ngap_ps = []\nfor lam in shared_lambdas:\n    if lam in up_errors and lam in down_errors:\n        u = np.array(up_errors[lam])\n        d = np.array(down_errors[lam])\n        gap = np.mean(u) - np.mean(d)\n        gaps.append(gap)\n        t, p = stats.ttest_ind(u, d)\n        gap_ps.append(p)\n\ncolors_bar = [\"#D62728\" if g > 0 else \"#1F77B4\" for g in gaps]\nbars = ax.bar(range(len(shared_lambdas)), gaps, color=colors_bar, alpha=0.7, edgecolor=\"black\")\nfor i, (g, p) in enumerate(zip(gaps, gap_ps)):\n    sig = \"***\" if p < 0.001 else \"**\" if p < 0.01 else \"*\" if p < 0.05 else \"ns\"\n    ax.text(i, g + (0.005 if g > 0 else -0.015), sig, ha=\"center\", fontsize=10, fontweight=\"bold\")\nax.set_xticks(range(len(shared_lambdas)))\nax.set_xticklabels([f\"{l:.2f}\" for l in shared_lambdas])\nax.axhline(0, color=\"black\", lw=1)\nax.set_xlabel(\"\u03bb\", fontsize=13)\nax.set_ylabel(\"Gap (\u03bb\u2191 error \u2212 \u03bb\u2193 error)\", fontsize=13)\nax.set_title(\"(B) Hysteresis Gap at Matched \u03bb\", fontsize=14, fontweight=\"bold\")\nax.grid(True, alpha=0.3, axis=\"y\")\n\n# ============================================================\n# Fig X3-C: Individual seed traces\n# ============================================================\nax = axes[2]\nfor r in results:\n    up_trace = [(rec[\"lambda\"], rec[\"test_error\"]) for rec in r[\"sweep_log\"] if rec[\"phase\"] == \"up\"]\n    down_trace = [(rec[\"lambda\"], rec[\"test_error\"]) for rec in r[\"sweep_log\"] if rec[\"phase\"] == \"down\"]\n    ax.plot([x[0] for x in up_trace], [x[1] for x in up_trace],\n            '-', color=\"#D62728\", alpha=0.25, linewidth=1)\n    ax.plot([x[0] for x in down_trace], [x[1] for x in down_trace],\n            '-', color=\"#1F77B4\", alpha=0.25, linewidth=1)\n\nax.plot(lambda_up, up_means, 'o-', color=\"#D62728\", linewidth=3, markersize=8,\n        label=\"\u03bb\u2191 mean\", zorder=5)\nax.plot(lambda_down, down_means, 's-', color=\"#1F77B4\", linewidth=3, markersize=8,\n        label=\"\u03bb\u2193 mean\", zorder=5)\nax.axhline(0.5, color=\"gray\", ls=\":\", alpha=0.4)\nax.set_xlabel(\"\u03bb\", fontsize=13)\nax.set_ylabel(\"Test Error\", fontsize=13)\nax.set_title(\"(C) Individual Seed Traces\", fontsize=14, fontweight=\"bold\")\nax.legend(fontsize=11)\nax.grid(True, alpha=0.3)\n\nfig.suptitle(\"X3-slow: Hysteresis Test \u2014 SST-2 / DistilBERT / \u03b7=0.8 / 10 ep per step\",\n             fontsize=15, fontweight=\"bold\", y=1.02)\nplt.tight_layout()\nsave_figure_to_drive(fig, \"X3_slow_hysteresis.png\")\nplt.show()\n\n# Summary stats\nprint(\"\\n\" + \"=\"*60)\nprint(\"HYSTERESIS SUMMARY\")\nprint(\"=\"*60)\nprint(f\"{'\u03bb':>6} | {'\u2191 mean':>8} | {'\u2193 mean':>8} | {'gap':>8} | {'p-value':>10} | {'sig':>5}\")\nprint(\"-\"*55)\nfor i, lam in enumerate(shared_lambdas):\n    u = np.mean(up_errors[lam])\n    d = np.mean(down_errors[lam])\n    print(f\"{lam:>6.2f} | {u:>8.4f} | {d:>8.4f} | {gaps[i]:>8.4f} | {gap_ps[i]:>10.2e} | \"\n          f\"{'***' if gap_ps[i]<0.001 else '**' if gap_ps[i]<0.01 else '*' if gap_ps[i]<0.05 else 'ns':>5}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"Results: {DRIVE_OUTPUT}\")\nfor f in sorted(os.listdir(DRIVE_OUTPUT)):\n    sz = os.path.getsize(os.path.join(DRIVE_OUTPUT, f))\n    print(f\"  {f} ({sz/1024:.1f} KB)\")\n\nimport shutil\nshutil.make_archive(\"X3_slow_results\", \"zip\", LOCAL_OUTPUT)\ntry:\n    from google.colab import files\n    files.download(\"X3_fast_results.zip\")\nexcept: pass\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}