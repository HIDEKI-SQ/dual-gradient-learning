{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Y1-B: Time-Control \u2014 Fixed \u03bb=0.60, Short Training (3 epochs)\\n\\n**Purpose**: Anchor/replication of X1b result (\u03bb=0.60 is good at 3 epochs).\\n\\n- Clean start per seed\\n- \u03bb = 0.60 fixed, 3 epochs\\n- \u03b7 = 0.8, r = 5%, SST-2, DistilBERT\\n- Seeds: 20\\n\\n**Expected**: error \u2248 0.185 (matching X1b)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install -q transformers datasets accelerate\n\nimport os\nimport json\nimport time\nimport copy\nimport random\nimport warnings\nimport logging\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import (\n    DistilBertTokenizer, DistilBertForSequenceClassification\n)\nfrom datasets import load_dataset\n\nwarnings.filterwarnings(\"ignore\")\nlogging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\nos.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\nos.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    gpu = torch.cuda.get_device_properties(0)\n    print(f\"GPU: {gpu.name} ({gpu.total_mem/1e9:.1f} GB)\" if hasattr(gpu, 'total_mem') else f\"GPU: {gpu.name}\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "CONFIG = {\n    \"experiment\": \"exp_Y1B_time_control_short\",\n    \"experiment_id\": \"Y1B\",\n    \"series\": \"Y\",\n    \"experiment_id_prefix\": \"Y1B\",\n    \"dataset\": \"SST-2\",\n    \"num_classes\": 2,\n    \"model_name\": \"distilbert-base-uncased\",\n    \"noise_type\": \"symmetric\",\n    \"noise_rate\": 0.8,\n    \"trusted_ratio\": 0.05,\n    \"lambda_fixed\": 0.60,\n    \"epochs\": 3,  # short \u2014 matches X1b\n    \"seeds\": list(range(20)),\n    \"batch_size\": 64,\n    \"learning_rate\": 2e-5,\n    \"weight_decay\": 0.01,\n    \"warmup_ratio\": 0.1,\n    \"max_seq_length\": 128,\n    \"use_bf16\": True,\n    \"max_train_samples\": 16000,\n    \"output_dir\": \"Y1B_results\",\n}\n\nos.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n\nn_used = min(int(67349 * 0.95), CONFIG[\"max_train_samples\"])\nsteps_per_epoch = n_used // CONFIG[\"batch_size\"]\nn_runs = len(CONFIG[\"seeds\"])\n\nprint(f\"Experiment: {CONFIG['experiment']}\")\nprint(f\"\u03bb fixed: {CONFIG['lambda_fixed']}\")\nprint(f\"Epochs: {CONFIG['epochs']}\")\nprint(f\"Seeds: {n_runs}\")\nest_per_run = steps_per_epoch * CONFIG[\"epochs\"] * 25 / 1000 + 15\nprint(f\"Est. per run: ~{est_per_run:.0f}s\")\nprint(f\"Est. total: ~{n_runs * est_per_run / 3600:.1f} h\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Drive Mount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from google.colab import drive\ndrive.mount('/content/drive')\n\nDRIVE_BASE = \"/content/drive/MyDrive/Paper-A_Y_series\"\nDRIVE_OUTPUT = f\"{DRIVE_BASE}/exp_Y1B_time_control_short\"\nos.makedirs(DRIVE_OUTPUT, exist_ok=True)\nLOCAL_OUTPUT = CONFIG[\"output_dir\"]\n\ndef save_to_drive(filename, data):\n    for d in [LOCAL_OUTPUT, DRIVE_OUTPUT]:\n        with open(os.path.join(d, filename), \"w\") as f:\n            json.dump(data, f, indent=2)\n\ndef save_figure_to_drive(fig, filename, dpi=300):\n    for d in [LOCAL_OUTPUT, DRIVE_OUTPUT]:\n        fig.savefig(os.path.join(d, filename), dpi=dpi, bbox_inches=\"tight\", facecolor=\"white\")\n\nprint(f\"Local: {LOCAL_OUTPUT}\")\nprint(f\"Drive: {DRIVE_OUTPUT}\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Seed Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Loading SST-2...\")\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\nraw_dataset = load_dataset(\"glue\", \"sst2\")\n\ntrain_texts = list(raw_dataset[\"train\"][\"sentence\"])\ntrain_labels = list(raw_dataset[\"train\"][\"label\"])\ntest_texts = list(raw_dataset[\"validation\"][\"sentence\"])\ntest_labels = list(raw_dataset[\"validation\"][\"label\"])\n\nprint(f\"Train: {len(train_texts)}, Test: {len(test_texts)}\")\n\nprint(\"Tokenizing...\")\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=\"max_length\",\n                            max_length=CONFIG[\"max_seq_length\"], return_tensors=\"pt\")\ntest_encodings = tokenizer(test_texts, truncation=True, padding=\"max_length\",\n                           max_length=CONFIG[\"max_seq_length\"], return_tensors=\"pt\")\nprint(\"Done.\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class TextDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n    def __len__(self):\n        return len(self.labels)\n    def __getitem__(self, idx):\n        item = {k: v[idx] for k, v in self.encodings.items()}\n        item[\"labels\"] = self.labels[idx]\n        return item\n\ntest_dataset = TextDataset(test_encodings, torch.tensor(test_labels, dtype=torch.long))\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\nprint(f\"Test loader: {len(test_dataset)} samples\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Noise Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def prepare_noisy_and_trusted(train_labels, noise_rate, trusted_ratio,\n                              num_classes, seed, max_noisy_samples=None):\n    rng = np.random.RandomState(seed + 10000)\n    n = len(train_labels)\n    labels = np.array(train_labels)\n    indices = np.arange(n)\n\n    trusted_indices = []\n    for c in range(num_classes):\n        class_idx = indices[labels == c]\n        n_sel = max(1, int(len(class_idx) * trusted_ratio))\n        trusted_indices.extend(rng.choice(class_idx, size=n_sel, replace=False))\n    trusted_indices = np.array(sorted(trusted_indices))\n\n    noisy_mask = np.ones(n, dtype=bool)\n    noisy_mask[trusted_indices] = False\n    noisy_indices = indices[noisy_mask]\n\n    noisy_labels = labels.copy()\n    n_flip = int(len(noisy_indices) * noise_rate)\n    flip_idx = rng.choice(noisy_indices, size=n_flip, replace=False)\n    for idx in flip_idx:\n        candidates = [c for c in range(num_classes) if c != noisy_labels[idx]]\n        noisy_labels[idx] = rng.choice(candidates)\n\n    if max_noisy_samples and len(noisy_indices) > max_noisy_samples:\n        noisy_indices = np.sort(rng.choice(noisy_indices, size=max_noisy_samples, replace=False))\n\n    actual = np.mean(noisy_labels[noisy_indices] != labels[noisy_indices])\n    print(f\"  Trusted: {len(trusted_indices)} | Noisy: {len(noisy_indices)} (\u03b7={actual:.3f})\")\n    return torch.tensor(noisy_labels, dtype=torch.long), trusted_indices, noisy_indices\n\nprint(\"Noise test:\")\n_, ti, ni = prepare_noisy_and_trusted(train_labels, 0.8, 0.05, 2, 0, 16000)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_head_param_names(model):\n    return [n for n, _ in model.named_parameters() if \"classifier\" in n or \"pre_classifier\" in n]\n\ndef extract_grads(model):\n    return {n: p.grad.clone() for n, p in model.named_parameters() if p.grad is not None}\n\ndef normalize_grad_dict(grads):\n    flat = torch.cat([g.flatten() for g in grads.values()])\n    norm = flat.norm()\n    if norm > 0:\n        return {n: g / norm for n, g in grads.items()}, norm.item()\n    return grads, 0.0\n\ndef cosine_sim(grads_a, grads_b, param_names=None):\n    keys = [n for n in (param_names or sorted(set(grads_a) & set(grads_b)))\n            if n in grads_a and n in grads_b]\n    if not keys: return 0.0\n    va = torch.cat([grads_a[n].flatten() for n in keys])\n    vb = torch.cat([grads_b[n].flatten() for n in keys])\n    return F.cosine_similarity(va.unsqueeze(0), vb.unsqueeze(0)).item()\n\ndef set_mixed_grad(model, g_struct_n, g_value_n, lam):\n    for n, p in model.named_parameters():\n        if n in g_struct_n and n in g_value_n:\n            p.grad = (1 - lam) * g_struct_n[n] + lam * g_value_n[n]\n        elif n in g_struct_n:\n            p.grad = (1 - lam) * g_struct_n[n]\n        elif n in g_value_n:\n            p.grad = lam * g_value_n[n]\n\n@torch.no_grad()\ndef evaluate(model, test_loader, device):\n    model.eval()\n    correct = total = 0\n    total_loss = 0.0\n    n = 0\n    for batch in test_loader:\n        ids = batch[\"input_ids\"].to(device)\n        mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n        out = model(input_ids=ids, attention_mask=mask, labels=labels)\n        preds = out.logits.argmax(dim=-1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n        total_loss += out.loss.item()\n        n += 1\n    model.train()\n    acc = correct / total\n    return acc, 1.0 - acc, total_loss / n\n\nprint(\"Core functions defined.\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def run_fixed_lambda(seed, config):\n    \"\"\"\n    Execute one dual-gradient training run at fixed \u03bb with epoch-level logging.\n    Clean start (fresh model + optimizer).\n    \"\"\"\n    lam = config[\"lambda_fixed\"]\n    exp_id = f\"{config['experiment_id_prefix']}-{seed:03d}-lam{lam:.2f}\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    set_seed(seed)\n    t_start = time.time()\n\n    # Data preparation\n    noisy_labels, trusted_idx, noisy_idx = prepare_noisy_and_trusted(\n        train_labels, config[\"noise_rate\"], config[\"trusted_ratio\"],\n        config[\"num_classes\"], seed, config.get(\"max_train_samples\")\n    )\n\n    noisy_dataset = TextDataset(\n        {k: v[noisy_idx] for k, v in train_encodings.items()},\n        noisy_labels[noisy_idx]\n    )\n    trusted_dataset = TextDataset(\n        {k: v[trusted_idx] for k, v in train_encodings.items()},\n        noisy_labels[trusted_idx]\n    )\n\n    struct_loader = DataLoader(noisy_dataset, batch_size=config[\"batch_size\"],\n                               shuffle=True, drop_last=True)\n    value_loader = DataLoader(trusted_dataset,\n                              batch_size=min(config[\"batch_size\"], len(trusted_dataset)),\n                              shuffle=True, drop_last=False)\n\n    # Model \u2014 fresh init\n    set_seed(seed)\n    model = DistilBertForSequenceClassification.from_pretrained(\n        config[\"model_name\"], num_labels=config[\"num_classes\"]\n    ).to(device)\n    model.train()\n    head_params = get_head_param_names(model)\n\n    optimizer = torch.optim.AdamW(\n        model.parameters(),\n        lr=config[\"learning_rate\"],\n        weight_decay=config[\"weight_decay\"]\n    )\n\n    use_amp = config.get(\"use_bf16\", False) and device.type == \"cuda\"\n    amp_dtype = torch.bfloat16 if use_amp else torch.float32\n\n    # Training\n    epoch_logs = []\n    global_step = 0\n    best_error = 1.0\n    best_epoch = 0\n\n    for epoch in range(config[\"epochs\"]):\n        value_iter = iter(value_loader)\n        epoch_loss_s = 0.0\n        epoch_loss_v = 0.0\n        epoch_cos_sv = []\n        n_batches = 0\n\n        for struct_batch in struct_loader:\n            # Struct gradient\n            optimizer.zero_grad()\n            s_ids = struct_batch[\"input_ids\"].to(device)\n            s_mask = struct_batch[\"attention_mask\"].to(device)\n            s_labels = struct_batch[\"labels\"].to(device)\n            with torch.amp.autocast(\"cuda\", dtype=amp_dtype, enabled=use_amp):\n                out_s = model(input_ids=s_ids, attention_mask=s_mask, labels=s_labels)\n            out_s.loss.backward()\n            g_struct = extract_grads(model)\n\n            # Value gradient\n            optimizer.zero_grad()\n            try:\n                vb = next(value_iter)\n            except StopIteration:\n                value_iter = iter(value_loader)\n                vb = next(value_iter)\n\n            v_ids = vb[\"input_ids\"].to(device)\n            v_mask = vb[\"attention_mask\"].to(device)\n            v_labels = vb[\"labels\"].to(device)\n            with torch.amp.autocast(\"cuda\", dtype=amp_dtype, enabled=use_amp):\n                out_v = model(input_ids=v_ids, attention_mask=v_mask, labels=v_labels)\n            out_v.loss.backward()\n            g_value = extract_grads(model)\n\n            # Normalize\n            g_struct_n, norm_s = normalize_grad_dict(g_struct)\n            g_value_n, norm_v = normalize_grad_dict(g_value)\n\n            # Cosine similarity\n            c_sv = cosine_sim(g_struct_n, g_value_n, head_params)\n            epoch_cos_sv.append(c_sv)\n\n            # Mixed gradient\n            optimizer.zero_grad()\n            set_mixed_grad(model, g_struct_n, g_value_n, lam)\n            optimizer.step()\n\n            epoch_loss_s += out_s.loss.item()\n            epoch_loss_v += out_v.loss.item()\n            n_batches += 1\n            global_step += 1\n\n        # End of epoch eval\n        acc, error, test_loss = evaluate(model, test_loader, device)\n        if error < best_error:\n            best_error = error\n            best_epoch = epoch + 1\n\n        epoch_log = {\n            \"epoch\": epoch + 1,\n            \"test_error\": round(error, 6),\n            \"test_acc\": round(acc, 6),\n            \"test_loss\": round(test_loss, 6),\n            \"train_loss_struct\": round(epoch_loss_s / max(n_batches, 1), 6),\n            \"train_loss_value\": round(epoch_loss_v / max(n_batches, 1), 6),\n            \"mean_cos_sv\": round(float(np.mean(epoch_cos_sv)), 6),\n            \"n_batches\": n_batches\n        }\n        epoch_logs.append(epoch_log)\n\n        if (epoch + 1) % max(1, config[\"epochs\"] // 10) == 0 or epoch == 0:\n            print(f\"    Epoch {epoch+1}/{config['epochs']}: error={error:.4f} cos_sv={epoch_log['mean_cos_sv']:.4f}\")\n\n    elapsed = time.time() - t_start\n    result = {\n        \"experiment_id\": exp_id,\n        \"experiment\": config[\"experiment\"],\n        \"seed\": seed,\n        \"lambda\": lam,\n        \"noise_rate\": config[\"noise_rate\"],\n        \"trusted_ratio\": config[\"trusted_ratio\"],\n        \"epochs\": config[\"epochs\"],\n        \"test_error\": round(epoch_logs[-1][\"test_error\"], 6),\n        \"best_error\": round(best_error, 6),\n        \"best_epoch\": best_epoch,\n        \"epoch_logs\": epoch_logs,\n        \"time_seconds\": round(elapsed, 1)\n    }\n    return result\n\nprint(\"Training function defined.\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Execute"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "all_results = []\nn_runs = len(CONFIG[\"seeds\"])\nt_total = time.time()\n\nprint(\"=\" * 70)\nprint(f\"Y1-B: Time Control (Short) \u2014 \u03bb={CONFIG['lambda_fixed']}, {CONFIG['epochs']} ep, {n_runs} seeds\")\nprint(\"=\" * 70)\n\nfor i, seed in enumerate(CONFIG[\"seeds\"]):\n    print(f\"\\nSeed {seed} ({i+1}/{n_runs})\")\n    result = run_fixed_lambda(seed, CONFIG)\n    all_results.append(result)\n\n    elapsed = time.time() - t_total\n    eta = elapsed / (i+1) * (n_runs - i - 1)\n    print(f\"  final={result['test_error']:.4f} ({result['time_seconds']:.0f}s) [ETA: {eta/60:.0f}m]\")\n\n    save_to_drive(f\"Y1B_seed{seed:02d}.json\", result)\n\ntotal_time = time.time() - t_total\nsave_to_drive(\"Y1B_results.json\", all_results)\nsave_to_drive(\"Y1B_config.json\", CONFIG)\n\nerrors = [r[\"test_error\"] for r in all_results]\nprint(f\"\\n{'='*70}\")\nprint(f\"Y1-B COMPLETE: {n_runs} runs in {total_time/60:.1f} min\")\nprint(f\"Final error: {np.mean(errors):.4f} \u00b1 {np.std(errors):.4f}\")\nprint(f\"X1b reference: ~0.185\")\nprint(f\"{'='*70}\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n\npath = os.path.join(LOCAL_OUTPUT, \"Y1B_results.json\")\nif not os.path.exists(path):\n    path = os.path.join(DRIVE_OUTPUT, \"Y1B_results.json\")\nwith open(path) as f:\n    results = json.load(f)\n\nfinals = [r[\"test_error\"] for r in results]\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 5))\nax.hist(finals, bins=12, color=\"#1F77B4\", alpha=0.7, edgecolor=\"black\")\nax.axvline(np.mean(finals), color=\"red\", ls=\"--\", lw=2, label=f\"Mean: {np.mean(finals):.4f} \u00b1 {np.std(finals):.4f}\")\nax.axvline(0.185, color=\"#D62728\", ls=\":\", lw=2, alpha=0.7, label=\"X1b ref: 0.185\")\nax.set_xlabel(\"Final Test Error (3 epochs)\", fontsize=13)\nax.set_ylabel(\"Count\", fontsize=13)\nax.set_title(\"Y1-B: Anchor (\u03bb=0.60, 3 ep, clean start)\", fontsize=13, fontweight=\"bold\")\nax.legend(fontsize=11)\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nsave_figure_to_drive(fig, \"Y1B_analysis.png\")\nplt.show()\n\nprint(f\"Y1-B final: {np.mean(finals):.4f} \u00b1 {np.std(finals):.4f}\")\nprint(f\"X1b ref:    ~0.185\")\nmatch = abs(np.mean(finals) - 0.185) < 0.05\nprint(f\"Match: {'YES' if match else 'NO'} (\u0394 = {abs(np.mean(finals)-0.185):.4f})\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save & Download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"All results saved to: {DRIVE_OUTPUT}\")\nfor f in sorted(os.listdir(DRIVE_OUTPUT)):\n    sz = os.path.getsize(os.path.join(DRIVE_OUTPUT, f))\n    print(f\"  {f} ({sz/1024:.1f} KB)\")\n\nimport shutil\nshutil.make_archive(CONFIG[\"output_dir\"], \"zip\", LOCAL_OUTPUT)\ntry:\n    from google.colab import files\n    files.download(f\"{CONFIG['output_dir']}.zip\")\nexcept:\n    pass\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ]
}