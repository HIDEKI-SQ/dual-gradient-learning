{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W2: Causal Intervention\n",
    "\n",
    "**Purpose**: Demonstrate that hysteresis is truly history-dependent via causal intervention\n",
    "\n",
    "**Protocol**:\n",
    "- Start from Ordered checkpoint (low error) and Collapse checkpoint (high error)\n",
    "- Apply sudden Œª jump to the SAME target Œª value\n",
    "- Observe: Do they converge to same state, or remain distinct?\n",
    "\n",
    "**Key Question**: At the same Œª, does history matter? (Causal test of bistability)\n",
    "\n",
    "**Interventions**:\n",
    "1. Ordered @ Œª=0.35 ‚Üí Jump to Œª=0.50 ‚Üí Train N epochs\n",
    "2. Collapse @ Œª=0.60 ‚Üí Jump to Œª=0.50 ‚Üí Train N epochs\n",
    "3. Compare final states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os, glob, json, time\n",
    "from datetime import datetime\n",
    "\n",
    "EXP_NAME = 'exp_W2_causal_intervention'\n",
    "NOTEBOOK_ID = 'W2'\n",
    "BASE_DIR = '/content/drive/MyDrive/dual-gradient-learning/Paper-A'\n",
    "\n",
    "existing = glob.glob(f'{BASE_DIR}/{EXP_NAME}_*')\n",
    "if existing:\n",
    "    SAVE_DIR = sorted(existing)[-1]\n",
    "    print(f'üîÑ Resuming: {SAVE_DIR}')\n",
    "else:\n",
    "    TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    SAVE_DIR = f'{BASE_DIR}/{EXP_NAME}_{TIMESTAMP}'\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    print(f'üÜï New: {SAVE_DIR}')\n",
    "\n",
    "os.makedirs(f'{SAVE_DIR}/checkpoints', exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR}/figures', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils import parameters_to_vector\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core parameters\n",
    "BATCH_SIZE = 256\n",
    "NUM_WORKERS = 4\n",
    "LR = 0.1\n",
    "K = 16\n",
    "NOISE_RATE = 0.4\n",
    "\n",
    "# Checkpoint creation\n",
    "ORDERED_LAMBDA = 0.35\n",
    "ORDERED_EPOCHS = 50\n",
    "ORDERED_THRESHOLD = 0.25\n",
    "\n",
    "COLLAPSE_LAMBDA = 0.60\n",
    "COLLAPSE_EPOCHS = 80\n",
    "COLLAPSE_THRESHOLD = 0.45\n",
    "\n",
    "# Intervention settings\n",
    "# Jump both checkpoints to these target Œª values\n",
    "TARGET_LAMBDAS = [0.40, 0.50, 0.55]  # Multiple intervention points\n",
    "POST_INTERVENTION_EPOCHS = 30  # Train this many epochs after jump\n",
    "EVAL_FREQ = 5  # Evaluate every N epochs\n",
    "\n",
    "N_SEEDS = 3\n",
    "\n",
    "print(f'Intervention targets: {TARGET_LAMBDAS}')\n",
    "print(f'Post-intervention epochs: {POST_INTERVENTION_EPOCHS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet18():\n",
    "    model = resnet18(weights=None, num_classes=10)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "    return model\n",
    "\n",
    "class IndexedDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        return img, label, idx\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def inject_label_noise(labels, noise_rate, seed):\n",
    "    np.random.seed(seed)\n",
    "    noisy = labels.copy()\n",
    "    n_noisy = int(noise_rate * len(labels))\n",
    "    idx = np.random.choice(len(labels), n_noisy, replace=False)\n",
    "    for i in idx:\n",
    "        noisy[i] = np.random.choice([l for l in range(10) if l != labels[i]])\n",
    "    return noisy\n",
    "\n",
    "def load_cifar10():\n",
    "    tr = transforms.Compose([transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(),\n",
    "                             transforms.ToTensor(), transforms.Normalize((0.4914,0.4822,0.4465),(0.2023,0.1994,0.2010))])\n",
    "    te = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914,0.4822,0.4465),(0.2023,0.1994,0.2010))])\n",
    "    return torchvision.datasets.CIFAR10('./data', True, tr, download=True), torchvision.datasets.CIFAR10('./data', False, te, download=True)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            correct += (model(x).argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, opt, clean_t, noisy_t, lam, state):\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    step = state['step']\n",
    "    cached_gv = state['gv']\n",
    "    \n",
    "    for x, _, idx in train_loader:\n",
    "        x, idx = x.to(device), idx.to(device)\n",
    "        bn, bc = noisy_t[idx], clean_t[idx]\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss_s = crit(model(x), bn)\n",
    "        loss_s.backward(retain_graph=True)\n",
    "        g_s = parameters_to_vector([p.grad for p in model.parameters()]).clone()\n",
    "        \n",
    "        if step % K == 0 or cached_gv is None:\n",
    "            opt.zero_grad()\n",
    "            loss_v = crit(model(x), bc)\n",
    "            loss_v.backward()\n",
    "            cached_gv = parameters_to_vector([p.grad for p in model.parameters()]).clone()\n",
    "        \n",
    "        g_s_n = g_s / (g_s.norm() + 1e-12)\n",
    "        g_v_n = cached_gv / (cached_gv.norm() + 1e-12)\n",
    "        g_mix = (1 - lam) * g_s_n + lam * g_v_n\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        i = 0\n",
    "        for p in model.parameters():\n",
    "            n = p.numel()\n",
    "            p.grad = g_mix[i:i+n].view(p.shape).clone()\n",
    "            i += n\n",
    "        opt.step()\n",
    "        step += 1\n",
    "    \n",
    "    state['step'] = step\n",
    "    state['gv'] = cached_gv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = load_cifar10()\n",
    "clean_labels = np.array(trainset.targets)\n",
    "train_loader = DataLoader(IndexedDataset(trainset), BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader = DataLoader(testset, BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "m = get_resnet18().to(device)\n",
    "for _ in range(5): _ = m(torch.randn(BATCH_SIZE,3,32,32,device=device))\n",
    "del m; torch.cuda.empty_cache()\n",
    "print('Ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_intervention(checkpoint_state, checkpoint_type, target_lambda, seed, clean_t, noisy_t):\n",
    "    \"\"\"\n",
    "    Run causal intervention: jump to target_lambda and observe trajectory.\n",
    "    \"\"\"\n",
    "    set_seed(seed + 500 + int(target_lambda * 100))\n",
    "    \n",
    "    model = get_resnet18().to(device)\n",
    "    model.load_state_dict({k: v.to(device) for k, v in checkpoint_state.items()})\n",
    "    \n",
    "    # Use lower LR for intervention phase (like sweep)\n",
    "    opt = optim.SGD(model.parameters(), lr=LR * 0.01, momentum=0.9, weight_decay=5e-4)\n",
    "    state = {'step': 0, 'gv': None}\n",
    "    \n",
    "    init_error = 1 - evaluate(model, test_loader)\n",
    "    trajectory = [{'epoch': 0, 'error': init_error}]\n",
    "    \n",
    "    # Train at target lambda\n",
    "    for ep in range(POST_INTERVENTION_EPOCHS):\n",
    "        train_one_epoch(model, train_loader, opt, clean_t, noisy_t, target_lambda, state)\n",
    "        \n",
    "        if (ep + 1) % EVAL_FREQ == 0:\n",
    "            err = 1 - evaluate(model, test_loader)\n",
    "            trajectory.append({'epoch': ep + 1, 'error': err})\n",
    "    \n",
    "    final_error = 1 - evaluate(model, test_loader)\n",
    "    if trajectory[-1]['epoch'] != POST_INTERVENTION_EPOCHS:\n",
    "        trajectory.append({'epoch': POST_INTERVENTION_EPOCHS, 'error': final_error})\n",
    "    \n",
    "    del model; torch.cuda.empty_cache()\n",
    "    \n",
    "    return {\n",
    "        'source': checkpoint_type,\n",
    "        'target_lambda': target_lambda,\n",
    "        'init_error': init_error,\n",
    "        'final_error': final_error,\n",
    "        'trajectory': trajectory\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "ckpt_file = f'{SAVE_DIR}/{NOTEBOOK_ID}_checkpoint.json'\n",
    "\n",
    "if os.path.exists(ckpt_file):\n",
    "    results = json.load(open(ckpt_file))\n",
    "    done_seeds = {r['seed'] for r in results}\n",
    "    print(f'Loaded: {len(done_seeds)} seeds done')\n",
    "else:\n",
    "    done_seeds = set()\n",
    "\n",
    "for seed in range(N_SEEDS):\n",
    "    if seed in done_seeds:\n",
    "        print(f'Seed {seed}: Already done')\n",
    "        continue\n",
    "    \n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'SEED {seed}')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    # Setup\n",
    "    noisy_labels = inject_label_noise(clean_labels, NOISE_RATE, seed)\n",
    "    clean_t = torch.tensor(clean_labels, device=device)\n",
    "    noisy_t = torch.tensor(noisy_labels, device=device)\n",
    "    \n",
    "    seed_result = {'seed': seed, 'interventions': []}\n",
    "    \n",
    "    # === Phase 1: Create Ordered Checkpoint ===\n",
    "    print(f'\\n[Phase 1] Creating Ordered Checkpoint (Œª={ORDERED_LAMBDA})...')\n",
    "    set_seed(seed)\n",
    "    model = get_resnet18().to(device)\n",
    "    opt = optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "    sched = optim.lr_scheduler.MultiStepLR(opt, [30, 40], gamma=0.1)\n",
    "    state = {'step': 0, 'gv': None}\n",
    "    \n",
    "    for ep in range(ORDERED_EPOCHS):\n",
    "        train_one_epoch(model, train_loader, opt, clean_t, noisy_t, ORDERED_LAMBDA, state)\n",
    "        sched.step()\n",
    "        if (ep + 1) % 10 == 0:\n",
    "            err = 1 - evaluate(model, test_loader)\n",
    "            print(f'  Epoch {ep+1}: error={err:.4f}')\n",
    "    \n",
    "    ordered_error = 1 - evaluate(model, test_loader)\n",
    "    ordered_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "    seed_result['ordered_init_error'] = ordered_error\n",
    "    print(f'  ‚úÖ Ordered checkpoint: {ordered_error:.2%}')\n",
    "    del model; torch.cuda.empty_cache()\n",
    "    \n",
    "    # === Phase 2: Create Collapse Checkpoint ===\n",
    "    print(f'\\n[Phase 2] Creating Collapse Checkpoint (Œª={COLLAPSE_LAMBDA})...')\n",
    "    set_seed(seed + 100)\n",
    "    model = get_resnet18().to(device)\n",
    "    opt = optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "    sched = optim.lr_scheduler.MultiStepLR(opt, [40, 60], gamma=0.1)\n",
    "    state = {'step': 0, 'gv': None}\n",
    "    \n",
    "    for ep in range(COLLAPSE_EPOCHS):\n",
    "        train_one_epoch(model, train_loader, opt, clean_t, noisy_t, COLLAPSE_LAMBDA, state)\n",
    "        sched.step()\n",
    "        if (ep + 1) % 20 == 0:\n",
    "            err = 1 - evaluate(model, test_loader)\n",
    "            print(f'  Epoch {ep+1}: error={err:.4f}')\n",
    "    \n",
    "    collapse_error = 1 - evaluate(model, test_loader)\n",
    "    collapse_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "    seed_result['collapse_init_error'] = collapse_error\n",
    "    print(f'  üíÄ Collapse checkpoint: {collapse_error:.2%}')\n",
    "    del model; torch.cuda.empty_cache()\n",
    "    \n",
    "    # === Phase 3: Causal Interventions ===\n",
    "    print(f'\\n[Phase 3] Causal Interventions...')\n",
    "    \n",
    "    for target_lam in TARGET_LAMBDAS:\n",
    "        print(f'\\n  --- Target Œª = {target_lam} ---')\n",
    "        \n",
    "        # Intervention from Ordered\n",
    "        print(f'    From Ordered ({ordered_error:.2%}) ‚Üí Œª={target_lam}...')\n",
    "        ord_result = run_intervention(ordered_state, 'ordered', target_lam, seed, clean_t, noisy_t)\n",
    "        print(f'      Final: {ord_result[\"final_error\"]:.2%}')\n",
    "        \n",
    "        # Intervention from Collapse\n",
    "        print(f'    From Collapse ({collapse_error:.2%}) ‚Üí Œª={target_lam}...')\n",
    "        col_result = run_intervention(collapse_state, 'collapse', target_lam, seed, clean_t, noisy_t)\n",
    "        print(f'      Final: {col_result[\"final_error\"]:.2%}')\n",
    "        \n",
    "        # Gap after intervention\n",
    "        gap = col_result['final_error'] - ord_result['final_error']\n",
    "        print(f'    üìä Gap after {POST_INTERVENTION_EPOCHS} epochs: {gap*100:.1f}%')\n",
    "        \n",
    "        seed_result['interventions'].append({\n",
    "            'target_lambda': target_lam,\n",
    "            'ordered': ord_result,\n",
    "            'collapse': col_result,\n",
    "            'final_gap': gap\n",
    "        })\n",
    "    \n",
    "    elapsed = time.time() - t0\n",
    "    seed_result['time_seconds'] = elapsed\n",
    "    seed_result['experiment_id'] = f'{NOTEBOOK_ID}-seed{seed:02d}'\n",
    "    \n",
    "    results.append(seed_result)\n",
    "    json.dump(results, open(ckpt_file, 'w'), indent=2, default=str)\n",
    "    done_seeds.add(seed)\n",
    "    print(f'\\n  ‚è±Ô∏è Time: {elapsed/60:.1f} min')\n",
    "\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'{NOTEBOOK_ID} COMPLETE')\n",
    "print(f'{\"=\"*60}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "json.dump(results, open(f'{SAVE_DIR}/{NOTEBOOK_ID}_results.json', 'w'), indent=2, default=str)\n",
    "\n",
    "# Visualization: Intervention trajectories for each target Œª\n",
    "n_targets = len(TARGET_LAMBDAS)\n",
    "fig, axes = plt.subplots(1, n_targets, figsize=(6*n_targets, 5))\n",
    "if n_targets == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, target_lam in zip(axes, TARGET_LAMBDAS):\n",
    "    # Collect trajectories for this target\n",
    "    for r in results:\n",
    "        for intv in r['interventions']:\n",
    "            if intv['target_lambda'] == target_lam:\n",
    "                # Ordered trajectory\n",
    "                epochs_o = [t['epoch'] for t in intv['ordered']['trajectory']]\n",
    "                errors_o = [t['error'] for t in intv['ordered']['trajectory']]\n",
    "                ax.plot(epochs_o, errors_o, 'b-o', alpha=0.6, linewidth=2, markersize=5)\n",
    "                \n",
    "                # Collapse trajectory\n",
    "                epochs_c = [t['epoch'] for t in intv['collapse']['trajectory']]\n",
    "                errors_c = [t['error'] for t in intv['collapse']['trajectory']]\n",
    "                ax.plot(epochs_c, errors_c, 'r-s', alpha=0.6, linewidth=2, markersize=5)\n",
    "    \n",
    "    ax.axhline(0.40, color='orange', linestyle='--', alpha=0.5)\n",
    "    ax.set_xlabel('Epochs after intervention', fontsize=12)\n",
    "    ax.set_ylabel('Test Error', fontsize=12)\n",
    "    ax.set_title(f'Intervention to Œª={target_lam}', fontsize=14, fontweight='bold')\n",
    "    ax.legend(['From Ordered', 'From Collapse'], fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(-1, POST_INTERVENTION_EPOCHS + 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR}/figures/{NOTEBOOK_ID}_intervention_trajectories.png', dpi=150)\n",
    "plt.savefig(f'{SAVE_DIR}/figures/{NOTEBOOK_ID}_intervention_trajectories.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot: Final gap for each target Œª\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "gap_data = {lam: [] for lam in TARGET_LAMBDAS}\n",
    "for r in results:\n",
    "    for intv in r['interventions']:\n",
    "        gap_data[intv['target_lambda']].append(intv['final_gap'])\n",
    "\n",
    "x = np.arange(len(TARGET_LAMBDAS))\n",
    "means = [np.mean(gap_data[lam]) for lam in TARGET_LAMBDAS]\n",
    "stds = [np.std(gap_data[lam]) for lam in TARGET_LAMBDAS]\n",
    "\n",
    "bars = ax.bar(x, [m * 100 for m in means], yerr=[s * 100 for s in stds], \n",
    "              capsize=5, color='steelblue', alpha=0.8, edgecolor='navy')\n",
    "\n",
    "ax.axhline(10, color='green', linestyle='--', alpha=0.7, label='Strong gap threshold (10%)')\n",
    "ax.axhline(0, color='black', linestyle='-', alpha=0.3)\n",
    "\n",
    "ax.set_xlabel('Target Œª', fontsize=12)\n",
    "ax.set_ylabel('Remaining Gap after Intervention (%)', fontsize=12)\n",
    "ax.set_title(f'Causal Intervention: History-Dependence Test\\n({POST_INTERVENTION_EPOCHS} epochs at each target Œª)', fontsize=14)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'Œª={lam}' for lam in TARGET_LAMBDAS])\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (m, s) in enumerate(zip(means, stds)):\n",
    "    ax.text(i, m*100 + s*100 + 1, f'{m*100:.1f}%', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR}/figures/{NOTEBOOK_ID}_gap_after_intervention.png', dpi=150)\n",
    "plt.savefig(f'{SAVE_DIR}/figures/{NOTEBOOK_ID}_gap_after_intervention.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print('='*60)\n",
    "print(f'{NOTEBOOK_ID} SUMMARY: Causal Intervention')\n",
    "print('='*60)\n",
    "\n",
    "print(f'\\nüìä Experimental Design:')\n",
    "print(f'   Ordered checkpoint: Œª={ORDERED_LAMBDA}')\n",
    "print(f'   Collapse checkpoint: Œª={COLLAPSE_LAMBDA}')\n",
    "print(f'   Post-intervention epochs: {POST_INTERVENTION_EPOCHS}')\n",
    "print(f'   Target Œª values: {TARGET_LAMBDAS}')\n",
    "\n",
    "print(f'\\nüìä Results by target Œª:')\n",
    "for lam in TARGET_LAMBDAS:\n",
    "    gaps = gap_data[lam]\n",
    "    if gaps:\n",
    "        mean_gap = np.mean(gaps)\n",
    "        std_gap = np.std(gaps)\n",
    "        print(f'\\n   Œª={lam}:')\n",
    "        print(f'     Gap after intervention: {mean_gap*100:.1f} ¬± {std_gap*100:.1f}%')\n",
    "        if mean_gap > 0.10:\n",
    "            print(f'     ‚Üí ‚úÖ Strong history-dependence (bistability confirmed)')\n",
    "        elif mean_gap > 0.05:\n",
    "            print(f'     ‚Üí ‚ö†Ô∏è Moderate history-dependence')\n",
    "        else:\n",
    "            print(f'     ‚Üí ‚ùå Weak/no history-dependence (converging)')\n",
    "\n",
    "# Overall conclusion\n",
    "all_gaps = [g for gaps in gap_data.values() for g in gaps]\n",
    "overall_mean = np.mean(all_gaps) if all_gaps else 0\n",
    "\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'CONCLUSION:')\n",
    "if overall_mean > 0.10:\n",
    "    print(f'  ‚úÖ BISTABILITY CONFIRMED')\n",
    "    print(f'  ‚úÖ History determines state even at same Œª')\n",
    "    print(f'  ‚úÖ Average persistent gap: {overall_mean*100:.1f}%')\n",
    "elif overall_mean > 0.05:\n",
    "    print(f'  ‚ö†Ô∏è PARTIAL BISTABILITY')\n",
    "    print(f'  ‚ö†Ô∏è Some history-dependence persists')\n",
    "    print(f'  ‚ö†Ô∏è Average persistent gap: {overall_mean*100:.1f}%')\n",
    "else:\n",
    "    print(f'  ‚ùå NO BISTABILITY')\n",
    "    print(f'  ‚ùå States converge regardless of history')\n",
    "    print(f'  ‚ùå Average persistent gap: {overall_mean*100:.1f}%')\n",
    "print(f'{\"=\"*60}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary CSV\n",
    "summary_data = []\n",
    "for r in results:\n",
    "    for intv in r['interventions']:\n",
    "        summary_data.append({\n",
    "            'seed': r['seed'],\n",
    "            'target_lambda': intv['target_lambda'],\n",
    "            'ordered_init': intv['ordered']['init_error'],\n",
    "            'ordered_final': intv['ordered']['final_error'],\n",
    "            'collapse_init': intv['collapse']['init_error'],\n",
    "            'collapse_final': intv['collapse']['final_error'],\n",
    "            'final_gap': intv['final_gap']\n",
    "        })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "df_summary.to_csv(f'{SAVE_DIR}/{NOTEBOOK_ID}_summary.csv', index=False)\n",
    "print('Summary saved to CSV')\n",
    "print(df_summary.to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
