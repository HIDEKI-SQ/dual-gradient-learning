{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp R3b: Spinodal Estimation (Collapse â†’ Ordered)\n",
    "\n",
    "## ç›®çš„\n",
    "CollapseçŠ¶æ…‹ã‹ã‚‰é–‹å§‹ã—ã€Î»ã‚’ä¸‹ã’ã¦ã„ãã€Œå›å¾©ã‚¤ãƒ™ãƒ³ãƒˆã€ãŒèµ·ãã‚‹Î»_â†“ï¼ˆä¸‹å´ã‚¹ãƒ”ãƒãƒ¼ãƒ€ãƒ«ï¼‰ã‚’æ¸¬å®šã€‚\n",
    "\n",
    "## å®Ÿé¨“è¨­è¨ˆ\n",
    "- **åˆæœŸçŠ¶æ…‹**: collapse-initï¼ˆÎ»=0.65ã§30 epochå­¦ç¿’æ¸ˆã¿ï¼‰\n",
    "- **Sweepæ–¹å‘**: Î»â†“ï¼ˆcollapse â†’ orderedæ–¹å‘ï¼‰\n",
    "- **ã‚¤ãƒ™ãƒ³ãƒˆåˆ¤å®š**: test error < 25% ãŒæ•°ã‚¨ãƒãƒƒã‚¯ç¶™ç¶š\n",
    "- **ä¿æŒæ¡ä»¶**: short (1 epoch), long (3 epochs)\n",
    "- **Î·**: 0.4, 0.8\n",
    "- **Seeds**: 20\n",
    "- **Total**: 2Î· Ã— 2ä¿æŒ Ã— 10seeds = 40 runs\n",
    "\n",
    "## ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯\n",
    "- **æ¡ä»¶**: å…¨æ¡ä»¶ï¼ˆseeds 0-9ï¼‰\n",
    "- **runs**: 40\n",
    "- **æ¨å®šæ™‚é–“**: ~7æ™‚é–“\n",
    "\n",
    "## é–¢é€£ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯\n",
    "- R3a: orderedâ†’collapse (spinodal Î»_â†‘)\n",
    "- R3b (this): collapseâ†’ordered (spinodal Î»_â†“)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— =====\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "EXP_NAME = 'exp_R3_spinodal'\n",
    "NOTEBOOK_ID = 'R3b'\n",
    "BASE_DIR = '/content/drive/MyDrive/dual-gradient-learning/Paper-A'\n",
    "\n",
    "existing = glob.glob(f'{BASE_DIR}/{EXP_NAME}_*')\n",
    "if existing:\n",
    "    SAVE_DIR = sorted(existing)[-1]\n",
    "    print(f'ğŸ”„ Resuming from: {SAVE_DIR}')\n",
    "else:\n",
    "    TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    SAVE_DIR = f'{BASE_DIR}/{EXP_NAME}_{TIMESTAMP}'\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    print(f'ğŸ†• New experiment: {SAVE_DIR}')\n",
    "\n",
    "os.makedirs(f'{SAVE_DIR}/figures', exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR}/checkpoints', exist_ok=True)\n",
    "print(f'Save directory: {SAVE_DIR}')\n",
    "print(f'This notebook: {NOTEBOOK_ID} (collapseâ†’ordered)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils import parameters_to_vector\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ResNet18 =====\n",
    "def get_resnet18():\n",
    "    model = resnet18(weights=None, num_classes=10)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "    return model\n",
    "\n",
    "class IndexedDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        return img, label, idx\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== å®Ÿé¨“ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ =====\n",
    "BATCH_SIZE = 256\n",
    "NUM_WORKERS = 4\n",
    "LR = 0.1\n",
    "K = 16\n",
    "\n",
    "# Spinodal estimation parameters (collapse â†’ ordered)\n",
    "INIT_LAMBDA = 0.65  # collapse regime\n",
    "INIT_EPOCHS = 30    # epochs to establish collapse state\n",
    "RECOVERY_THRESHOLD = 0.25  # error threshold for recovery\n",
    "RECOVERY_SUSTAIN = 3  # epochs to sustain for confirmation\n",
    "MAX_SWEEP_EPOCHS = 100  # max epochs during sweep\n",
    "\n",
    "# Î» sweep range (collapse â†’ ordered)\n",
    "LAMBDA_START = 0.60\n",
    "LAMBDA_END = 0.20\n",
    "LAMBDA_STEP = -0.02  # negative for downward sweep\n",
    "\n",
    "# Conditions for this notebook\n",
    "NOISE_RATES = [0.4, 0.8]\n",
    "HOLD_TYPES = ['short', 'long']\n",
    "SEEDS = list(range(10))\n",
    "\n",
    "experiments = []\n",
    "for eta in NOISE_RATES:\n",
    "    for hold_type in HOLD_TYPES:\n",
    "        for seed in SEEDS:\n",
    "            experiments.append({\n",
    "                'noise_rate': eta,\n",
    "                'hold_type': hold_type,\n",
    "                'hold_epochs': 1 if hold_type == 'short' else 3,\n",
    "                'seed': seed,\n",
    "                'direction': 'down'  # collapse â†’ ordered\n",
    "            })\n",
    "\n",
    "print(f'Total experiments: {len(experiments)}')\n",
    "print(f'Noise rates: {NOISE_RATES}')\n",
    "print(f'Hold types: {HOLD_TYPES}')\n",
    "print(f'Seeds: {SEEDS}')\n",
    "\n",
    "config = {\n",
    "    'experiment': EXP_NAME,\n",
    "    'notebook_id': NOTEBOOK_ID,\n",
    "    'direction': 'collapse_to_ordered',\n",
    "    'parameters': {\n",
    "        'init_lambda': INIT_LAMBDA,\n",
    "        'init_epochs': INIT_EPOCHS,\n",
    "        'recovery_threshold': RECOVERY_THRESHOLD,\n",
    "        'recovery_sustain': RECOVERY_SUSTAIN,\n",
    "        'lambda_range': [LAMBDA_START, LAMBDA_END, LAMBDA_STEP],\n",
    "        'noise_rates': NOISE_RATES,\n",
    "        'hold_types': HOLD_TYPES,\n",
    "        'seeds': SEEDS\n",
    "    }\n",
    "}\n",
    "with open(f'{SAVE_DIR}/{NOTEBOOK_ID}_config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° =====\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def inject_label_noise(labels, noise_rate, seed):\n",
    "    np.random.seed(seed)\n",
    "    noisy_labels = labels.copy()\n",
    "    n_samples = len(labels)\n",
    "    n_noisy = int(noise_rate * n_samples)\n",
    "    noisy_indices = np.random.choice(n_samples, n_noisy, replace=False)\n",
    "    for idx in noisy_indices:\n",
    "        original = labels[idx]\n",
    "        new_label = np.random.choice([l for l in range(10) if l != original])\n",
    "        noisy_labels[idx] = new_label\n",
    "    return noisy_labels\n",
    "\n",
    "def load_cifar10():\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "    return trainset, testset\n",
    "\n",
    "def get_data_loaders(trainset, testset):\n",
    "    train_loader = DataLoader(IndexedDataset(trainset), batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    test_loader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Training functions =====\n",
    "def train_one_epoch(model, optimizer, train_loader, clean_labels_tensor, noisy_labels_tensor, lam, cached_g_value, global_step):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    epoch_cos = []\n",
    "    \n",
    "    for inputs, _, indices in train_loader:\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        indices = indices.to(device, non_blocking=True)\n",
    "        batch_noisy = noisy_labels_tensor[indices]\n",
    "        batch_clean = clean_labels_tensor[indices]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss_struct = criterion(outputs, batch_noisy)\n",
    "        loss_struct.backward(retain_graph=True)\n",
    "        g_struct = parameters_to_vector([p.grad for p in model.parameters()]).clone()\n",
    "        \n",
    "        if global_step % K == 0 or cached_g_value is None:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss_value = criterion(outputs, batch_clean)\n",
    "            loss_value.backward()\n",
    "            cached_g_value = parameters_to_vector([p.grad for p in model.parameters()]).clone()\n",
    "        \n",
    "        g_struct_norm = g_struct / (g_struct.norm() + 1e-12)\n",
    "        g_value_norm = cached_g_value / (cached_g_value.norm() + 1e-12)\n",
    "        \n",
    "        cos_sim = (g_struct_norm @ g_value_norm).item()\n",
    "        epoch_cos.append(cos_sim)\n",
    "        \n",
    "        g_mix = (1 - lam) * g_struct_norm + lam * g_value_norm\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        idx = 0\n",
    "        for p in model.parameters():\n",
    "            numel = p.numel()\n",
    "            p.grad = g_mix[idx:idx+numel].view(p.shape).clone()\n",
    "            idx += numel\n",
    "        optimizer.step()\n",
    "        global_step += 1\n",
    "    \n",
    "    return cached_g_value, global_step, np.mean(epoch_cos)\n",
    "\n",
    "def find_spinodal_down(model, optimizer, train_loader, test_loader, clean_labels_tensor, noisy_labels_tensor, hold_epochs):\n",
    "    \"\"\"\n",
    "    Find Î»_â†“ (recovery spinodal) by sweeping Î» downward from collapse state.\n",
    "    Returns the Î» at which recovery is detected.\n",
    "    \"\"\"\n",
    "    cached_g_value = None\n",
    "    global_step = 0\n",
    "    global_epoch = 0\n",
    "    \n",
    "    history = []\n",
    "    recovery_counter = 0\n",
    "    recovery_lambda = None\n",
    "    \n",
    "    # Phase 1: Initialize in collapse state\n",
    "    print(f'    Phase 1: Establishing collapse state at Î»={INIT_LAMBDA}')\n",
    "    for ep in range(INIT_EPOCHS):\n",
    "        cached_g_value, global_step, avg_cos = train_one_epoch(\n",
    "            model, optimizer, train_loader, clean_labels_tensor, noisy_labels_tensor,\n",
    "            INIT_LAMBDA, cached_g_value, global_step\n",
    "        )\n",
    "        global_epoch += 1\n",
    "        \n",
    "        if (ep + 1) % 10 == 0:\n",
    "            acc = evaluate(model, test_loader)\n",
    "            print(f'      Epoch {ep+1}: error={1-acc:.4f}')\n",
    "    \n",
    "    init_acc = evaluate(model, test_loader)\n",
    "    print(f'    Initial state: error={1-init_acc:.4f}')\n",
    "    \n",
    "    # Phase 2: Sweep Î» downward\n",
    "    print(f'    Phase 2: Sweeping Î» from {LAMBDA_START} to {LAMBDA_END}')\n",
    "    current_lambda = LAMBDA_START\n",
    "    \n",
    "    while current_lambda >= LAMBDA_END:\n",
    "        # Train at current Î» for hold_epochs\n",
    "        for h in range(hold_epochs):\n",
    "            cached_g_value, global_step, avg_cos = train_one_epoch(\n",
    "                model, optimizer, train_loader, clean_labels_tensor, noisy_labels_tensor,\n",
    "                current_lambda, cached_g_value, global_step\n",
    "            )\n",
    "            global_epoch += 1\n",
    "        \n",
    "        # Evaluate\n",
    "        test_acc = evaluate(model, test_loader)\n",
    "        test_error = 1 - test_acc\n",
    "        \n",
    "        history.append({\n",
    "            'lambda': current_lambda,\n",
    "            'test_error': test_error,\n",
    "            'global_epoch': global_epoch\n",
    "        })\n",
    "        \n",
    "        # Check for recovery\n",
    "        if test_error < RECOVERY_THRESHOLD:\n",
    "            recovery_counter += 1\n",
    "            if recovery_counter >= RECOVERY_SUSTAIN:\n",
    "                recovery_lambda = current_lambda\n",
    "                print(f'    âœ… RECOVERY detected at Î»={recovery_lambda:.3f} (error={test_error:.4f})')\n",
    "                break\n",
    "        else:\n",
    "            recovery_counter = 0\n",
    "        \n",
    "        current_lambda += LAMBDA_STEP  # negative step\n",
    "        current_lambda = round(current_lambda, 3)\n",
    "    \n",
    "    if recovery_lambda is None:\n",
    "        recovery_lambda = LAMBDA_END  # Censored\n",
    "        print(f'    No recovery within range (censored at Î»={LAMBDA_END})')\n",
    "    \n",
    "    return recovery_lambda, history, init_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ãƒ‡ãƒ¼ã‚¿æº–å‚™ =====\n",
    "trainset, testset = load_cifar10()\n",
    "clean_labels = np.array(trainset.targets)\n",
    "train_loader, test_loader = get_data_loaders(trainset, testset)\n",
    "print('Data prepared')\n",
    "\n",
    "warmup_model = get_resnet18().to(device)\n",
    "for _ in range(20):\n",
    "    _ = warmup_model(torch.randn(BATCH_SIZE, 3, 32, 32, device=device))\n",
    "del warmup_model\n",
    "torch.cuda.empty_cache()\n",
    "print('GPU warmed up')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ãƒ¡ã‚¤ãƒ³å®Ÿé¨“ãƒ«ãƒ¼ãƒ— =====\n",
    "results = []\n",
    "checkpoint_file = f'{SAVE_DIR}/{NOTEBOOK_ID}_checkpoint.json'\n",
    "completed = set()\n",
    "\n",
    "if os.path.exists(checkpoint_file):\n",
    "    with open(checkpoint_file, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    for r in results:\n",
    "        completed.add((r['noise_rate'], r['hold_type'], r['seed']))\n",
    "    print(f'Checkpoint loaded: {len(results)} runs completed')\n",
    "\n",
    "total = len(experiments)\n",
    "\n",
    "for exp in experiments:\n",
    "    eta = exp['noise_rate']\n",
    "    hold_type = exp['hold_type']\n",
    "    hold_epochs = exp['hold_epochs']\n",
    "    seed = exp['seed']\n",
    "    \n",
    "    if (eta, hold_type, seed) in completed:\n",
    "        continue\n",
    "    \n",
    "    run_num = len(completed) + 1\n",
    "    print(f'\\n[{run_num}/{total}] Î·={eta} hold={hold_type} seed={seed}')\n",
    "    \n",
    "    set_seed(seed)\n",
    "    noisy_labels = inject_label_noise(clean_labels, eta, seed)\n",
    "    clean_labels_tensor = torch.tensor(clean_labels, device=device)\n",
    "    noisy_labels_tensor = torch.tensor(noisy_labels, device=device)\n",
    "    \n",
    "    model = get_resnet18().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    recovery_lambda, history, init_acc = find_spinodal_down(\n",
    "        model, optimizer, train_loader, test_loader,\n",
    "        clean_labels_tensor, noisy_labels_tensor, hold_epochs\n",
    "    )\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    result = {\n",
    "        'experiment_id': f'{NOTEBOOK_ID}-{run_num:03d}',\n",
    "        'experiment': EXP_NAME,\n",
    "        'notebook_id': NOTEBOOK_ID,\n",
    "        'direction': 'down',\n",
    "        'noise_rate': eta,\n",
    "        'hold_type': hold_type,\n",
    "        'hold_epochs': hold_epochs,\n",
    "        'seed': seed,\n",
    "        'spinodal_lambda': recovery_lambda,\n",
    "        'init_error': 1 - init_acc,\n",
    "        'time_seconds': elapsed,\n",
    "        'history': history\n",
    "    }\n",
    "    results.append(result)\n",
    "    completed.add((eta, hold_type, seed))\n",
    "    \n",
    "    print(f'  Î»_â†“ = {recovery_lambda:.3f} | Time: {elapsed/60:.1f} min')\n",
    "    \n",
    "    with open(checkpoint_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    remaining = total - run_num\n",
    "    eta_hours = remaining * elapsed / 3600\n",
    "    print(f'  Progress: {run_num}/{total} | ETA: {eta_hours:.1f} hours')\n",
    "    \n",
    "    del model, optimizer\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print(f'{NOTEBOOK_ID} COMPLETED!')\n",
    "print('='*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== çµæœä¿å­˜ã¨åˆ†æ =====\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open(f'{SAVE_DIR}/{NOTEBOOK_ID}_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "df = pd.DataFrame([{k: v for k, v in r.items() if k != 'history'} for r in results])\n",
    "df.to_csv(f'{SAVE_DIR}/{NOTEBOOK_ID}_results.csv', index=False)\n",
    "\n",
    "print('\\n--- Spinodal Î»_â†“ Summary ---')\n",
    "for eta in NOISE_RATES:\n",
    "    for hold_type in HOLD_TYPES:\n",
    "        subset = df[(df['noise_rate'] == eta) & (df['hold_type'] == hold_type)]\n",
    "        if len(subset) > 0:\n",
    "            mean_sp = subset['spinodal_lambda'].mean()\n",
    "            std_sp = subset['spinodal_lambda'].std()\n",
    "            print(f'  Î·={eta}, {hold_type}: Î»_â†“ = {mean_sp:.3f} Â± {std_sp:.3f}')\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "for i, eta in enumerate(NOISE_RATES):\n",
    "    ax = axes[i]\n",
    "    for hold_type, color in [('short', 'blue'), ('long', 'red')]:\n",
    "        subset = df[(df['noise_rate'] == eta) & (df['hold_type'] == hold_type)]\n",
    "        if len(subset) > 0:\n",
    "            ax.scatter(subset['seed'], subset['spinodal_lambda'], \n",
    "                      c=color, label=hold_type, alpha=0.7, s=50)\n",
    "            ax.axhline(subset['spinodal_lambda'].mean(), color=color, linestyle='--', alpha=0.5)\n",
    "    \n",
    "    ax.set_xlabel('Seed', fontsize=12)\n",
    "    ax.set_ylabel('Î»_â†“ (Recovery Spinodal)', fontsize=12)\n",
    "    ax.set_title(f'Î·={eta}: Recovery Spinodal Distribution', fontsize=12)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR}/figures/{NOTEBOOK_ID}_spinodal_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nResults saved to {SAVE_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== R3a + R3b çµ±åˆåˆ†æï¼ˆä¸¡æ–¹å®Œäº†å¾Œã«å®Ÿè¡Œï¼‰=====\n",
    "print('='*70)\n",
    "print('SPINODAL COMPARISON (R3a + R3b)')\n",
    "print('='*70)\n",
    "\n",
    "# Load R3a results if available\n",
    "r3a_file = f'{SAVE_DIR}/R3a_results.csv'\n",
    "if os.path.exists(r3a_file):\n",
    "    df_r3a = pd.read_csv(r3a_file)\n",
    "    df_r3b = df\n",
    "    \n",
    "    print('\\n--- Hysteresis Width (Î»_â†‘ - Î»_â†“) ---')\n",
    "    for eta in NOISE_RATES:\n",
    "        for hold_type in HOLD_TYPES:\n",
    "            up_vals = df_r3a[(df_r3a['noise_rate'] == eta) & (df_r3a['hold_type'] == hold_type)]['spinodal_lambda']\n",
    "            down_vals = df_r3b[(df_r3b['noise_rate'] == eta) & (df_r3b['hold_type'] == hold_type)]['spinodal_lambda']\n",
    "            \n",
    "            if len(up_vals) > 0 and len(down_vals) > 0:\n",
    "                width = up_vals.mean() - down_vals.mean()\n",
    "                print(f'  Î·={eta}, {hold_type}: Î”Î» = {width:.3f} (Î»_â†‘={up_vals.mean():.3f}, Î»_â†“={down_vals.mean():.3f})')\n",
    "else:\n",
    "    print('R3a results not found. Run R3a first, then re-run this cell.')"
   ]
  }
 ],
 "metadata": {
  "kernelnel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
