{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp L: Causal Intervention - Value Alignment Controls Performance\n",
    "\n",
    "## 目的\n",
    "「価値整合度（clean方向へのcosθ）が性能を因果的に支配する」ことを証明する。\n",
    "\n",
    "## 核アイデア\n",
    "clean labels の勾配 g_clean を「真の価値方向」として定義し、\n",
    "この方向への整合度（cosθ）を直接制御した人工的な価値勾配を構成する。\n",
    "\n",
    "```\n",
    "g_value(c) = c·ĝ_clean + √(1-c²)·u\n",
    "```\n",
    "\n",
    "ここで：\n",
    "- ĝ_clean = clean labels の勾配（正規化）\n",
    "- c = cos_target（-0.9 〜 +0.9）\n",
    "- u = ĝ_clean に直交する単位ベクトル\n",
    "\n",
    "これにより cos(g_value, g_clean) = c が保証される。\n",
    "\n",
    "## exp_I との違い（重要）\n",
    "- exp_I: g_struct（noisy）を基準にcosθを制御 → 解釈が逆転\n",
    "- exp_L: g_clean（clean）を基準にcosθを制御 → 正しい因果介入\n",
    "\n",
    "## 実験設計\n",
    "- **ノイズ率**: η = 0.4\n",
    "- **λ**: 0.30, 0.35, 0.40（ordered領域）\n",
    "- **cos_target**: -0.9, -0.6, -0.3, 0.0, +0.3, +0.6, +0.9（7点）\n",
    "- **Seeds**: 0, 1, 2\n",
    "\n",
    "## Runs計算\n",
    "3 λ × 7 c × 3 seeds = **63 runs**\n",
    "\n",
    "## 期待される結果\n",
    "- c > 0（clean方向に整合）→ **性能改善**\n",
    "- c < 0（clean方向と逆）→ **性能悪化**\n",
    "- c = 0（cleanに直交）→ 中間\n",
    "\n",
    "→ **価値整合度が性能を直接支配する因果的証拠**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== セットアップ =====\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "EXP_NAME = 'exp_L_causal_intervention'\n",
    "TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "BASE_DIR = '/content/drive/MyDrive/dual-gradient-learning/Paper-A'\n",
    "SAVE_DIR = f'{BASE_DIR}/{EXP_NAME}_{TIMESTAMP}'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR}/figures', exist_ok=True)\n",
    "\n",
    "print(f'Experiment: {EXP_NAME}')\n",
    "print(f'Timestamp: {TIMESTAMP}')\n",
    "print(f'Save directory: {SAVE_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils import parameters_to_vector\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== モデル定義 =====\n",
    "class IndexedDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        return img, label, idx\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, 3, stride, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, 3, 1, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(planes))\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        return F.relu(out)\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.in_planes = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, 1, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(64, 2, 1)\n",
    "        self.layer2 = self._make_layer(128, 2, 2)\n",
    "        self.layer3 = self._make_layer(256, 2, 2)\n",
    "        self.layer4 = self._make_layer(512, 2, 2)\n",
    "        self.linear = nn.Linear(512, num_classes)\n",
    "    def _make_layer(self, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for s in strides:\n",
    "            layers.append(BasicBlock(self.in_planes, planes, s))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.adaptive_avg_pool2d(out, 1)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        return self.linear(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 実験パラメータ =====\n",
    "BATCH_SIZE = 256\n",
    "NUM_WORKERS = 4\n",
    "EPOCHS = 100\n",
    "LR = 0.1\n",
    "K = 16  # g_clean のキャッシュ更新頻度\n",
    "\n",
    "NOISE_RATE = 0.4\n",
    "LAMBDAS = [0.30, 0.35, 0.40]\n",
    "COSINES = [-0.9, -0.6, -0.3, 0.0, 0.3, 0.6, 0.9]\n",
    "SEEDS = [0, 1, 2]\n",
    "\n",
    "# 実験条件リスト生成\n",
    "experiments = []\n",
    "for lam in LAMBDAS:\n",
    "    for cos_target in COSINES:\n",
    "        for seed in SEEDS:\n",
    "            experiments.append({\n",
    "                'lambda': lam,\n",
    "                'cos_target': cos_target,\n",
    "                'seed': seed\n",
    "            })\n",
    "\n",
    "total_runs = len(experiments)\n",
    "print(f'Total runs: {total_runs}')\n",
    "print(f'Estimated time: {total_runs * 9.5 / 60:.1f} hours')\n",
    "\n",
    "# config保存\n",
    "config = {\n",
    "    'experiment': EXP_NAME,\n",
    "    'timestamp': TIMESTAMP,\n",
    "    'description': 'Causal intervention: control alignment to g_clean (NOT g_struct)',\n",
    "    'parameters': {\n",
    "        'lambdas': LAMBDAS,\n",
    "        'cosines': COSINES,\n",
    "        'seeds': SEEDS,\n",
    "        'noise_rate': NOISE_RATE,\n",
    "        'epochs': EPOCHS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'lr': LR,\n",
    "        'K': K\n",
    "    },\n",
    "    'total_runs': total_runs\n",
    "}\n",
    "with open(f'{SAVE_DIR}/exp_L_config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print(f'Config saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ユーティリティ関数 =====\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def load_cifar10():\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "    return trainset, testset\n",
    "\n",
    "def get_data_loaders(trainset, testset):\n",
    "    indexed_trainset = IndexedDataset(trainset)\n",
    "    train_loader = DataLoader(indexed_trainset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                              num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, drop_last=True)\n",
    "    test_loader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                             num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def inject_noise(labels, noise_rate, seed):\n",
    "    np.random.seed(seed + 1000)\n",
    "    noisy_labels = labels.copy()\n",
    "    n_noisy = int(noise_rate * len(labels))\n",
    "    noisy_indices = np.random.choice(len(labels), n_noisy, replace=False)\n",
    "    for idx in noisy_indices:\n",
    "        choices = [i for i in range(10) if i != labels[idx]]\n",
    "        noisy_labels[idx] = np.random.choice(choices)\n",
    "    return noisy_labels\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 核心: g_clean を基準に cosθ を制御する価値勾配の構成 =====\n",
    "def construct_value_gradient_aligned_to_clean(g_clean, cos_target):\n",
    "    \"\"\"\n",
    "    clean勾配 g_clean を基準に、cos(g_value, g_clean) = cos_target となる\n",
    "    人工的な価値勾配を構成する。\n",
    "    \n",
    "    g_value(c) = c·ĝ_clean + √(1-c²)·u\n",
    "    \n",
    "    ここで：\n",
    "    - ĝ_clean = g_clean の正規化\n",
    "    - c = cos_target\n",
    "    - u = ĝ_clean に直交する単位ベクトル\n",
    "    \n",
    "    【重要】exp_I との違い:\n",
    "    - exp_I: g_struct（noisy方向）を基準 → 解釈が逆転\n",
    "    - exp_L: g_clean（価値方向）を基準 → 正しい因果介入\n",
    "    \"\"\"\n",
    "    # clean勾配を正規化\n",
    "    g_clean_norm = g_clean / (g_clean.norm() + 1e-12)\n",
    "    \n",
    "    # ランダムベクトルを生成し、g_cleanに直交する成分を抽出\n",
    "    random_vec = torch.randn_like(g_clean)\n",
    "    \n",
    "    # Gram-Schmidt直交化\n",
    "    proj = (random_vec @ g_clean_norm) * g_clean_norm\n",
    "    orthogonal = random_vec - proj\n",
    "    u = orthogonal / (orthogonal.norm() + 1e-12)\n",
    "    \n",
    "    # 目標cosineを持つ価値勾配を構成\n",
    "    cos_target_clamped = max(-0.999, min(0.999, cos_target))\n",
    "    sin_component = np.sqrt(1 - cos_target_clamped**2)\n",
    "    \n",
    "    g_value = cos_target_clamped * g_clean_norm + sin_component * u\n",
    "    \n",
    "    # 検証: 実際のcosine（g_cleanとの角度）を計算\n",
    "    actual_cos_to_clean = (g_clean_norm @ g_value).item()\n",
    "    \n",
    "    return g_value, actual_cos_to_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 学習関数: 正しい因果介入版 =====\n",
    "def train_with_causal_intervention(model, train_loader, test_loader, \n",
    "                                    clean_labels, noisy_labels, lam, cos_target):\n",
    "    \"\"\"\n",
    "    g_clean（価値方向）への整合度を直接制御したdual-gradient learning。\n",
    "    \n",
    "    【重要な設計】\n",
    "    1. g_struct = noisy labels からの勾配（データに従う力）\n",
    "    2. g_clean = clean labels からの勾配（真の価値方向）\n",
    "    3. g_value = g_clean 方向への整合度が cos_target になるよう人工構成\n",
    "    4. g_mix = (1-λ) * g_struct + λ * g_value\n",
    "    \n",
    "    cos_target > 0 → g_value は g_clean に近い → 価値整合 → 改善期待\n",
    "    cos_target < 0 → g_value は g_clean と逆 → 価値非整合 → 悪化期待\n",
    "    \"\"\"\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 75], gamma=0.1)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    clean_labels_tensor = torch.tensor(clean_labels, device=device)\n",
    "    noisy_labels_tensor = torch.tensor(noisy_labels, device=device)\n",
    "    \n",
    "    cached_g_clean = None\n",
    "    global_step = 0\n",
    "    \n",
    "    best_acc = 0\n",
    "    cos_to_clean_history = []  # g_value と g_clean の実際の角度\n",
    "    cos_struct_clean_history = []  # g_struct と g_clean の実際の角度（参考）\n",
    "    history = {'epoch': [], 'test_acc': [], 'test_error': []}\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        epoch_cos_to_clean = []\n",
    "        epoch_cos_struct_clean = []\n",
    "        \n",
    "        for inputs, _, indices in train_loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            indices = indices.to(device, non_blocking=True)\n",
    "            batch_noisy = noisy_labels_tensor[indices]\n",
    "            batch_clean = clean_labels_tensor[indices]\n",
    "            \n",
    "            # ===== Step 1: Structure gradient (from noisy labels) =====\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss_struct = criterion(outputs, batch_noisy)\n",
    "            loss_struct.backward(retain_graph=True)\n",
    "            g_struct = parameters_to_vector([p.grad for p in model.parameters()]).clone()\n",
    "            \n",
    "            # ===== Step 2: Clean gradient (価値の基準方向) =====\n",
    "            if global_step % K == 0 or cached_g_clean is None:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss_clean = criterion(outputs, batch_clean)\n",
    "                loss_clean.backward()\n",
    "                cached_g_clean = parameters_to_vector([p.grad for p in model.parameters()]).clone()\n",
    "            \n",
    "            # ===== Step 3: g_clean を基準に cos_target の角度を持つ g_value を構成 =====\n",
    "            g_value, actual_cos_to_clean = construct_value_gradient_aligned_to_clean(\n",
    "                cached_g_clean, cos_target\n",
    "            )\n",
    "            epoch_cos_to_clean.append(actual_cos_to_clean)\n",
    "            \n",
    "            # 参考: g_struct と g_clean の角度も記録\n",
    "            g_struct_norm = g_struct / (g_struct.norm() + 1e-12)\n",
    "            g_clean_norm = cached_g_clean / (cached_g_clean.norm() + 1e-12)\n",
    "            cos_struct_clean = (g_struct_norm @ g_clean_norm).item()\n",
    "            epoch_cos_struct_clean.append(cos_struct_clean)\n",
    "            \n",
    "            # ===== Step 4: Normalize and mix =====\n",
    "            g_value_norm = g_value / (g_value.norm() + 1e-12)\n",
    "            g_mix = (1 - lam) * g_struct_norm + lam * g_value_norm\n",
    "            \n",
    "            # Apply mixed gradient\n",
    "            optimizer.zero_grad()\n",
    "            idx = 0\n",
    "            for p in model.parameters():\n",
    "                numel = p.numel()\n",
    "                p.grad = g_mix[idx:idx+numel].view(p.shape).clone()\n",
    "                idx += numel\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "        \n",
    "        scheduler.step()\n",
    "        cos_to_clean_history.append(np.mean(epoch_cos_to_clean))\n",
    "        cos_struct_clean_history.append(np.mean(epoch_cos_struct_clean))\n",
    "        \n",
    "        # 10エポックごとに評価\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            acc = evaluate(model, test_loader)\n",
    "            best_acc = max(best_acc, acc)\n",
    "            history['epoch'].append(epoch + 1)\n",
    "            history['test_acc'].append(acc)\n",
    "            history['test_error'].append(1 - acc)\n",
    "    \n",
    "    final_acc = evaluate(model, test_loader)\n",
    "    avg_cos_to_clean = np.mean(cos_to_clean_history)\n",
    "    avg_cos_struct_clean = np.mean(cos_struct_clean_history)\n",
    "    \n",
    "    return final_acc, max(best_acc, final_acc), avg_cos_to_clean, avg_cos_struct_clean, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== データ準備 =====\n",
    "trainset, testset = load_cifar10()\n",
    "clean_labels = np.array(trainset.targets)\n",
    "train_loader, test_loader = get_data_loaders(trainset, testset)\n",
    "\n",
    "print('Verifying IndexedDataset...')\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f'  ✓ Batch: images {list(sample_batch[0].shape)}, labels {list(sample_batch[1].shape)}, indices {list(sample_batch[2].shape)}')\n",
    "\n",
    "# GPU warmup\n",
    "warmup_model = ResNet18().to(device)\n",
    "for _ in range(20):\n",
    "    _ = warmup_model(torch.randn(BATCH_SIZE, 3, 32, 32, device=device))\n",
    "del warmup_model\n",
    "torch.cuda.empty_cache()\n",
    "print('Warmup complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== メイン実験ループ =====\n",
    "results = []\n",
    "checkpoint_file = f'{SAVE_DIR}/exp_L_checkpoint.json'\n",
    "completed = set()\n",
    "\n",
    "# チェックポイント復元\n",
    "if os.path.exists(checkpoint_file):\n",
    "    with open(checkpoint_file, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    for r in results:\n",
    "        completed.add((r['lambda'], r['cos_target'], r['seed']))\n",
    "    print(f'Checkpoint loaded: {len(completed)} runs completed')\n",
    "\n",
    "# ノイズラベル生成（seed=0で固定、実験間で同じノイズパターン）\n",
    "noisy_labels = inject_noise(clean_labels, NOISE_RATE, seed=0)\n",
    "print(f'Noise injected: {np.mean(noisy_labels != clean_labels)*100:.1f}% corrupted')\n",
    "\n",
    "run_counter = 0\n",
    "exp_start = time.time()\n",
    "\n",
    "for exp in experiments:\n",
    "    run_counter += 1\n",
    "    lam = exp['lambda']\n",
    "    cos_target = exp['cos_target']\n",
    "    seed = exp['seed']\n",
    "    \n",
    "    key = (lam, cos_target, seed)\n",
    "    if key in completed:\n",
    "        continue\n",
    "    \n",
    "    print(f'\\n[{run_counter}/{total_runs}] λ={lam:.2f} cos_target={cos_target:+.1f} seed={seed}')\n",
    "    t0 = time.time()\n",
    "    \n",
    "    set_seed(seed)\n",
    "    model = ResNet18().to(device)\n",
    "    \n",
    "    final_acc, best_acc, avg_cos_to_clean, avg_cos_struct_clean, history = train_with_causal_intervention(\n",
    "        model, train_loader, test_loader,\n",
    "        clean_labels, noisy_labels, lam, cos_target\n",
    "    )\n",
    "    elapsed = time.time() - t0\n",
    "    \n",
    "    result = {\n",
    "        'experiment_id': f'L-{run_counter:03d}',\n",
    "        'experiment': EXP_NAME,\n",
    "        'lambda': lam,\n",
    "        'cos_target': cos_target,\n",
    "        'cos_actual_to_clean': avg_cos_to_clean,\n",
    "        'cos_struct_clean': avg_cos_struct_clean,\n",
    "        'seed': seed,\n",
    "        'noise_rate': NOISE_RATE,\n",
    "        'test_acc': final_acc,\n",
    "        'test_error': 1 - final_acc,\n",
    "        'best_acc': best_acc,\n",
    "        'best_error': 1 - best_acc,\n",
    "        'time_seconds': elapsed,\n",
    "        'history': history\n",
    "    }\n",
    "    results.append(result)\n",
    "    \n",
    "    # チェックポイント保存\n",
    "    with open(checkpoint_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    # ステータス表示\n",
    "    status = ''\n",
    "    if cos_target > 0 and (1 - final_acc) < 0.15:\n",
    "        status = ' ✅ IMPROVED (expected for cos>0)'\n",
    "    elif cos_target < 0 and (1 - final_acc) > 0.25:\n",
    "        status = ' ⚠️ DEGRADED (expected for cos<0)'\n",
    "    \n",
    "    print(f'  Error: {1-final_acc:.4f} | cos_to_clean: {avg_cos_to_clean:.4f} | Time: {elapsed/60:.1f} min{status}')\n",
    "    \n",
    "    # 進捗推定\n",
    "    completed_count = len(results)\n",
    "    avg_time = sum([r['time_seconds'] for r in results]) / completed_count\n",
    "    remaining = total_runs - completed_count\n",
    "    eta_hours = (remaining * avg_time) / 3600\n",
    "    print(f'  Progress: {completed_count}/{total_runs} | ETA: {eta_hours:.1f} hours')\n",
    "\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'EXPERIMENT COMPLETE')\n",
    "print(f'Total time: {(time.time()-exp_start)/3600:.2f} hours')\n",
    "print(f'{\"=\"*60}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 結果保存 =====\n",
    "import pandas as pd\n",
    "\n",
    "with open(f'{SAVE_DIR}/exp_L_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "results_flat = [{k: v for k, v in r.items() if k != 'history'} for r in results]\n",
    "df = pd.DataFrame(results_flat)\n",
    "df.to_csv(f'{SAVE_DIR}/exp_L_results.csv', index=False)\n",
    "\n",
    "print(f'Results saved to {SAVE_DIR}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 因果分析 =====\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame([{k: v for k, v in r.items() if k != 'history'} for r in results])\n",
    "\n",
    "print('='*70)\n",
    "print('CAUSAL ANALYSIS: cos(g_value, g_clean) → Performance')\n",
    "print('='*70)\n",
    "print('\\n【重要】cos_target > 0 は「価値（clean方向）に整合」を意味する')\n",
    "print('        → 期待: cos > 0 で改善、cos < 0 で悪化\\n')\n",
    "\n",
    "# λごとの分析\n",
    "for lam in LAMBDAS:\n",
    "    print(f'\\n--- λ = {lam} ---')\n",
    "    df_l = df[df['lambda'] == lam]\n",
    "    \n",
    "    stats = df_l.groupby('cos_target').agg({\n",
    "        'test_error': ['mean', 'std'],\n",
    "        'cos_actual_to_clean': 'mean'\n",
    "    }).round(4)\n",
    "    print(stats)\n",
    "    \n",
    "    # 相関計算\n",
    "    corr = df_l['cos_target'].corr(df_l['test_error'])\n",
    "    print(f'\\n  Correlation(cos_target, error): {corr:.4f}')\n",
    "    if corr < -0.7:\n",
    "        print('  → ✅ STRONG NEGATIVE: Higher alignment → Lower error (CAUSAL SUPPORT)')\n",
    "    elif corr > 0.7:\n",
    "        print('  → ⚠️ UNEXPECTED POSITIVE: Check design!')\n",
    "\n",
    "# 全体の相関\n",
    "print('\\n' + '='*70)\n",
    "print('OVERALL CORRELATION')\n",
    "print('='*70)\n",
    "overall_corr = df['cos_target'].corr(df['test_error'])\n",
    "print(f'Correlation(cos_target, test_error): {overall_corr:.4f}')\n",
    "\n",
    "if overall_corr < -0.5:\n",
    "    print('\\n✅ CAUSAL PROOF ESTABLISHED:')\n",
    "    print('   Value alignment (cos to g_clean) directly controls performance.')\n",
    "    print('   Higher alignment → Better generalization.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 可視化: cos_target vs Performance =====\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for i, lam in enumerate(LAMBDAS):\n",
    "    ax = axes[i]\n",
    "    df_l = df[df['lambda'] == lam]\n",
    "    \n",
    "    stats = df_l.groupby('cos_target')['test_error'].agg(['mean', 'std'])\n",
    "    \n",
    "    ax.errorbar(stats.index, stats['mean'], yerr=stats['std'],\n",
    "                marker='o', capsize=4, linewidth=2, markersize=8, color='C0')\n",
    "    \n",
    "    # 参照線\n",
    "    ax.axvline(x=0, color='gray', linestyle=':', alpha=0.5, label='Orthogonal')\n",
    "    \n",
    "    # 期待される傾向を示す矢印\n",
    "    ax.annotate('', xy=(0.8, stats['mean'].min()), xytext=(0.8, stats['mean'].max()),\n",
    "                arrowprops=dict(arrowstyle='->', color='green', lw=2))\n",
    "    ax.text(0.85, (stats['mean'].min() + stats['mean'].max())/2, 'Expected\\nimprovement',\n",
    "            fontsize=9, color='green', va='center')\n",
    "    \n",
    "    ax.set_xlabel('cos(g_value, g_clean) - Target', fontsize=12)\n",
    "    ax.set_ylabel('Test Error', fontsize=12)\n",
    "    ax.set_title(f'Causal Intervention: λ = {lam}', fontsize=13, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.set_xlim([-1, 1])\n",
    "\n",
    "plt.suptitle('Value Alignment (to g_clean) Controls Performance', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR}/figures/exp_L_causal_intervention.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Figure saved: {SAVE_DIR}/figures/exp_L_causal_intervention.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 可視化: λ × cos_target ヒートマップ =====\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ピボットテーブル作成\n",
    "pivot = df.pivot_table(values='test_error', index='cos_target', columns='lambda', aggfunc='mean')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "im = ax.imshow(pivot.values, cmap='RdYlGn_r', aspect='auto',\n",
    "               extent=[min(LAMBDAS)-0.025, max(LAMBDAS)+0.025, min(COSINES)-0.15, max(COSINES)+0.15])\n",
    "\n",
    "ax.set_xlabel('λ', fontsize=12)\n",
    "ax.set_ylabel('cos(g_value, g_clean)', fontsize=12)\n",
    "ax.set_title('Phase Diagram: λ × Value Alignment → Test Error', fontsize=13, fontweight='bold')\n",
    "\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label('Test Error', fontsize=11)\n",
    "\n",
    "# 注釈\n",
    "ax.axhline(y=0, color='white', linestyle='--', linewidth=1, alpha=0.7)\n",
    "ax.text(max(LAMBDAS)+0.03, 0.6, 'Aligned\\n(expected: good)', fontsize=9, va='center')\n",
    "ax.text(max(LAMBDAS)+0.03, -0.6, 'Anti-aligned\\n(expected: bad)', fontsize=9, va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR}/figures/exp_L_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Figure saved: {SAVE_DIR}/figures/exp_L_heatmap.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== exp_I との比較（もし両方の結果がある場合） =====\n",
    "print('\\n' + '='*70)\n",
    "print('COMPARISON: exp_I vs exp_L')\n",
    "print('='*70)\n",
    "print('\\nexp_I: cos_target は g_struct（noisy方向）との角度')\n",
    "print('       → cos > 0 は「ノイズ方向に従う」→ 悪化が期待される')\n",
    "print('\\nexp_L: cos_target は g_clean（価値方向）との角度')\n",
    "print('       → cos > 0 は「価値方向に整合」→ 改善が期待される')\n",
    "print('\\n【結論】因果介入実験では「何を基準にcosを定義するか」が決定的に重要')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== サマリー =====\n",
    "print('\\n' + '='*70)\n",
    "print('EXPERIMENT L: CAUSAL INTERVENTION - SUMMARY')\n",
    "print('='*70)\n",
    "\n",
    "# 主要な発見\n",
    "df_positive = df[df['cos_target'] > 0]['test_error'].mean()\n",
    "df_negative = df[df['cos_target'] < 0]['test_error'].mean()\n",
    "df_zero = df[df['cos_target'] == 0]['test_error'].mean()\n",
    "\n",
    "print(f'\\nMean Test Error by alignment region:')\n",
    "print(f'  cos > 0 (aligned to clean):     {df_positive:.4f}')\n",
    "print(f'  cos = 0 (orthogonal to clean):  {df_zero:.4f}')\n",
    "print(f'  cos < 0 (anti-aligned to clean): {df_negative:.4f}')\n",
    "\n",
    "print(f'\\nConclusion:')\n",
    "if df_positive < df_zero < df_negative:\n",
    "    print('  ✅ CAUSAL PROOF ESTABLISHED')\n",
    "    print('  ✅ Value alignment (cos to g_clean) directly controls performance')\n",
    "    print('  ✅ Higher alignment → Better generalization')\n",
    "    print('  ✅ This is the expected result for a correct causal intervention')\n",
    "else:\n",
    "    print('  ⚠️ Results require further analysis')\n",
    "\n",
    "print(f'\\nSave directory: {SAVE_DIR}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
