{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TŒ¥1: 90% Trapping Analysis\n",
    "\n",
    "**Purpose**: Identify early indicators that distinguish 90% trapped seeds from 40-55% collapse seeds\n",
    "\n",
    "**Protocol**:\n",
    "- Train multiple seeds at collapse-inducing Œª\n",
    "- Log detailed metrics throughout training\n",
    "- Compare trajectories of seeds that end up at 90% vs 40-55%\n",
    "\n",
    "**Key Metrics**:\n",
    "- cos(g_s, g_v): Structure-value alignment\n",
    "- ||g_s||, ||g_v||: Gradient norms\n",
    "- Error trajectory\n",
    "- Loss trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os, glob, json, time\n",
    "from datetime import datetime\n",
    "\n",
    "EXP_NAME = 'exp_Td1_90_analysis'\n",
    "NOTEBOOK_ID = 'Td1'\n",
    "BASE_DIR = '/content/drive/MyDrive/dual-gradient-learning/Paper-A'\n",
    "\n",
    "existing = glob.glob(f'{BASE_DIR}/{EXP_NAME}_*')\n",
    "if existing:\n",
    "    SAVE_DIR = sorted(existing)[-1]\n",
    "    print(f'üîÑ Resuming: {SAVE_DIR}')\n",
    "else:\n",
    "    TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    SAVE_DIR = f'{BASE_DIR}/{EXP_NAME}_{TIMESTAMP}'\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    print(f'üÜï New: {SAVE_DIR}')\n",
    "\n",
    "os.makedirs(f'{SAVE_DIR}/figures', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils import parameters_to_vector\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "BATCH_SIZE = 256\n",
    "NUM_WORKERS = 4\n",
    "LR = 0.1\n",
    "K = 16\n",
    "NOISE_RATE = 0.4\n",
    "\n",
    "# Training for collapse\n",
    "COLLAPSE_LAMBDA = 0.60\n",
    "N_EPOCHS = 100\n",
    "LOG_FREQ = 20  # Log every N steps\n",
    "EVAL_FREQ = 5  # Evaluate every N epochs\n",
    "\n",
    "# More seeds to capture both 90% and 40-55% outcomes\n",
    "N_SEEDS = 10\n",
    "\n",
    "# Classification thresholds\n",
    "TRAPPED_THRESHOLD = 0.85  # >85% = trapped (90% class)\n",
    "PARTIAL_THRESHOLD = 0.55  # 40-55% = partial collapse\n",
    "\n",
    "print(f'Œª = {COLLAPSE_LAMBDA}')\n",
    "print(f'Seeds: {N_SEEDS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet18():\n",
    "    model = resnet18(weights=None, num_classes=10)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "    return model\n",
    "\n",
    "class IndexedDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        return img, label, idx\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def inject_label_noise(labels, noise_rate, seed):\n",
    "    np.random.seed(seed)\n",
    "    noisy = labels.copy()\n",
    "    n_noisy = int(noise_rate * len(labels))\n",
    "    idx = np.random.choice(len(labels), n_noisy, replace=False)\n",
    "    for i in idx:\n",
    "        noisy[i] = np.random.choice([l for l in range(10) if l != labels[i]])\n",
    "    return noisy\n",
    "\n",
    "def load_cifar10():\n",
    "    tr = transforms.Compose([transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(),\n",
    "                             transforms.ToTensor(), transforms.Normalize((0.4914,0.4822,0.4465),(0.2023,0.1994,0.2010))])\n",
    "    te = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914,0.4822,0.4465),(0.2023,0.1994,0.2010))])\n",
    "    return torchvision.datasets.CIFAR10('./data', True, tr, download=True), torchvision.datasets.CIFAR10('./data', False, te, download=True)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            correct += (model(x).argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return correct / total\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    return (a @ b / (a.norm() * b.norm() + 1e-12)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_logging(model, train_loader, opt, clean_t, noisy_t, lam, state, log_list):\n",
    "    \"\"\"Train one epoch with detailed logging\"\"\"\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    step = state['step']\n",
    "    cached_gv = state['gv']\n",
    "    epoch = state['epoch']\n",
    "    \n",
    "    for x, _, idx in train_loader:\n",
    "        x, idx = x.to(device), idx.to(device)\n",
    "        bn, bc = noisy_t[idx], clean_t[idx]\n",
    "        \n",
    "        # g_struct\n",
    "        opt.zero_grad()\n",
    "        loss_s = crit(model(x), bn)\n",
    "        loss_s.backward(retain_graph=True)\n",
    "        g_s = parameters_to_vector([p.grad for p in model.parameters()]).clone()\n",
    "        \n",
    "        # g_value (always compute for logging)\n",
    "        opt.zero_grad()\n",
    "        loss_v = crit(model(x), bc)\n",
    "        loss_v.backward()\n",
    "        g_v = parameters_to_vector([p.grad for p in model.parameters()]).clone()\n",
    "        \n",
    "        if step % K == 0 or cached_gv is None:\n",
    "            cached_gv = g_v.clone()\n",
    "        \n",
    "        # Normalize and mix\n",
    "        g_s_n = g_s / (g_s.norm() + 1e-12)\n",
    "        g_v_n = cached_gv / (cached_gv.norm() + 1e-12)\n",
    "        g_mix = (1 - lam) * g_s_n + lam * g_v_n\n",
    "        \n",
    "        # Log\n",
    "        if step % LOG_FREQ == 0:\n",
    "            log_list.append({\n",
    "                'epoch': epoch,\n",
    "                'step': step,\n",
    "                'c_sv': cosine_sim(g_s, g_v),\n",
    "                'c_mc': cosine_sim(g_mix, g_v),\n",
    "                'norm_s': g_s.norm().item(),\n",
    "                'norm_v': g_v.norm().item(),\n",
    "                'norm_mix': g_mix.norm().item(),\n",
    "                'loss_s': loss_s.item(),\n",
    "                'loss_v': loss_v.item()\n",
    "            })\n",
    "        \n",
    "        # Apply\n",
    "        opt.zero_grad()\n",
    "        i = 0\n",
    "        for p in model.parameters():\n",
    "            n = p.numel()\n",
    "            p.grad = g_mix[i:i+n].view(p.shape).clone()\n",
    "            i += n\n",
    "        opt.step()\n",
    "        step += 1\n",
    "    \n",
    "    state['step'] = step\n",
    "    state['gv'] = cached_gv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = load_cifar10()\n",
    "clean_labels = np.array(trainset.targets)\n",
    "train_loader = DataLoader(IndexedDataset(trainset), BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader = DataLoader(testset, BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "m = get_resnet18().to(device)\n",
    "for _ in range(10): _ = m(torch.randn(BATCH_SIZE,3,32,32,device=device))\n",
    "del m; torch.cuda.empty_cache()\n",
    "print('Ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "\n",
    "for seed in range(N_SEEDS):\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'SEED {seed}')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    set_seed(seed)\n",
    "    noisy_labels = inject_label_noise(clean_labels, NOISE_RATE, seed)\n",
    "    clean_t = torch.tensor(clean_labels, device=device)\n",
    "    noisy_t = torch.tensor(noisy_labels, device=device)\n",
    "    \n",
    "    model = get_resnet18().to(device)\n",
    "    opt = optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "    sched = optim.lr_scheduler.MultiStepLR(opt, [50, 80], gamma=0.1)\n",
    "    \n",
    "    state = {'step': 0, 'gv': None, 'epoch': 0}\n",
    "    step_logs = []\n",
    "    error_trajectory = []\n",
    "    \n",
    "    for ep in range(N_EPOCHS):\n",
    "        state['epoch'] = ep\n",
    "        train_with_logging(model, train_loader, opt, clean_t, noisy_t, COLLAPSE_LAMBDA, state, step_logs)\n",
    "        sched.step()\n",
    "        \n",
    "        if (ep + 1) % EVAL_FREQ == 0:\n",
    "            err = 1 - evaluate(model, test_loader)\n",
    "            error_trajectory.append({'epoch': ep + 1, 'error': err})\n",
    "            print(f'  Epoch {ep+1}: error={err:.4f}')\n",
    "    \n",
    "    final_error = 1 - evaluate(model, test_loader)\n",
    "    \n",
    "    # Classify\n",
    "    if final_error >= TRAPPED_THRESHOLD:\n",
    "        category = 'trapped_90'\n",
    "        print(f'  üî¥ TRAPPED (90%): {final_error:.4f}')\n",
    "    elif final_error >= PARTIAL_THRESHOLD:\n",
    "        category = 'partial_collapse'\n",
    "        print(f'  üü° PARTIAL COLLAPSE (40-55%): {final_error:.4f}')\n",
    "    else:\n",
    "        category = 'ordered'\n",
    "        print(f'  üü¢ ORDERED: {final_error:.4f}')\n",
    "    \n",
    "    all_results.append({\n",
    "        'seed': seed,\n",
    "        'final_error': final_error,\n",
    "        'category': category,\n",
    "        'error_trajectory': error_trajectory,\n",
    "        'step_logs': step_logs,\n",
    "        'experiment_id': f'{NOTEBOOK_ID}-seed{seed:02d}'\n",
    "    })\n",
    "    \n",
    "    del model; torch.cuda.empty_cache()\n",
    "\n",
    "json.dump(all_results, open(f'{SAVE_DIR}/{NOTEBOOK_ID}_results.json', 'w'), indent=2, default=str)\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'{NOTEBOOK_ID} COMPLETE')\n",
    "print(f'{\"=\"*60}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Separate by category\n",
    "trapped = [r for r in all_results if r['category'] == 'trapped_90']\n",
    "partial = [r for r in all_results if r['category'] == 'partial_collapse']\n",
    "ordered = [r for r in all_results if r['category'] == 'ordered']\n",
    "\n",
    "print(f'\\nüìä Classification:')\n",
    "print(f'   Trapped (90%): {len(trapped)}')\n",
    "print(f'   Partial (40-55%): {len(partial)}')\n",
    "print(f'   Ordered: {len(ordered)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create step-level DataFrames for each category\n",
    "def create_step_df(results_list, category_name):\n",
    "    data = []\n",
    "    for r in results_list:\n",
    "        for log in r['step_logs']:\n",
    "            data.append({\n",
    "                'seed': r['seed'],\n",
    "                'category': category_name,\n",
    "                **log\n",
    "            })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df_trapped = create_step_df(trapped, 'trapped_90') if trapped else pd.DataFrame()\n",
    "df_partial = create_step_df(partial, 'partial_collapse') if partial else pd.DataFrame()\n",
    "df_ordered = create_step_df(ordered, 'ordered') if ordered else pd.DataFrame()\n",
    "\n",
    "df_all = pd.concat([df_trapped, df_partial, df_ordered], ignore_index=True)\n",
    "df_all.to_csv(f'{SAVE_DIR}/{NOTEBOOK_ID}_step_logs.csv', index=False)\n",
    "\n",
    "print(f'Total step logs: {len(df_all)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Compare trapped vs partial\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Color scheme\n",
    "colors = {'trapped_90': 'red', 'partial_collapse': 'orange', 'ordered': 'blue'}\n",
    "labels = {'trapped_90': 'Trapped (90%)', 'partial_collapse': 'Partial (40-55%)', 'ordered': 'Ordered'}\n",
    "\n",
    "# 1. Error trajectory\n",
    "ax = axes[0, 0]\n",
    "for r in all_results:\n",
    "    epochs = [t['epoch'] for t in r['error_trajectory']]\n",
    "    errors = [t['error'] for t in r['error_trajectory']]\n",
    "    ax.plot(epochs, errors, 'o-', color=colors[r['category']], alpha=0.5, linewidth=1.5)\n",
    "\n",
    "ax.axhline(0.90, color='red', linestyle='--', alpha=0.3, label='90%')\n",
    "ax.axhline(0.55, color='orange', linestyle='--', alpha=0.3, label='55%')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Test Error')\n",
    "ax.set_title('Error Trajectory')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. c_sv over time\n",
    "ax = axes[0, 1]\n",
    "for cat, df in [('trapped_90', df_trapped), ('partial_collapse', df_partial)]:\n",
    "    if len(df) > 0:\n",
    "        mean_by_epoch = df.groupby('epoch')['c_sv'].mean()\n",
    "        ax.plot(mean_by_epoch.index, mean_by_epoch.values, 'o-', \n",
    "                color=colors[cat], linewidth=2, label=labels[cat])\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('cos(g_s, g_v)')\n",
    "ax.set_title('Structure-Value Alignment')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. c_mc over time\n",
    "ax = axes[0, 2]\n",
    "for cat, df in [('trapped_90', df_trapped), ('partial_collapse', df_partial)]:\n",
    "    if len(df) > 0:\n",
    "        mean_by_epoch = df.groupby('epoch')['c_mc'].mean()\n",
    "        ax.plot(mean_by_epoch.index, mean_by_epoch.values, 'o-',\n",
    "                color=colors[cat], linewidth=2, label=labels[cat])\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('cos(g_mix, g_clean)')\n",
    "ax.set_title('Mix-Clean Alignment')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. norm_s over time\n",
    "ax = axes[1, 0]\n",
    "for cat, df in [('trapped_90', df_trapped), ('partial_collapse', df_partial)]:\n",
    "    if len(df) > 0:\n",
    "        mean_by_epoch = df.groupby('epoch')['norm_s'].mean()\n",
    "        ax.plot(mean_by_epoch.index, mean_by_epoch.values, 'o-',\n",
    "                color=colors[cat], linewidth=2, label=labels[cat])\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('||g_struct||')\n",
    "ax.set_title('Structure Gradient Norm')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. loss_v over time\n",
    "ax = axes[1, 1]\n",
    "for cat, df in [('trapped_90', df_trapped), ('partial_collapse', df_partial)]:\n",
    "    if len(df) > 0:\n",
    "        mean_by_epoch = df.groupby('epoch')['loss_v'].mean()\n",
    "        ax.plot(mean_by_epoch.index, mean_by_epoch.values, 'o-',\n",
    "                color=colors[cat], linewidth=2, label=labels[cat])\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss (clean labels)')\n",
    "ax.set_title('Clean Loss')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Early divergence detection\n",
    "ax = axes[1, 2]\n",
    "# Check early epochs (0-20) for divergence\n",
    "early_epochs = [0, 5, 10, 15, 20]\n",
    "for cat, df in [('trapped_90', df_trapped), ('partial_collapse', df_partial)]:\n",
    "    if len(df) > 0:\n",
    "        early_c_sv = df[df['epoch'].isin(early_epochs)].groupby('epoch')['c_sv'].mean()\n",
    "        ax.plot(early_c_sv.index, early_c_sv.values, 'o-',\n",
    "                color=colors[cat], linewidth=2, markersize=8, label=labels[cat])\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('cos(g_s, g_v)')\n",
    "ax.set_title('Early Divergence (epochs 0-20)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR}/figures/{NOTEBOOK_ID}_analysis.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical comparison at early epochs\n",
    "print('\\n' + '='*60)\n",
    "print('EARLY INDICATOR ANALYSIS')\n",
    "print('='*60)\n",
    "\n",
    "for epoch_check in [5, 10, 20]:\n",
    "    print(f'\\nüìä At epoch {epoch_check}:')\n",
    "    \n",
    "    if len(df_trapped) > 0 and len(df_partial) > 0:\n",
    "        trap_at_ep = df_trapped[df_trapped['epoch'] == epoch_check]\n",
    "        part_at_ep = df_partial[df_partial['epoch'] == epoch_check]\n",
    "        \n",
    "        if len(trap_at_ep) > 0 and len(part_at_ep) > 0:\n",
    "            for metric in ['c_sv', 'c_mc', 'norm_s', 'loss_v']:\n",
    "                trap_mean = trap_at_ep[metric].mean()\n",
    "                part_mean = part_at_ep[metric].mean()\n",
    "                diff = trap_mean - part_mean\n",
    "                print(f'   {metric}: Trapped={trap_mean:.4f}, Partial={part_mean:.4f}, Diff={diff:+.4f}')\n",
    "    else:\n",
    "        print('   Insufficient data for comparison')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print('\\n' + '='*60)\n",
    "print(f'{NOTEBOOK_ID} SUMMARY')\n",
    "print('='*60)\n",
    "\n",
    "print(f'\\nüìä Outcome Distribution:')\n",
    "print(f'   Trapped (90%):     {len(trapped)}/{N_SEEDS} ({len(trapped)/N_SEEDS*100:.0f}%)')\n",
    "print(f'   Partial (40-55%):  {len(partial)}/{N_SEEDS} ({len(partial)/N_SEEDS*100:.0f}%)')\n",
    "print(f'   Ordered (<40%):    {len(ordered)}/{N_SEEDS} ({len(ordered)/N_SEEDS*100:.0f}%)')\n",
    "\n",
    "print(f'\\nüìä Key Finding:')\n",
    "if len(df_trapped) > 0 and len(df_partial) > 0:\n",
    "    # Check if c_sv diverges early\n",
    "    early_trap_c_sv = df_trapped[df_trapped['epoch'] <= 10]['c_sv'].mean()\n",
    "    early_part_c_sv = df_partial[df_partial['epoch'] <= 10]['c_sv'].mean()\n",
    "    \n",
    "    if abs(early_trap_c_sv - early_part_c_sv) > 0.05:\n",
    "        print(f'   ‚úÖ Early divergence detected in c_sv')\n",
    "        print(f'      Trapped: {early_trap_c_sv:.4f}, Partial: {early_part_c_sv:.4f}')\n",
    "    else:\n",
    "        print(f'   ‚ö†Ô∏è No clear early divergence in c_sv')\n",
    "else:\n",
    "    print(f'   Need more data (both trapped and partial outcomes)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
