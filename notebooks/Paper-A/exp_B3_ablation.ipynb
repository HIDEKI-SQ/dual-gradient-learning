{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp B3: Ablation Study - Disentangling Direction vs Step Size\n",
    "\n",
    "## 目的\n",
    "ソフィアの指摘に基づき、以下を切り分ける：\n",
    "1. **g_mix再正規化**の効果（暗黙のLR低下を除去）\n",
    "2. **キャッシュK=1 vs K=16**の効果\n",
    "3. **真のAnti-aligned（-g_clean）**の効果\n",
    "\n",
    "### 実験条件\n",
    "- λ: 0.2, 0.3（最も効果が見える範囲）\n",
    "- Noise: 40%のみ\n",
    "- Seeds: 0, 1, 2\n",
    "\n",
    "### 条件（4種類）\n",
    "1. **Baseline**: Aligned + renorm=False + K=16（現状のexp_C相当）\n",
    "2. **+Renorm**: Aligned + renorm=True + K=16\n",
    "3. **+Renorm+NoCache**: Aligned + renorm=True + K=1\n",
    "4. **True Anti**: g_value = -g_clean + renorm=True + K=1\n",
    "\n",
    "### runs数: 24 runs (4条件 × 2λ × 3seeds)\n",
    "### 推定時間: ~3.5時間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "SAVE_DIR = '/content/drive/MyDrive/dual-gradient-results/exp_B3_ablation'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print(f'Save directory: {SAVE_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils import parameters_to_vector\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexedDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        return img, label, idx\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, 3, stride, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, 3, 1, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(planes))\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        return F.relu(out)\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.in_planes = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, 1, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(64, 2, 1)\n",
    "        self.layer2 = self._make_layer(128, 2, 2)\n",
    "        self.layer3 = self._make_layer(256, 2, 2)\n",
    "        self.layer4 = self._make_layer(512, 2, 2)\n",
    "        self.linear = nn.Linear(512, num_classes)\n",
    "    def _make_layer(self, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for s in strides:\n",
    "            layers.append(BasicBlock(self.in_planes, planes, s))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.adaptive_avg_pool2d(out, 1)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        return self.linear(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "NUM_WORKERS = 4\n",
    "EPOCHS = 100\n",
    "LR = 0.1\n",
    "NOISE_RATE = 0.4  # 固定\n",
    "\n",
    "LAMBDAS = [0.2, 0.3]\n",
    "SEEDS = [0, 1, 2]\n",
    "\n",
    "# 4つの実験条件\n",
    "CONDITIONS = [\n",
    "    {'name': 'Baseline',      'value_type': 'aligned', 'renorm': False, 'K': 16},\n",
    "    {'name': '+Renorm',       'value_type': 'aligned', 'renorm': True,  'K': 16},\n",
    "    {'name': '+Renorm+K1',    'value_type': 'aligned', 'renorm': True,  'K': 1},\n",
    "    {'name': 'TrueAnti+K1',   'value_type': 'anti',    'renorm': True,  'K': 1},\n",
    "]\n",
    "\n",
    "total_runs = len(CONDITIONS) * len(LAMBDAS) * len(SEEDS)\n",
    "print(f'Total runs: {total_runs}')\n",
    "print(f'Estimated time: {total_runs * 9.2 / 60:.1f} hours')\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def load_cifar10():\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "    return trainset, testset\n",
    "\n",
    "def get_data_loaders(trainset, testset):\n",
    "    indexed_trainset = IndexedDataset(trainset)\n",
    "    train_loader = DataLoader(indexed_trainset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                              num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, drop_last=True)\n",
    "    test_loader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                             num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def inject_noise(labels, noise_rate, seed):\n",
    "    np.random.seed(seed + 1000)\n",
    "    noisy_labels = labels.copy()\n",
    "    n_noisy = int(noise_rate * len(labels))\n",
    "    noisy_indices = np.random.choice(len(labels), n_noisy, replace=False)\n",
    "    for idx in noisy_indices:\n",
    "        choices = [i for i in range(10) if i != labels[idx]]\n",
    "        noisy_labels[idx] = np.random.choice(choices)\n",
    "    return noisy_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "def train_dual_gradient_ablation(model, train_loader, test_loader, \n",
    "                                  clean_labels, noisy_labels, lam,\n",
    "                                  value_type='aligned', renorm=False, K=16):\n",
    "    \"\"\"\n",
    "    切り分け実験用の統一訓練関数\n",
    "    \n",
    "    Args:\n",
    "        value_type: 'aligned' (g_clean) or 'anti' (-g_clean)\n",
    "        renorm: g_mixを再正規化するか\n",
    "        K: 勾配キャッシュの更新頻度（1=毎バッチ計算）\n",
    "    \"\"\"\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 75], gamma=0.1)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    clean_labels_tensor = torch.tensor(clean_labels, device=device)\n",
    "    noisy_labels_tensor = torch.tensor(noisy_labels, device=device)\n",
    "    \n",
    "    cached_value_grad = None\n",
    "    global_step = 0\n",
    "    best_acc = 0\n",
    "    \n",
    "    # ロギング用\n",
    "    cos_log = []\n",
    "    norm_log = []\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        for inputs, _, indices in train_loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            indices = indices.to(device, non_blocking=True)\n",
    "            batch_noisy = noisy_labels_tensor[indices]\n",
    "            batch_clean = clean_labels_tensor[indices]\n",
    "            \n",
    "            # ===== 構造勾配（ノイズラベルから）=====\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss_struct = criterion(outputs, batch_noisy)\n",
    "            loss_struct.backward(retain_graph=True)\n",
    "            g_struct = parameters_to_vector([p.grad for p in model.parameters()]).clone()\n",
    "            \n",
    "            # ===== 価値勾配（条件に応じて計算）=====\n",
    "            if global_step % K == 0 or cached_value_grad is None:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss_value = criterion(outputs, batch_clean)  # 真のラベルで計算\n",
    "                loss_value.backward()\n",
    "                g_clean = parameters_to_vector([p.grad for p in model.parameters()]).clone()\n",
    "                \n",
    "                if value_type == 'aligned':\n",
    "                    cached_value_grad = g_clean\n",
    "                elif value_type == 'anti':\n",
    "                    # ★★★ 真のAnti-aligned: -g_clean ★★★\n",
    "                    cached_value_grad = -g_clean\n",
    "            \n",
    "            # ===== 勾配の正規化と混合 =====\n",
    "            g_struct_norm = g_struct / (g_struct.norm() + 1e-12)\n",
    "            g_value_norm = cached_value_grad / (cached_value_grad.norm() + 1e-12)\n",
    "            g_mix = (1 - lam) * g_struct_norm + lam * g_value_norm\n",
    "            \n",
    "            # ★★★ 再正規化オプション ★★★\n",
    "            if renorm:\n",
    "                g_mix = g_mix / (g_mix.norm() + 1e-12)\n",
    "            \n",
    "            # ロギング（最初の100ステップのみ）\n",
    "            if global_step < 100:\n",
    "                cos_sim = F.cosine_similarity(g_struct_norm.unsqueeze(0), g_value_norm.unsqueeze(0)).item()\n",
    "                mix_norm = g_mix.norm().item()\n",
    "                cos_log.append(cos_sim)\n",
    "                norm_log.append(mix_norm)\n",
    "            \n",
    "            # パラメータ更新\n",
    "            optimizer.zero_grad()\n",
    "            idx = 0\n",
    "            for p in model.parameters():\n",
    "                numel = p.numel()\n",
    "                p.grad = g_mix[idx:idx+numel].view(p.shape).clone()\n",
    "                idx += numel\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "        \n",
    "        scheduler.step()\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            best_acc = max(best_acc, evaluate(model, test_loader))\n",
    "    \n",
    "    final_acc = evaluate(model, test_loader)\n",
    "    \n",
    "    # ロギング結果\n",
    "    avg_cos = np.mean(cos_log) if cos_log else 0\n",
    "    avg_norm = np.mean(norm_log) if norm_log else 0\n",
    "    \n",
    "    return final_acc, max(best_acc, final_acc), avg_cos, avg_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = load_cifar10()\n",
    "clean_labels = np.array(trainset.targets)\n",
    "train_loader, test_loader = get_data_loaders(trainset, testset)\n",
    "\n",
    "print('Verifying IndexedDataset...')\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f'  ✓ indices shape: {list(sample_batch[2].shape)}')\n",
    "\n",
    "print('\\nWarming up GPU...')\n",
    "warmup_model = ResNet18().to(device)\n",
    "for _ in range(20):\n",
    "    _ = warmup_model(torch.randn(BATCH_SIZE, 3, 32, 32, device=device))\n",
    "del warmup_model\n",
    "torch.cuda.empty_cache()\n",
    "print('Warmup complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "checkpoint_file = f'{SAVE_DIR}/checkpoint.json'\n",
    "completed = set()\n",
    "\n",
    "if os.path.exists(checkpoint_file):\n",
    "    with open(checkpoint_file, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    for r in results:\n",
    "        completed.add((r['condition'], r['lambda'], r['seed']))\n",
    "    print(f'Checkpoint loaded: {len(completed)} runs')\n",
    "\n",
    "run_counter = 0\n",
    "exp_start = time.time()\n",
    "\n",
    "for cond in CONDITIONS:\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'CONDITION: {cond[\"name\"]}')\n",
    "    print(f'  value_type={cond[\"value_type\"]}, renorm={cond[\"renorm\"]}, K={cond[\"K\"]}')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    for lam in LAMBDAS:\n",
    "        for seed in SEEDS:\n",
    "            run_counter += 1\n",
    "            key = (cond['name'], lam, seed)\n",
    "            \n",
    "            if key in completed:\n",
    "                print(f'[{run_counter}/{total_runs}] {cond[\"name\"]} λ={lam} seed={seed} - SKIPPED')\n",
    "                continue\n",
    "            \n",
    "            print(f'\\n[{run_counter}/{total_runs}] {cond[\"name\"]} λ={lam} seed={seed}')\n",
    "            t0 = time.time()\n",
    "            \n",
    "            set_seed(seed)\n",
    "            noisy_labels = inject_noise(clean_labels, NOISE_RATE, seed)\n",
    "            model = ResNet18().to(device)\n",
    "            \n",
    "            final_acc, best_acc, avg_cos, avg_norm = train_dual_gradient_ablation(\n",
    "                model, train_loader, test_loader,\n",
    "                clean_labels, noisy_labels, lam,\n",
    "                value_type=cond['value_type'],\n",
    "                renorm=cond['renorm'],\n",
    "                K=cond['K']\n",
    "            )\n",
    "            elapsed = time.time() - t0\n",
    "            \n",
    "            results.append({\n",
    "                'experiment': 'exp_B3_ablation',\n",
    "                'condition': cond['name'],\n",
    "                'value_type': cond['value_type'],\n",
    "                'renorm': cond['renorm'],\n",
    "                'K': cond['K'],\n",
    "                'lambda': lam,\n",
    "                'noise_rate': NOISE_RATE,\n",
    "                'seed': seed,\n",
    "                'test_acc': final_acc,\n",
    "                'test_error': 1 - final_acc,\n",
    "                'best_test_error': 1 - best_acc,\n",
    "                'avg_cos_similarity': avg_cos,\n",
    "                'avg_gmix_norm': avg_norm,\n",
    "                'time_seconds': elapsed\n",
    "            })\n",
    "            \n",
    "            with open(checkpoint_file, 'w') as f:\n",
    "                json.dump(results, f, indent=2)\n",
    "            \n",
    "            # 判定\n",
    "            ce_baseline = 0.38\n",
    "            if (1 - final_acc) < ce_baseline * 0.7:\n",
    "                status = ' ✅ IMPROVED'\n",
    "            elif (1 - final_acc) > ce_baseline:\n",
    "                status = ' ⚠️ DEGRADED'\n",
    "            else:\n",
    "                status = ' ~ marginal'\n",
    "            \n",
    "            print(f'  Error: {1-final_acc:.4f} | Best: {1-best_acc:.4f} | cos: {avg_cos:.4f} | ||g_mix||: {avg_norm:.4f} | Time: {elapsed/60:.1f} min{status}')\n",
    "\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'EXPERIMENT COMPLETE')\n",
    "print(f'Total time: {(time.time()-exp_start)/3600:.2f} hours')\n",
    "print(f'{\"=\"*60}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open(f'{SAVE_DIR}/exp_B3_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('ABLATION STUDY SUMMARY')\n",
    "print('='*70)\n",
    "\n",
    "summary = df.groupby(['condition', 'lambda']).agg({\n",
    "    'test_error': ['mean', 'std'],\n",
    "    'avg_cos_similarity': 'mean',\n",
    "    'avg_gmix_norm': 'mean'\n",
    "}).round(4)\n",
    "print(summary)\n",
    "summary.to_csv(f'{SAVE_DIR}/exp_B3_summary.csv')\n",
    "\n",
    "# 切り分け分析\n",
    "print('\\n' + '='*70)\n",
    "print('KEY COMPARISONS')\n",
    "print('='*70)\n",
    "\n",
    "print('\\n【Q1: 再正規化の効果（暗黙のLR低下を除去）】')\n",
    "for lam in LAMBDAS:\n",
    "    baseline = df[(df['condition']=='Baseline') & (df['lambda']==lam)]['test_error'].mean()\n",
    "    renorm = df[(df['condition']=='+Renorm') & (df['lambda']==lam)]['test_error'].mean()\n",
    "    print(f'  λ={lam}: Baseline={baseline:.3f}, +Renorm={renorm:.3f}, Δ={renorm-baseline:+.3f}')\n",
    "\n",
    "print('\\n【Q2: キャッシュK=1の効果】')\n",
    "for lam in LAMBDAS:\n",
    "    renorm = df[(df['condition']=='+Renorm') & (df['lambda']==lam)]['test_error'].mean()\n",
    "    renorm_k1 = df[(df['condition']=='+Renorm+K1') & (df['lambda']==lam)]['test_error'].mean()\n",
    "    print(f'  λ={lam}: +Renorm(K=16)={renorm:.3f}, +Renorm+K1={renorm_k1:.3f}, Δ={renorm_k1-renorm:+.3f}')\n",
    "\n",
    "print('\\n【Q3: 真のAnti-aligned（-g_clean）の効果】')\n",
    "for lam in LAMBDAS:\n",
    "    aligned = df[(df['condition']=='+Renorm+K1') & (df['lambda']==lam)]['test_error'].mean()\n",
    "    anti = df[(df['condition']=='TrueAnti+K1') & (df['lambda']==lam)]['test_error'].mean()\n",
    "    print(f'  λ={lam}: Aligned={aligned:.3f}, TrueAnti={anti:.3f}, Δ={anti-aligned:+.3f}')\n",
    "    if anti > 0.38:\n",
    "        print(f'         ✅ TrueAnti is WORSE than CE baseline (0.38) as expected!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: 条件別のTest Error\n",
    "ax1 = axes[0]\n",
    "for cond in CONDITIONS:\n",
    "    df_c = df[df['condition'] == cond['name']]\n",
    "    g = df_c.groupby('lambda')['test_error'].agg(['mean', 'std'])\n",
    "    ax1.errorbar(g.index, g['mean'], yerr=g['std'], marker='o', capsize=4, \n",
    "                 linewidth=2, markersize=8, label=cond['name'])\n",
    "ax1.axhline(y=0.38, color='gray', linestyle='--', alpha=0.5, label='CE baseline')\n",
    "ax1.set_xlabel('λ', fontsize=12)\n",
    "ax1.set_ylabel('Test Error', fontsize=12)\n",
    "ax1.set_title('Ablation: Test Error by Condition', fontsize=13)\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: ||g_mix|| の比較\n",
    "ax2 = axes[1]\n",
    "for cond in CONDITIONS:\n",
    "    df_c = df[df['condition'] == cond['name']]\n",
    "    g = df_c.groupby('lambda')['avg_gmix_norm'].mean()\n",
    "    ax2.plot(g.index, g.values, marker='s', linewidth=2, markersize=8, label=cond['name'])\n",
    "ax2.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5, label='Unit norm')\n",
    "ax2.set_xlabel('λ', fontsize=12)\n",
    "ax2.set_ylabel('||g_mix||', fontsize=12)\n",
    "ax2.set_title('Gradient Norm (Step Size)', fontsize=13)\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# Plot 3: cos(g_struct, g_value) の比較\n",
    "ax3 = axes[2]\n",
    "for cond in CONDITIONS:\n",
    "    df_c = df[df['condition'] == cond['name']]\n",
    "    g = df_c.groupby('lambda')['avg_cos_similarity'].mean()\n",
    "    ax3.plot(g.index, g.values, marker='^', linewidth=2, markersize=8, label=cond['name'])\n",
    "ax3.axhline(y=0, color='gray', linestyle='--', alpha=0.5, label='Orthogonal')\n",
    "ax3.set_xlabel('λ', fontsize=12)\n",
    "ax3.set_ylabel('cos(g_struct, g_value)', fontsize=12)\n",
    "ax3.set_title('Gradient Alignment', fontsize=13)\n",
    "ax3.legend()\n",
    "ax3.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR}/exp_B3_ablation_plot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'Figure saved to: {SAVE_DIR}/exp_B3_ablation_plot.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
