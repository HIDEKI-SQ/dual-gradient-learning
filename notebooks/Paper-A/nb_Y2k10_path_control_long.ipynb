{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Y2-k10: Path-Control \u2014 Long Damage Exposure (10 epochs at \u03bb=0.20)\\n\\n**Purpose**: Directly test whether low-\u03bb exposure causes irreversible damage.\\n\\n- Phase 1: \u03bb=0.60, 3 epochs (form good state)\\n- Phase 2: \u03bb=0.20, 10 epochs (damage injection)\\n- Phase 3: \u03bb=0.60, 13 epochs (attempted recovery: check at 3ep + extend to 13ep)\\n- \u03b7 = 0.8, r = 5%, SST-2, DistilBERT\\n- Seeds: 20\\n\\n**If Phase 3 fails to recover** \u2192 path-dependent trapping confirmed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install -q transformers datasets accelerate\n\nimport os\nimport json\nimport time\nimport copy\nimport random\nimport warnings\nimport logging\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import (\n    DistilBertTokenizer, DistilBertForSequenceClassification\n)\nfrom datasets import load_dataset\n\nwarnings.filterwarnings(\"ignore\")\nlogging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\nos.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\nos.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    gpu = torch.cuda.get_device_properties(0)\n    print(f\"GPU: {gpu.name} ({gpu.total_mem/1e9:.1f} GB)\" if hasattr(gpu, 'total_mem') else f\"GPU: {gpu.name}\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "CONFIG = {\n    \"experiment\": \"exp_Y2k10_path_control_long\",\n    \"experiment_id\": \"Y2k10\",\n    \"series\": \"Y\",\n    \"experiment_id_prefix\": \"Y2k10\",\n    \"dataset\": \"SST-2\",\n    \"num_classes\": 2,\n    \"model_name\": \"distilbert-base-uncased\",\n    \"noise_type\": \"symmetric\",\n    \"noise_rate\": 0.8,\n    \"trusted_ratio\": 0.05,\n    \"phases\": [\n        {\"name\": \"phase1_good\", \"lambda\": 0.60, \"epochs\": 3},\n        {\"name\": \"phase2_damage\", \"lambda\": 0.20, \"epochs\": 10},\n        {\"name\": \"phase3_recovery\", \"lambda\": 0.60, \"epochs\": 13},\n    ],\n    \"seeds\": list(range(20)),\n    \"batch_size\": 64,\n    \"learning_rate\": 2e-5,\n    \"weight_decay\": 0.01,\n    \"warmup_ratio\": 0.1,\n    \"max_seq_length\": 128,\n    \"use_bf16\": True,\n    \"max_train_samples\": 16000,\n    \"output_dir\": \"Y2k10_results\",\n}\n\nos.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n\ntotal_ep = sum(p[\"epochs\"] for p in CONFIG[\"phases\"])\nn_runs = len(CONFIG[\"seeds\"])\nn_used = min(int(67349 * 0.95), CONFIG[\"max_train_samples\"])\nsteps_per_epoch = n_used // CONFIG[\"batch_size\"]\n\nprint(f\"Experiment: {CONFIG['experiment']}\")\nprint(f\"Phases:\")\nfor p in CONFIG[\"phases\"]:\n    print(f\"  {p['name']}: \u03bb={p['lambda']}, {p['epochs']} ep\")\nprint(f\"Total epochs per run: {total_ep}\")\nprint(f\"Seeds: {n_runs}\")\nest_per_run = steps_per_epoch * total_ep * 25 / 1000 + total_ep * 5\nprint(f\"Est. per run: ~{est_per_run/60:.1f} min\")\nprint(f\"Est. total: ~{n_runs * est_per_run / 3600:.1f} h\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Drive Mount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from google.colab import drive\ndrive.mount('/content/drive')\n\nDRIVE_BASE = \"/content/drive/MyDrive/Paper-A_Y_series\"\nDRIVE_OUTPUT = f\"{DRIVE_BASE}/exp_Y2k10_path_control_long\"\nos.makedirs(DRIVE_OUTPUT, exist_ok=True)\nLOCAL_OUTPUT = CONFIG[\"output_dir\"]\n\ndef save_to_drive(filename, data):\n    for d in [LOCAL_OUTPUT, DRIVE_OUTPUT]:\n        with open(os.path.join(d, filename), \"w\") as f:\n            json.dump(data, f, indent=2)\n\ndef save_figure_to_drive(fig, filename, dpi=300):\n    for d in [LOCAL_OUTPUT, DRIVE_OUTPUT]:\n        fig.savefig(os.path.join(d, filename), dpi=dpi, bbox_inches=\"tight\", facecolor=\"white\")\n\nprint(f\"Local: {LOCAL_OUTPUT}\")\nprint(f\"Drive: {DRIVE_OUTPUT}\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Seed Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Loading SST-2...\")\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\nraw_dataset = load_dataset(\"glue\", \"sst2\")\n\ntrain_texts = list(raw_dataset[\"train\"][\"sentence\"])\ntrain_labels = list(raw_dataset[\"train\"][\"label\"])\ntest_texts = list(raw_dataset[\"validation\"][\"sentence\"])\ntest_labels = list(raw_dataset[\"validation\"][\"label\"])\n\nprint(f\"Train: {len(train_texts)}, Test: {len(test_texts)}\")\n\nprint(\"Tokenizing...\")\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=\"max_length\",\n                            max_length=CONFIG[\"max_seq_length\"], return_tensors=\"pt\")\ntest_encodings = tokenizer(test_texts, truncation=True, padding=\"max_length\",\n                           max_length=CONFIG[\"max_seq_length\"], return_tensors=\"pt\")\nprint(\"Done.\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class TextDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n    def __len__(self):\n        return len(self.labels)\n    def __getitem__(self, idx):\n        item = {k: v[idx] for k, v in self.encodings.items()}\n        item[\"labels\"] = self.labels[idx]\n        return item\n\ntest_dataset = TextDataset(test_encodings, torch.tensor(test_labels, dtype=torch.long))\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\nprint(f\"Test loader: {len(test_dataset)} samples\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Noise Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def prepare_noisy_and_trusted(train_labels, noise_rate, trusted_ratio,\n                              num_classes, seed, max_noisy_samples=None):\n    rng = np.random.RandomState(seed + 10000)\n    n = len(train_labels)\n    labels = np.array(train_labels)\n    indices = np.arange(n)\n\n    trusted_indices = []\n    for c in range(num_classes):\n        class_idx = indices[labels == c]\n        n_sel = max(1, int(len(class_idx) * trusted_ratio))\n        trusted_indices.extend(rng.choice(class_idx, size=n_sel, replace=False))\n    trusted_indices = np.array(sorted(trusted_indices))\n\n    noisy_mask = np.ones(n, dtype=bool)\n    noisy_mask[trusted_indices] = False\n    noisy_indices = indices[noisy_mask]\n\n    noisy_labels = labels.copy()\n    n_flip = int(len(noisy_indices) * noise_rate)\n    flip_idx = rng.choice(noisy_indices, size=n_flip, replace=False)\n    for idx in flip_idx:\n        candidates = [c for c in range(num_classes) if c != noisy_labels[idx]]\n        noisy_labels[idx] = rng.choice(candidates)\n\n    if max_noisy_samples and len(noisy_indices) > max_noisy_samples:\n        noisy_indices = np.sort(rng.choice(noisy_indices, size=max_noisy_samples, replace=False))\n\n    actual = np.mean(noisy_labels[noisy_indices] != labels[noisy_indices])\n    print(f\"  Trusted: {len(trusted_indices)} | Noisy: {len(noisy_indices)} (\u03b7={actual:.3f})\")\n    return torch.tensor(noisy_labels, dtype=torch.long), trusted_indices, noisy_indices\n\nprint(\"Noise test:\")\n_, ti, ni = prepare_noisy_and_trusted(train_labels, 0.8, 0.05, 2, 0, 16000)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_head_param_names(model):\n    return [n for n, _ in model.named_parameters() if \"classifier\" in n or \"pre_classifier\" in n]\n\ndef extract_grads(model):\n    return {n: p.grad.clone() for n, p in model.named_parameters() if p.grad is not None}\n\ndef normalize_grad_dict(grads):\n    flat = torch.cat([g.flatten() for g in grads.values()])\n    norm = flat.norm()\n    if norm > 0:\n        return {n: g / norm for n, g in grads.items()}, norm.item()\n    return grads, 0.0\n\ndef cosine_sim(grads_a, grads_b, param_names=None):\n    keys = [n for n in (param_names or sorted(set(grads_a) & set(grads_b)))\n            if n in grads_a and n in grads_b]\n    if not keys: return 0.0\n    va = torch.cat([grads_a[n].flatten() for n in keys])\n    vb = torch.cat([grads_b[n].flatten() for n in keys])\n    return F.cosine_similarity(va.unsqueeze(0), vb.unsqueeze(0)).item()\n\ndef set_mixed_grad(model, g_struct_n, g_value_n, lam):\n    for n, p in model.named_parameters():\n        if n in g_struct_n and n in g_value_n:\n            p.grad = (1 - lam) * g_struct_n[n] + lam * g_value_n[n]\n        elif n in g_struct_n:\n            p.grad = (1 - lam) * g_struct_n[n]\n        elif n in g_value_n:\n            p.grad = lam * g_value_n[n]\n\n@torch.no_grad()\ndef evaluate(model, test_loader, device):\n    model.eval()\n    correct = total = 0\n    total_loss = 0.0\n    n = 0\n    for batch in test_loader:\n        ids = batch[\"input_ids\"].to(device)\n        mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n        out = model(input_ids=ids, attention_mask=mask, labels=labels)\n        preds = out.logits.argmax(dim=-1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n        total_loss += out.loss.item()\n        n += 1\n    model.train()\n    acc = correct / total\n    return acc, 1.0 - acc, total_loss / n\n\nprint(\"Core functions defined.\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Function (Multi-Phase)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def run_path_control(seed, config):\n    \"\"\"\n    Execute multi-phase path-control experiment.\n    Phase 1: \u03bb_high (good-state formation)\n    Phase 2: \u03bb_low (damage injection)\n    Phase 3: \u03bb_high (attempted recovery)\n    Model and optimizer state carry over between phases.\n    \"\"\"\n    phases = config[\"phases\"]\n    exp_id = f\"{config['experiment_id_prefix']}-{seed:03d}\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    set_seed(seed)\n    t_start = time.time()\n\n    # Data preparation\n    noisy_labels, trusted_idx, noisy_idx = prepare_noisy_and_trusted(\n        train_labels, config[\"noise_rate\"], config[\"trusted_ratio\"],\n        config[\"num_classes\"], seed, config.get(\"max_train_samples\")\n    )\n\n    noisy_dataset = TextDataset(\n        {k: v[noisy_idx] for k, v in train_encodings.items()},\n        noisy_labels[noisy_idx]\n    )\n    trusted_dataset = TextDataset(\n        {k: v[trusted_idx] for k, v in train_encodings.items()},\n        noisy_labels[trusted_idx]\n    )\n\n    struct_loader = DataLoader(noisy_dataset, batch_size=config[\"batch_size\"],\n                               shuffle=True, drop_last=True)\n    value_loader = DataLoader(trusted_dataset,\n                              batch_size=min(config[\"batch_size\"], len(trusted_dataset)),\n                              shuffle=True, drop_last=False)\n\n    # Model \u2014 fresh init (once)\n    set_seed(seed)\n    model = DistilBertForSequenceClassification.from_pretrained(\n        config[\"model_name\"], num_labels=config[\"num_classes\"]\n    ).to(device)\n    model.train()\n    head_params = get_head_param_names(model)\n\n    # Optimizer \u2014 persists across ALL phases\n    optimizer = torch.optim.AdamW(\n        model.parameters(),\n        lr=config[\"learning_rate\"],\n        weight_decay=config[\"weight_decay\"]\n    )\n\n    use_amp = config.get(\"use_bf16\", False) and device.type == \"cuda\"\n    amp_dtype = torch.bfloat16 if use_amp else torch.float32\n\n    # Initial eval (before any training)\n    acc0, err0, loss0 = evaluate(model, test_loader, device)\n    print(f\"    [Init] error={err0:.4f}\")\n\n    all_epoch_logs = []\n    phase_summaries = []\n    global_epoch = 0\n\n    for phase_idx, phase in enumerate(phases):\n        phase_name = phase[\"name\"]\n        lam = phase[\"lambda\"]\n        n_epochs = phase[\"epochs\"]\n        phase_logs = []\n\n        print(f\"    Phase {phase_idx+1} ({phase_name}): \u03bb={lam:.2f}, {n_epochs} epochs\")\n\n        for ep in range(n_epochs):\n            global_epoch += 1\n            value_iter = iter(value_loader)\n            epoch_loss_s = 0.0\n            epoch_loss_v = 0.0\n            epoch_cos_sv = []\n            n_batches = 0\n\n            for struct_batch in struct_loader:\n                # Struct gradient\n                optimizer.zero_grad()\n                s_ids = struct_batch[\"input_ids\"].to(device)\n                s_mask = struct_batch[\"attention_mask\"].to(device)\n                s_labels = struct_batch[\"labels\"].to(device)\n                with torch.amp.autocast(\"cuda\", dtype=amp_dtype, enabled=use_amp):\n                    out_s = model(input_ids=s_ids, attention_mask=s_mask, labels=s_labels)\n                out_s.loss.backward()\n                g_struct = extract_grads(model)\n\n                # Value gradient\n                optimizer.zero_grad()\n                try:\n                    vb = next(value_iter)\n                except StopIteration:\n                    value_iter = iter(value_loader)\n                    vb = next(value_iter)\n\n                v_ids = vb[\"input_ids\"].to(device)\n                v_mask = vb[\"attention_mask\"].to(device)\n                v_labels = vb[\"labels\"].to(device)\n                with torch.amp.autocast(\"cuda\", dtype=amp_dtype, enabled=use_amp):\n                    out_v = model(input_ids=v_ids, attention_mask=v_mask, labels=v_labels)\n                out_v.loss.backward()\n                g_value = extract_grads(model)\n\n                # Normalize\n                g_struct_n, norm_s = normalize_grad_dict(g_struct)\n                g_value_n, norm_v = normalize_grad_dict(g_value)\n\n                c_sv = cosine_sim(g_struct_n, g_value_n, head_params)\n                epoch_cos_sv.append(c_sv)\n\n                # Mixed gradient\n                optimizer.zero_grad()\n                set_mixed_grad(model, g_struct_n, g_value_n, lam)\n                optimizer.step()\n\n                epoch_loss_s += out_s.loss.item()\n                epoch_loss_v += out_v.loss.item()\n                n_batches += 1\n\n            # End of epoch eval\n            acc, error, test_loss = evaluate(model, test_loader, device)\n\n            epoch_log = {\n                \"global_epoch\": global_epoch,\n                \"phase\": phase_name,\n                \"phase_epoch\": ep + 1,\n                \"lambda\": lam,\n                \"test_error\": round(error, 6),\n                \"test_acc\": round(acc, 6),\n                \"test_loss\": round(test_loss, 6),\n                \"train_loss_struct\": round(epoch_loss_s / max(n_batches, 1), 6),\n                \"train_loss_value\": round(epoch_loss_v / max(n_batches, 1), 6),\n                \"mean_cos_sv\": round(float(np.mean(epoch_cos_sv)), 6),\n                \"n_batches\": n_batches\n            }\n            all_epoch_logs.append(epoch_log)\n            phase_logs.append(epoch_log)\n\n            print(f\"      Epoch {global_epoch} (phase_ep {ep+1}/{n_epochs}): \"\n                  f\"error={error:.4f} cos_sv={epoch_log['mean_cos_sv']:.4f}\")\n\n        # Phase summary\n        phase_final = phase_logs[-1][\"test_error\"]\n        phase_best = min(l[\"test_error\"] for l in phase_logs)\n        phase_summaries.append({\n            \"phase\": phase_name,\n            \"lambda\": lam,\n            \"epochs\": n_epochs,\n            \"final_error\": round(phase_final, 6),\n            \"best_error\": round(phase_best, 6)\n        })\n\n    elapsed = time.time() - t_start\n    result = {\n        \"experiment_id\": exp_id,\n        \"experiment\": config[\"experiment\"],\n        \"seed\": seed,\n        \"noise_rate\": config[\"noise_rate\"],\n        \"trusted_ratio\": config[\"trusted_ratio\"],\n        \"phases\": [{\"name\": p[\"name\"], \"lambda\": p[\"lambda\"], \"epochs\": p[\"epochs\"]} for p in phases],\n        \"total_epochs\": global_epoch,\n        \"init_error\": round(err0, 6),\n        \"final_error\": round(all_epoch_logs[-1][\"test_error\"], 6),\n        \"phase_summaries\": phase_summaries,\n        \"epoch_logs\": all_epoch_logs,\n        \"time_seconds\": round(elapsed, 1)\n    }\n    return result\n\nprint(\"Multi-phase training function defined.\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Execute"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "all_results = []\nn_runs = len(CONFIG[\"seeds\"])\nt_total = time.time()\n\nprint(\"=\" * 70)\nprint(f\"Y2-k10: Path Control (Long Damage) \u2014 {n_runs} seeds\")\nfor p in CONFIG[\"phases\"]:\n    print(f\"  {p['name']}: \u03bb={p['lambda']}, {p['epochs']} ep\")\nprint(\"=\" * 70)\n\nfor i, seed in enumerate(CONFIG[\"seeds\"]):\n    print(f\"\\nSeed {seed} ({i+1}/{n_runs})\")\n    print(\"-\" * 50)\n    result = run_path_control(seed, CONFIG)\n    all_results.append(result)\n\n    elapsed = time.time() - t_total\n    eta = elapsed / (i+1) * (n_runs - i - 1)\n    ps = result[\"phase_summaries\"]\n    print(f\"  Summary: \" + \" \u2192 \".join(f\"{p['phase']}={p['final_error']:.4f}\" for p in ps))\n    print(f\"  ({result['time_seconds']:.0f}s) [ETA: {eta/60:.0f}m]\")\n\n    save_to_drive(f\"Y2k10_seed{seed:02d}.json\", result)\n\ntotal_time = time.time() - t_total\nsave_to_drive(\"Y2k10_results.json\", all_results)\nsave_to_drive(\"Y2k10_config.json\", CONFIG)\n\nprint(f\"\\n{'='*70}\")\nprint(f\"Y2-k10 COMPLETE: {n_runs} runs in {total_time/60:.1f} min\")\nfor phase_name in [\"phase1_good\", \"phase2_damage\", \"phase3_recovery\"]:\n    errs = [p[\"final_error\"] for r in all_results for p in r[\"phase_summaries\"] if p[\"phase\"]==phase_name]\n    if errs:\n        print(f\"  {phase_name}: {np.mean(errs):.4f} \u00b1 {np.std(errs):.4f}\")\nprint(f\"{'='*70}\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\nfrom scipy import stats\n\npath = os.path.join(LOCAL_OUTPUT, \"Y2k10_results.json\")\nif not os.path.exists(path):\n    path = os.path.join(DRIVE_OUTPUT, \"Y2k10_results.json\")\nwith open(path) as f:\n    results = json.load(f)\n\nfig, axes = plt.subplots(1, 3, figsize=(21, 6))\n\n# ---- (A) Individual seed trajectories ----\nax = axes[0]\nphase_colors = {\"phase1_good\": \"#2CA02C\", \"phase2_damage\": \"#D62728\", \"phase3_recovery\": \"#1F77B4\"}\nphase_boundaries = [3, 13, 26]  # cumulative epoch boundaries\n\nfor r in results:\n    epochs_all = [l[\"global_epoch\"] for l in r[\"epoch_logs\"]]\n    errors_all = [l[\"test_error\"] for l in r[\"epoch_logs\"]]\n    ax.plot(epochs_all, errors_all, '-', color=\"gray\", alpha=0.2, linewidth=0.8)\n\n# Mean trajectory\nall_trajs = np.array([[l[\"test_error\"] for l in r[\"epoch_logs\"]] for r in results])\nmeans = all_trajs.mean(axis=0)\nstds = all_trajs.std(axis=0)\ntotal_ep = all_trajs.shape[1]\nepochs_range = np.arange(1, total_ep + 1)\n\nax.plot(epochs_range, means, 'o-', color=\"black\", linewidth=2.5, markersize=4, label=\"Mean\", zorder=5)\nax.fill_between(epochs_range, means - stds, means + stds, alpha=0.15, color=\"black\")\n\n# Phase boundaries\nfor b in phase_boundaries[:-1]:\n    ax.axvline(b + 0.5, color=\"orange\", ls=\"--\", lw=1.5, alpha=0.7)\nax.axhline(0.5, color=\"gray\", ls=\":\", alpha=0.4)\n\nax.text(2, 0.95, \"Phase 1\\n\u03bb=0.60\", ha=\"center\", fontsize=10, color=\"#2CA02C\", fontweight=\"bold\")\nax.text(8, 0.95, \"Phase 2\\n\u03bb=0.20\", ha=\"center\", fontsize=10, color=\"#D62728\", fontweight=\"bold\")\nax.text(20, 0.95, \"Phase 3\\n\u03bb=0.60\", ha=\"center\", fontsize=10, color=\"#1F77B4\", fontweight=\"bold\")\n\nax.set_xlabel(\"Global Epoch\", fontsize=13)\nax.set_ylabel(\"Test Error\", fontsize=13)\nax.set_title(\"(A) Y2-k10: Error Trajectory\", fontsize=13, fontweight=\"bold\")\nax.legend(fontsize=10)\nax.grid(True, alpha=0.3)\nax.set_ylim(0, 1.0)\n\n# ---- (B) Phase summary (bar chart) ----\nax = axes[1]\nphase_names = [\"phase1_good\", \"phase2_damage\", \"phase3_recovery\"]\nphase_labels_short = [\"Phase 1\\n(\u03bb=0.60, 3ep)\", \"Phase 2\\n(\u03bb=0.20, 10ep)\", \"Phase 3\\n(\u03bb=0.60, 13ep)\"]\ncolors_bar = [\"#2CA02C\", \"#D62728\", \"#1F77B4\"]\n\nfor pi, (pname, plabel) in enumerate(zip(phase_names, phase_labels_short)):\n    errs = [p[\"final_error\"] for r in results for p in r[\"phase_summaries\"] if p[\"phase\"] == pname]\n    ax.bar(pi, np.mean(errs), yerr=np.std(errs), color=colors_bar[pi], alpha=0.7,\n           edgecolor=\"black\", capsize=8)\n    ax.text(pi, np.mean(errs) + np.std(errs) + 0.02, f\"{np.mean(errs):.3f}\",\n            ha=\"center\", fontsize=12, fontweight=\"bold\")\n\nax.axhline(0.185, color=\"#FF7F0E\", ls=\"--\", lw=2, alpha=0.7, label=\"X1b (\u03bb=0.60, 3ep) ref\")\nax.axhline(0.5, color=\"gray\", ls=\":\", alpha=0.4)\nax.set_xticks(range(3))\nax.set_xticklabels(phase_labels_short, fontsize=10)\nax.set_ylabel(\"Final Test Error\", fontsize=13)\nax.set_title(\"(B) Phase-End Errors\", fontsize=13, fontweight=\"bold\")\nax.legend(fontsize=10)\nax.grid(True, alpha=0.3, axis=\"y\")\nax.set_ylim(0, 1.0)\n\n# ---- (C) Recovery curve detail (Phase 3 only) ----\nax = axes[2]\np3_start = 3 + 10  # global epoch where phase 3 starts\np3_trajs = all_trajs[:, p3_start:]\np3_means = p3_trajs.mean(axis=0)\np3_stds = p3_trajs.std(axis=0)\np3_epochs = np.arange(1, p3_trajs.shape[1] + 1)\n\nax.plot(p3_epochs, p3_means, 'o-', color=\"#1F77B4\", linewidth=2.5, markersize=6, label=\"Phase 3 (\u03bb=0.60)\")\nax.fill_between(p3_epochs, p3_means - p3_stds, p3_means + p3_stds, alpha=0.15, color=\"#1F77B4\")\n\n# Phase 1 final as reference\np1_finals = all_trajs[:, 2]  # epoch 3\nax.axhline(np.mean(p1_finals), color=\"#2CA02C\", ls=\"--\", lw=2, label=f\"Phase 1 final: {np.mean(p1_finals):.3f}\")\nax.axhline(0.185, color=\"#FF7F0E\", ls=\":\", lw=2, alpha=0.7, label=\"X1b ref: 0.185\")\nax.axhline(0.5, color=\"gray\", ls=\":\", alpha=0.4)\nax.axvline(3.5, color=\"red\", ls=\"--\", alpha=0.5, label=\"3 ep checkpoint\")\n\nax.set_xlabel(\"Phase 3 Epoch\", fontsize=13)\nax.set_ylabel(\"Test Error\", fontsize=13)\nax.set_title(\"(C) Recovery Attempt (Phase 3 Detail)\", fontsize=13, fontweight=\"bold\")\nax.legend(fontsize=10)\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nsave_figure_to_drive(fig, \"Y2k10_analysis.png\")\nplt.show()\n\n# Quantitative summary\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Y2-k10 Summary\")\nprint(\"=\" * 60)\nfor pname in phase_names:\n    errs = [p[\"final_error\"] for r in results for p in r[\"phase_summaries\"] if p[\"phase\"] == pname]\n    print(f\"  {pname}: {np.mean(errs):.4f} \u00b1 {np.std(errs):.4f}\")\n\np1_err = np.mean([p[\"final_error\"] for r in results for p in r[\"phase_summaries\"] if p[\"phase\"]==\"phase1_good\"])\np3_err = np.mean([p[\"final_error\"] for r in results for p in r[\"phase_summaries\"] if p[\"phase\"]==\"phase3_recovery\"])\nrecovery_gap = p3_err - p1_err\n\nprint(f\"\\nRecovery gap (Phase3 - Phase1): {recovery_gap:+.4f}\")\nif recovery_gap > 0.10:\n    print(\"\u2192 CONCLUSION: Low-\u03bb exposure causes LASTING DAMAGE that does not recover\")\n    print(\"  This supports PATH-DEPENDENT TRAPPING\")\nelif recovery_gap < 0.03:\n    print(\"\u2192 CONCLUSION: Full recovery \u2014 damage is REVERSIBLE\")\n    print(\"  X3's phenomenon may be protocol-specific\")\nelse:\n    print(\"\u2192 CONCLUSION: Partial recovery \u2014 some damage persists\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save & Download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"All results saved to: {DRIVE_OUTPUT}\")\nfor f in sorted(os.listdir(DRIVE_OUTPUT)):\n    sz = os.path.getsize(os.path.join(DRIVE_OUTPUT, f))\n    print(f\"  {f} ({sz/1024:.1f} KB)\")\n\nimport shutil\nshutil.make_archive(CONFIG[\"output_dir\"], \"zip\", LOCAL_OUTPUT)\ntry:\n    from google.colab import files\n    files.download(f\"{CONFIG['output_dir']}.zip\")\nexcept:\n    pass\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ]
}