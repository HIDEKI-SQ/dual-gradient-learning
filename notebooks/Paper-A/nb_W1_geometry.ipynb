{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W1: Geometric Logs by Branch\n",
    "\n",
    "**Purpose**: Record gradient geometry (cos, norms) to show branches are geometrically distinct\n",
    "\n",
    "**Protocol**:\n",
    "- Run sweep on both branches (ordered/collapse)\n",
    "- Record at each Î»: cos(g_s, g_v), cos(g_v, g_c), ||g_s||, ||g_v||, ||g_mix||\n",
    "- Show systematic differences between branches\n",
    "\n",
    "**Key Metrics**:\n",
    "- c_sv: cos(g_struct, g_value)\n",
    "- c_vc: cos(g_value, g_clean)  \n",
    "- c_mc: cos(g_mix, g_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os, glob, json, time\n",
    "from datetime import datetime\n",
    "\n",
    "EXP_NAME = 'exp_W1_geometry'\n",
    "NOTEBOOK_ID = 'W1'\n",
    "BASE_DIR = '/content/drive/MyDrive/dual-gradient-learning/Paper-A'\n",
    "\n",
    "ORDERED_CKPT_DIR = sorted(glob.glob(f'{BASE_DIR}/exp_Ta_prep_ordered_*'))[-1] + '/checkpoints'\n",
    "COLLAPSE_CKPT_DIR = sorted(glob.glob(f'{BASE_DIR}/exp_Ta_prep_collapse_*'))[-1] + '/checkpoints'\n",
    "print(f'Ordered: {ORDERED_CKPT_DIR}')\n",
    "print(f'Collapse: {COLLAPSE_CKPT_DIR}')\n",
    "\n",
    "existing = glob.glob(f'{BASE_DIR}/{EXP_NAME}_*')\n",
    "if existing:\n",
    "    SAVE_DIR = sorted(existing)[-1]\n",
    "    print(f'ðŸ”„ Resuming: {SAVE_DIR}')\n",
    "else:\n",
    "    TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    SAVE_DIR = f'{BASE_DIR}/{EXP_NAME}_{TIMESTAMP}'\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    print(f'ðŸ†• New: {SAVE_DIR}')\n",
    "\n",
    "os.makedirs(f'{SAVE_DIR}/figures', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils import parameters_to_vector\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "NUM_WORKERS = 4\n",
    "LR = 0.1\n",
    "K = 16\n",
    "\n",
    "NOISE_RATE = 0.4\n",
    "LAMBDA_START = 0.30\n",
    "LAMBDA_END = 0.70\n",
    "LAMBDA_STEP = 0.04  # Coarser for speed (11 points)\n",
    "EPOCHS_PER_LAMBDA = 3\n",
    "LOG_FREQ = 50  # Log geometry every N steps\n",
    "\n",
    "LAMBDA_GRID_UP = np.round(np.arange(LAMBDA_START, LAMBDA_END + LAMBDA_STEP/2, LAMBDA_STEP), 2)\n",
    "LAMBDA_GRID_DOWN = np.round(np.arange(LAMBDA_END, LAMBDA_START - LAMBDA_STEP/2, -LAMBDA_STEP), 2)\n",
    "\n",
    "N_SEEDS = 3  # Reduced for speed\n",
    "\n",
    "print(f'Î» points: {len(LAMBDA_GRID_UP)}')\n",
    "print(f'Log frequency: every {LOG_FREQ} steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet18():\n",
    "    model = resnet18(weights=None, num_classes=10)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "    return model\n",
    "\n",
    "class IndexedDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        return img, label, idx\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def inject_label_noise(labels, noise_rate, seed):\n",
    "    np.random.seed(seed)\n",
    "    noisy = labels.copy()\n",
    "    n_noisy = int(noise_rate * len(labels))\n",
    "    idx = np.random.choice(len(labels), n_noisy, replace=False)\n",
    "    for i in idx:\n",
    "        noisy[i] = np.random.choice([l for l in range(10) if l != labels[i]])\n",
    "    return noisy\n",
    "\n",
    "def load_cifar10():\n",
    "    tr = transforms.Compose([transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(),\n",
    "                             transforms.ToTensor(), transforms.Normalize((0.4914,0.4822,0.4465),(0.2023,0.1994,0.2010))])\n",
    "    te = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914,0.4822,0.4465),(0.2023,0.1994,0.2010))])\n",
    "    return torchvision.datasets.CIFAR10('./data', True, tr, download=True), torchvision.datasets.CIFAR10('./data', False, te, download=True)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            correct += (model(x).argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return correct / total\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    return (a @ b / (a.norm() * b.norm() + 1e-12)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch_with_logging(model, train_loader, opt, clean_t, noisy_t, lam, state):\n",
    "    \"\"\"Train one epoch with geometry logging\"\"\"\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    \n",
    "    step = state['step']\n",
    "    cached_gv = state['gv']\n",
    "    geometry_logs = []\n",
    "    \n",
    "    for x, _, idx in train_loader:\n",
    "        x, idx = x.to(device), idx.to(device)\n",
    "        bn = noisy_t[idx]  # Noisy labels (structure)\n",
    "        bc = clean_t[idx]  # Clean labels (value)\n",
    "        \n",
    "        # Compute g_struct (noisy labels)\n",
    "        opt.zero_grad()\n",
    "        loss_s = crit(model(x), bn)\n",
    "        loss_s.backward(retain_graph=True)\n",
    "        g_s = parameters_to_vector([p.grad for p in model.parameters()]).clone()\n",
    "        \n",
    "        # Compute g_value (clean labels) - always compute for logging\n",
    "        opt.zero_grad()\n",
    "        loss_v = crit(model(x), bc)\n",
    "        loss_v.backward()\n",
    "        g_v = parameters_to_vector([p.grad for p in model.parameters()]).clone()\n",
    "        \n",
    "        # Cache for efficiency (still use cached for actual training)\n",
    "        if step % K == 0 or cached_gv is None:\n",
    "            cached_gv = g_v.clone()\n",
    "        \n",
    "        # Normalize\n",
    "        g_s_n = g_s / (g_s.norm() + 1e-12)\n",
    "        g_v_n = cached_gv / (cached_gv.norm() + 1e-12)\n",
    "        \n",
    "        # Mix\n",
    "        g_mix = (1 - lam) * g_s_n + lam * g_v_n\n",
    "        \n",
    "        # Log geometry at intervals\n",
    "        if step % LOG_FREQ == 0:\n",
    "            log_entry = {\n",
    "                'step': step,\n",
    "                'lambda': float(lam),\n",
    "                'c_sv': cosine_sim(g_s, g_v),       # cos(struct, value)\n",
    "                'c_sc': cosine_sim(g_s, g_v),       # Same as c_sv when value=clean\n",
    "                'c_vc': 1.0,                         # cos(value, clean) = 1 when value IS clean\n",
    "                'c_mc': cosine_sim(g_mix, g_v),     # cos(mix, clean)\n",
    "                'norm_s': g_s.norm().item(),\n",
    "                'norm_v': g_v.norm().item(),\n",
    "                'norm_mix': g_mix.norm().item(),\n",
    "                'loss_s': loss_s.item(),\n",
    "                'loss_v': loss_v.item()\n",
    "            }\n",
    "            geometry_logs.append(log_entry)\n",
    "        \n",
    "        # Apply gradient\n",
    "        opt.zero_grad()\n",
    "        i = 0\n",
    "        for p in model.parameters():\n",
    "            n = p.numel()\n",
    "            p.grad = g_mix[i:i+n].view(p.shape).clone()\n",
    "            i += n\n",
    "        opt.step()\n",
    "        step += 1\n",
    "    \n",
    "    state['step'] = step\n",
    "    state['gv'] = cached_gv\n",
    "    \n",
    "    return geometry_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sweep_with_geometry(ckpt_path, train_loader, test_loader, clean_labels, noisy_labels, direction='up'):\n",
    "    \"\"\"Run sweep and collect geometry logs\"\"\"\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    seed = ckpt['seed']\n",
    "    init_error = ckpt['final_error']\n",
    "    \n",
    "    lambda_grid = LAMBDA_GRID_UP if direction == 'up' else LAMBDA_GRID_DOWN\n",
    "    branch = 'ordered_up' if direction == 'up' else 'collapse_down'\n",
    "    \n",
    "    print(f'    Loaded: seed={seed}, init_error={init_error:.4f}, branch={branch}')\n",
    "    \n",
    "    clean_t = torch.tensor(clean_labels, device=device)\n",
    "    noisy_t = torch.tensor(noisy_labels, device=device)\n",
    "    \n",
    "    set_seed(seed + 5000)\n",
    "    model = get_resnet18().to(device)\n",
    "    model.load_state_dict(ckpt['model_state_dict'])\n",
    "    \n",
    "    opt = optim.SGD(model.parameters(), lr=LR * 0.01, momentum=0.9, weight_decay=5e-4)\n",
    "    state = {'step': 0, 'gv': None}\n",
    "    \n",
    "    trajectory = []\n",
    "    all_geometry_logs = []\n",
    "    \n",
    "    for lam in lambda_grid:\n",
    "        epoch_logs = []\n",
    "        for _ in range(EPOCHS_PER_LAMBDA):\n",
    "            logs = train_one_epoch_with_logging(model, train_loader, opt, clean_t, noisy_t, lam, state)\n",
    "            epoch_logs.extend(logs)\n",
    "        \n",
    "        err = 1 - evaluate(model, test_loader)\n",
    "        trajectory.append({'lambda': float(lam), 'error': err})\n",
    "        \n",
    "        # Aggregate geometry for this Î»\n",
    "        if epoch_logs:\n",
    "            agg = {\n",
    "                'lambda': float(lam),\n",
    "                'error': err,\n",
    "                'c_sv_mean': np.mean([l['c_sv'] for l in epoch_logs]),\n",
    "                'c_sv_std': np.std([l['c_sv'] for l in epoch_logs]),\n",
    "                'c_mc_mean': np.mean([l['c_mc'] for l in epoch_logs]),\n",
    "                'c_mc_std': np.std([l['c_mc'] for l in epoch_logs]),\n",
    "                'norm_s_mean': np.mean([l['norm_s'] for l in epoch_logs]),\n",
    "                'norm_v_mean': np.mean([l['norm_v'] for l in epoch_logs]),\n",
    "                'norm_mix_mean': np.mean([l['norm_mix'] for l in epoch_logs]),\n",
    "                'loss_s_mean': np.mean([l['loss_s'] for l in epoch_logs]),\n",
    "                'loss_v_mean': np.mean([l['loss_v'] for l in epoch_logs]),\n",
    "                'n_logs': len(epoch_logs)\n",
    "            }\n",
    "            all_geometry_logs.append(agg)\n",
    "        \n",
    "        print(f'      Î»={lam:.2f}: err={err:.4f}, c_sv={agg[\"c_sv_mean\"]:.3f}, c_mc={agg[\"c_mc_mean\"]:.3f}')\n",
    "    \n",
    "    return {\n",
    "        'seed': seed,\n",
    "        'branch': branch,\n",
    "        'init_error': init_error,\n",
    "        'final_error': trajectory[-1]['error'],\n",
    "        'trajectory': trajectory,\n",
    "        'geometry': all_geometry_logs,\n",
    "        'checkpoint_source': os.path.basename(ckpt_path)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = load_cifar10()\n",
    "clean_labels = np.array(trainset.targets)\n",
    "train_loader = DataLoader(IndexedDataset(trainset), BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader = DataLoader(testset, BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "# Get checkpoints\n",
    "ordered_ckpts = sorted(glob.glob(f'{ORDERED_CKPT_DIR}/ordered_seed*.pth'))[:N_SEEDS]\n",
    "collapse_ckpts = sorted(glob.glob(f'{COLLAPSE_CKPT_DIR}/collapse_seed*.pth'))\n",
    "collapse_ckpts = [c for c in collapse_ckpts if torch.load(c, map_location='cpu')['final_error'] < 0.85][:N_SEEDS]\n",
    "\n",
    "print(f'Ordered: {len(ordered_ckpts)}, Collapse: {len(collapse_ckpts)}')\n",
    "\n",
    "m = get_resnet18().to(device)\n",
    "for _ in range(10): _ = m(torch.randn(BATCH_SIZE,3,32,32,device=device))\n",
    "del m; torch.cuda.empty_cache()\n",
    "print('Ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# Ordered branch\n",
    "print('='*60)\n",
    "print('ORDERED BRANCH (Î» â†‘)')\n",
    "print('='*60)\n",
    "\n",
    "for i, ckpt_path in enumerate(ordered_ckpts):\n",
    "    print(f'\\n[{i+1}/{len(ordered_ckpts)}] {os.path.basename(ckpt_path)}')\n",
    "    \n",
    "    ckpt_temp = torch.load(ckpt_path, map_location='cpu')\n",
    "    seed = ckpt_temp['seed']\n",
    "    noisy_labels = inject_label_noise(clean_labels, NOISE_RATE, seed)\n",
    "    \n",
    "    result = run_sweep_with_geometry(ckpt_path, train_loader, test_loader, clean_labels, noisy_labels, 'up')\n",
    "    result['experiment_id'] = f'{NOTEBOOK_ID}-ord-{i+1:03d}'\n",
    "    results.append(result)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Collapse branch\n",
    "print('\\n' + '='*60)\n",
    "print('COLLAPSE BRANCH (Î» â†“)')\n",
    "print('='*60)\n",
    "\n",
    "for i, ckpt_path in enumerate(collapse_ckpts):\n",
    "    print(f'\\n[{i+1}/{len(collapse_ckpts)}] {os.path.basename(ckpt_path)}')\n",
    "    \n",
    "    ckpt_temp = torch.load(ckpt_path, map_location='cpu')\n",
    "    seed = ckpt_temp['seed']\n",
    "    noisy_labels = inject_label_noise(clean_labels, NOISE_RATE, seed)\n",
    "    \n",
    "    result = run_sweep_with_geometry(ckpt_path, train_loader, test_loader, clean_labels, noisy_labels, 'down')\n",
    "    result['experiment_id'] = f'{NOTEBOOK_ID}-col-{i+1:03d}'\n",
    "    results.append(result)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "json.dump(results, open(f'{SAVE_DIR}/{NOTEBOOK_ID}_results.json', 'w'), indent=2, default=str)\n",
    "print('\\n' + '='*60)\n",
    "print(f'{NOTEBOOK_ID} COMPLETE')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract geometry data\n",
    "geom_data = []\n",
    "for r in results:\n",
    "    for g in r['geometry']:\n",
    "        geom_data.append({\n",
    "            'seed': r['seed'],\n",
    "            'branch': r['branch'],\n",
    "            **g\n",
    "        })\n",
    "df_geom = pd.DataFrame(geom_data)\n",
    "df_geom.to_csv(f'{SAVE_DIR}/{NOTEBOOK_ID}_geometry.csv', index=False)\n",
    "\n",
    "# Separate branches\n",
    "df_ord = df_geom[df_geom['branch'] == 'ordered_up']\n",
    "df_col = df_geom[df_geom['branch'] == 'collapse_down']\n",
    "\n",
    "print(f'Geometry data: {len(df_geom)} points')\n",
    "print(f'  Ordered: {len(df_ord)}')\n",
    "print(f'  Collapse: {len(df_col)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Geometry by branch\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# 1. c_sv (cos struct-value) by Î»\n",
    "ax = axes[0, 0]\n",
    "if len(df_ord) > 0:\n",
    "    ord_mean = df_ord.groupby('lambda')['c_sv_mean'].mean()\n",
    "    ax.plot(ord_mean.index, ord_mean.values, 'b-o', linewidth=2, markersize=6, label='Ordered')\n",
    "if len(df_col) > 0:\n",
    "    col_mean = df_col.groupby('lambda')['c_sv_mean'].mean()\n",
    "    ax.plot(col_mean.index, col_mean.values, 'r-s', linewidth=2, markersize=6, label='Collapse')\n",
    "ax.set_xlabel('Î»')\n",
    "ax.set_ylabel('cos(g_struct, g_value)')\n",
    "ax.set_title('c_sv: Structure-Value Alignment')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. c_mc (cos mix-clean) by Î»\n",
    "ax = axes[0, 1]\n",
    "if len(df_ord) > 0:\n",
    "    ord_mean = df_ord.groupby('lambda')['c_mc_mean'].mean()\n",
    "    ax.plot(ord_mean.index, ord_mean.values, 'b-o', linewidth=2, markersize=6, label='Ordered')\n",
    "if len(df_col) > 0:\n",
    "    col_mean = df_col.groupby('lambda')['c_mc_mean'].mean()\n",
    "    ax.plot(col_mean.index, col_mean.values, 'r-s', linewidth=2, markersize=6, label='Collapse')\n",
    "ax.set_xlabel('Î»')\n",
    "ax.set_ylabel('cos(g_mix, g_clean)')\n",
    "ax.set_title('c_mc: Mix-Clean Alignment')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Test error by Î»\n",
    "ax = axes[0, 2]\n",
    "if len(df_ord) > 0:\n",
    "    ord_mean = df_ord.groupby('lambda')['error'].mean()\n",
    "    ax.plot(ord_mean.index, ord_mean.values, 'b-o', linewidth=2, markersize=6, label='Ordered')\n",
    "if len(df_col) > 0:\n",
    "    col_mean = df_col.groupby('lambda')['error'].mean()\n",
    "    ax.plot(col_mean.index, col_mean.values, 'r-s', linewidth=2, markersize=6, label='Collapse')\n",
    "ax.axhline(0.40, color='orange', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Î»')\n",
    "ax.set_ylabel('Test Error')\n",
    "ax.set_title('Performance')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Gradient norms\n",
    "ax = axes[1, 0]\n",
    "if len(df_ord) > 0:\n",
    "    ax.plot(df_ord.groupby('lambda')['norm_s_mean'].mean().index,\n",
    "            df_ord.groupby('lambda')['norm_s_mean'].mean().values, 'b-', linewidth=2, label='Ord ||g_s||')\n",
    "    ax.plot(df_ord.groupby('lambda')['norm_v_mean'].mean().index,\n",
    "            df_ord.groupby('lambda')['norm_v_mean'].mean().values, 'b--', linewidth=2, label='Ord ||g_v||')\n",
    "if len(df_col) > 0:\n",
    "    ax.plot(df_col.groupby('lambda')['norm_s_mean'].mean().index,\n",
    "            df_col.groupby('lambda')['norm_s_mean'].mean().values, 'r-', linewidth=2, label='Col ||g_s||')\n",
    "    ax.plot(df_col.groupby('lambda')['norm_v_mean'].mean().index,\n",
    "            df_col.groupby('lambda')['norm_v_mean'].mean().values, 'r--', linewidth=2, label='Col ||g_v||')\n",
    "ax.set_xlabel('Î»')\n",
    "ax.set_ylabel('Gradient Norm')\n",
    "ax.set_title('Gradient Magnitudes')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Loss values\n",
    "ax = axes[1, 1]\n",
    "if len(df_ord) > 0:\n",
    "    ax.plot(df_ord.groupby('lambda')['loss_s_mean'].mean().index,\n",
    "            df_ord.groupby('lambda')['loss_s_mean'].mean().values, 'b-', linewidth=2, label='Ord L_s')\n",
    "    ax.plot(df_ord.groupby('lambda')['loss_v_mean'].mean().index,\n",
    "            df_ord.groupby('lambda')['loss_v_mean'].mean().values, 'b--', linewidth=2, label='Ord L_v')\n",
    "if len(df_col) > 0:\n",
    "    ax.plot(df_col.groupby('lambda')['loss_s_mean'].mean().index,\n",
    "            df_col.groupby('lambda')['loss_s_mean'].mean().values, 'r-', linewidth=2, label='Col L_s')\n",
    "    ax.plot(df_col.groupby('lambda')['loss_v_mean'].mean().index,\n",
    "            df_col.groupby('lambda')['loss_v_mean'].mean().values, 'r--', linewidth=2, label='Col L_v')\n",
    "ax.set_xlabel('Î»')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Loss Values')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. c_sv vs error scatter\n",
    "ax = axes[1, 2]\n",
    "if len(df_ord) > 0:\n",
    "    ax.scatter(df_ord['c_sv_mean'], df_ord['error'], c='blue', alpha=0.6, label='Ordered', s=50)\n",
    "if len(df_col) > 0:\n",
    "    ax.scatter(df_col['c_sv_mean'], df_col['error'], c='red', alpha=0.6, label='Collapse', s=50)\n",
    "ax.set_xlabel('cos(g_struct, g_value)')\n",
    "ax.set_ylabel('Test Error')\n",
    "ax.set_title('Geometry-Performance Relationship')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR}/figures/{NOTEBOOK_ID}_geometry_comparison.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print('\\n' + '='*60)\n",
    "print(f'{NOTEBOOK_ID} GEOMETRY SUMMARY')\n",
    "print('='*60)\n",
    "\n",
    "if len(df_ord) > 0 and len(df_col) > 0:\n",
    "    print('\\nðŸ“Š Mean c_sv (struct-value alignment):')\n",
    "    print(f'   Ordered:  {df_ord[\"c_sv_mean\"].mean():.3f} Â± {df_ord[\"c_sv_mean\"].std():.3f}')\n",
    "    print(f'   Collapse: {df_col[\"c_sv_mean\"].mean():.3f} Â± {df_col[\"c_sv_mean\"].std():.3f}')\n",
    "    \n",
    "    print('\\nðŸ“Š Mean c_mc (mix-clean alignment):')\n",
    "    print(f'   Ordered:  {df_ord[\"c_mc_mean\"].mean():.3f} Â± {df_ord[\"c_mc_mean\"].std():.3f}')\n",
    "    print(f'   Collapse: {df_col[\"c_mc_mean\"].mean():.3f} Â± {df_col[\"c_mc_mean\"].std():.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
