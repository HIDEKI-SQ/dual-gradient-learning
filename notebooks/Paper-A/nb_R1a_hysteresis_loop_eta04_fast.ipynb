{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp R1a: Hysteresis Loop (Î·=0.4, Fast Sweep)\n",
    "\n",
    "## ç›®çš„\n",
    "çœŸã®ãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹ãƒ»ãƒ«ãƒ¼ãƒ—ã‚’æ¸¬å®šï¼šÎ»ã‚’æ®µéšçš„ã«ä¸Šæ˜‡â†’ä¸‹é™ã•ã›ã€m(Î»)ã®ä¸Šã‚Šæï¼ä¸‹ã‚Šæã‚’è¨˜éŒ²ã€‚\n",
    "\n",
    "## å®Ÿé¨“è¨­è¨ˆ\n",
    "- **Sweep**: Î» = 0.20 â†’ 0.65 â†’ 0.20 (stepwise)\n",
    "- **Step size**: Î”Î» = 0.02 (boundaryè¿‘å‚ 0.42-0.58 ã¯ Î”Î» = 0.01)\n",
    "- **Hold time**: Fast (1 epoch per Î» step)\n",
    "- **Î·**: 0.4\n",
    "- **Seeds**: 0-9 (10 runs)\n",
    "- **Total**: 10 runs\n",
    "\n",
    "## æ¨å®šæ™‚é–“\n",
    "~10 runs Ã— 45 min â‰ˆ **7.5 æ™‚é–“**\n",
    "\n",
    "## é–¢é€£ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯\n",
    "- R1a (this): Î·=0.4, fast\n",
    "- R1b: Î·=0.4, slow\n",
    "- R2a: Î·=0.8, fast\n",
    "- R2b: Î·=0.8, slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— =====\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "EXP_NAME = 'exp_R1_hysteresis_loop'\n",
    "NOTEBOOK_ID = 'R1a'  # ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®è­˜åˆ¥å­\n",
    "BASE_DIR = '/content/drive/MyDrive/dual-gradient-learning/Paper-A'\n",
    "\n",
    "# æ—¢å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¤œç´¢ï¼ˆR1a/b ã§å…±æœ‰ï¼‰\n",
    "existing = glob.glob(f'{BASE_DIR}/{EXP_NAME}_*')\n",
    "if existing:\n",
    "    SAVE_DIR = sorted(existing)[-1]\n",
    "    print(f'ğŸ”„ Resuming from: {SAVE_DIR}')\n",
    "else:\n",
    "    TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    SAVE_DIR = f'{BASE_DIR}/{EXP_NAME}_{TIMESTAMP}'\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    print(f'ğŸ†• New experiment: {SAVE_DIR}')\n",
    "\n",
    "os.makedirs(f'{SAVE_DIR}/figures', exist_ok=True)\n",
    "print(f'Save directory: {SAVE_DIR}')\n",
    "print(f'This notebook: {NOTEBOOK_ID} (Î·=0.4, fast)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils import parameters_to_vector\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ResNet18 for CIFAR-10 =====\n",
    "def get_resnet18():\n",
    "    model = resnet18(weights=None, num_classes=10)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "    return model\n",
    "\n",
    "# ===== IndexedDataset =====\n",
    "class IndexedDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        return img, label, idx\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Î» Sweep Schedule for Hysteresis Loop =====\n",
    "\n",
    "def generate_lambda_sweep(eta):\n",
    "    \"\"\"\n",
    "    Generate Î» values for hysteresis loop: up sweep + down sweep\n",
    "    Higher resolution near the transition boundary\n",
    "    \"\"\"\n",
    "    if eta == 0.4:\n",
    "        boundary_low, boundary_high = 0.42, 0.58\n",
    "    else:  # eta == 0.8\n",
    "        boundary_low, boundary_high = 0.38, 0.54\n",
    "    \n",
    "    lambda_values = []\n",
    "    \n",
    "    # Up sweep: 0.20 â†’ 0.65\n",
    "    lam = 0.20\n",
    "    while lam <= 0.65:\n",
    "        lambda_values.append(('up', round(lam, 3)))\n",
    "        # High resolution near boundary\n",
    "        if boundary_low <= lam <= boundary_high:\n",
    "            lam += 0.01\n",
    "        else:\n",
    "            lam += 0.02\n",
    "    \n",
    "    # Down sweep: 0.65 â†’ 0.20 (excluding 0.65 to avoid duplicate)\n",
    "    lam = 0.65 - 0.02\n",
    "    while lam >= 0.20:\n",
    "        lambda_values.append(('down', round(lam, 3)))\n",
    "        if boundary_low <= lam <= boundary_high:\n",
    "            lam -= 0.01\n",
    "        else:\n",
    "            lam -= 0.02\n",
    "    \n",
    "    return lambda_values\n",
    "\n",
    "# Test\n",
    "sweep = generate_lambda_sweep(0.4)\n",
    "up_count = sum(1 for s in sweep if s[0] == 'up')\n",
    "down_count = sum(1 for s in sweep if s[0] == 'down')\n",
    "print(f'Total Î» steps: {len(sweep)} (up: {up_count}, down: {down_count})')\n",
    "print(f'First 10: {sweep[:10]}')\n",
    "print(f'Last 10: {sweep[-10:]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== å®Ÿé¨“ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ =====\n",
    "BATCH_SIZE = 256\n",
    "NUM_WORKERS = 4\n",
    "LR = 0.1\n",
    "K = 16\n",
    "\n",
    "# ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å°‚ç”¨\n",
    "NOISE_RATE = 0.4\n",
    "HOLD_EPOCHS = 1  # Fast: 1 epoch per Î» step\n",
    "SWEEP_TYPE = 'fast'\n",
    "\n",
    "SEEDS = list(range(10))\n",
    "\n",
    "LAMBDA_SWEEP = generate_lambda_sweep(NOISE_RATE)\n",
    "TOTAL_EPOCHS = len(LAMBDA_SWEEP) * HOLD_EPOCHS\n",
    "\n",
    "experiments = [{'seed': s, 'noise_rate': NOISE_RATE, 'sweep_type': SWEEP_TYPE} for s in SEEDS]\n",
    "\n",
    "print(f'Noise rate: {NOISE_RATE}')\n",
    "print(f'Sweep type: {SWEEP_TYPE} (hold={HOLD_EPOCHS} epochs per Î»)')\n",
    "print(f'Î» steps: {len(LAMBDA_SWEEP)}')\n",
    "print(f'Total epochs per run: {TOTAL_EPOCHS}')\n",
    "print(f'Seeds: {SEEDS}')\n",
    "print(f'Total experiments: {len(experiments)}')\n",
    "print(f'Estimated time: {len(experiments) * TOTAL_EPOCHS * 0.5 / 60:.1f} hours')\n",
    "\n",
    "# Save config\n",
    "config = {\n",
    "    'experiment': EXP_NAME,\n",
    "    'notebook_id': NOTEBOOK_ID,\n",
    "    'description': 'Hysteresis loop measurement with stepwise Î» sweep',\n",
    "    'parameters': {\n",
    "        'noise_rate': NOISE_RATE,\n",
    "        'sweep_type': SWEEP_TYPE,\n",
    "        'hold_epochs': HOLD_EPOCHS,\n",
    "        'lambda_steps': len(LAMBDA_SWEEP),\n",
    "        'total_epochs': TOTAL_EPOCHS,\n",
    "        'seeds': SEEDS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'lr': LR,\n",
    "        'K': K\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f'{SAVE_DIR}/{NOTEBOOK_ID}_config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print(f'Config saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° =====\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def inject_label_noise(labels, noise_rate, seed):\n",
    "    np.random.seed(seed)\n",
    "    noisy_labels = labels.copy()\n",
    "    n_samples = len(labels)\n",
    "    n_noisy = int(noise_rate * n_samples)\n",
    "    noisy_indices = np.random.choice(n_samples, n_noisy, replace=False)\n",
    "    for idx in noisy_indices:\n",
    "        original = labels[idx]\n",
    "        new_label = np.random.choice([l for l in range(10) if l != original])\n",
    "        noisy_labels[idx] = new_label\n",
    "    return noisy_labels\n",
    "\n",
    "def load_cifar10():\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "    return trainset, testset\n",
    "\n",
    "def get_data_loaders(trainset, testset):\n",
    "    train_loader = DataLoader(IndexedDataset(trainset), batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    test_loader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Hysteresis Loop Training =====\n",
    "def train_hysteresis_loop(model, train_loader, test_loader, clean_labels, noisy_labels, lambda_sweep, hold_epochs):\n",
    "    \"\"\"\n",
    "    Train with stepwise Î» sweep, recording test error at each Î» step.\n",
    "    \"\"\"\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    clean_labels_tensor = torch.tensor(clean_labels, device=device)\n",
    "    noisy_labels_tensor = torch.tensor(noisy_labels, device=device)\n",
    "    \n",
    "    cached_g_value = None\n",
    "    global_step = 0\n",
    "    global_epoch = 0\n",
    "    \n",
    "    # Results storage\n",
    "    loop_data = []  # (direction, Î», test_error, test_acc)\n",
    "    epoch_history = []  # Full epoch-by-epoch history\n",
    "    \n",
    "    total_steps = len(lambda_sweep)\n",
    "    \n",
    "    for step_idx, (direction, lam) in enumerate(lambda_sweep):\n",
    "        # LR scheduling based on total epochs\n",
    "        if global_epoch == 50:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = LR * 0.1\n",
    "        elif global_epoch == 75:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = LR * 0.01\n",
    "        \n",
    "        # Train for hold_epochs at this Î»\n",
    "        for hold_ep in range(hold_epochs):\n",
    "            model.train()\n",
    "            epoch_cos = []\n",
    "            \n",
    "            for inputs, _, indices in train_loader:\n",
    "                inputs = inputs.to(device, non_blocking=True)\n",
    "                indices = indices.to(device, non_blocking=True)\n",
    "                batch_noisy = noisy_labels_tensor[indices]\n",
    "                batch_clean = clean_labels_tensor[indices]\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss_struct = criterion(outputs, batch_noisy)\n",
    "                loss_struct.backward(retain_graph=True)\n",
    "                g_struct = parameters_to_vector([p.grad for p in model.parameters()]).clone()\n",
    "                \n",
    "                if global_step % K == 0 or cached_g_value is None:\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss_value = criterion(outputs, batch_clean)\n",
    "                    loss_value.backward()\n",
    "                    cached_g_value = parameters_to_vector([p.grad for p in model.parameters()]).clone()\n",
    "                \n",
    "                g_struct_norm = g_struct / (g_struct.norm() + 1e-12)\n",
    "                g_value_norm = cached_g_value / (cached_g_value.norm() + 1e-12)\n",
    "                \n",
    "                cos_sim = (g_struct_norm @ g_value_norm).item()\n",
    "                epoch_cos.append(cos_sim)\n",
    "                \n",
    "                g_mix = (1 - lam) * g_struct_norm + lam * g_value_norm\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                idx = 0\n",
    "                for p in model.parameters():\n",
    "                    numel = p.numel()\n",
    "                    p.grad = g_mix[idx:idx+numel].view(p.shape).clone()\n",
    "                    idx += numel\n",
    "                optimizer.step()\n",
    "                global_step += 1\n",
    "            \n",
    "            global_epoch += 1\n",
    "        \n",
    "        # Evaluate at end of this Î» step\n",
    "        test_acc = evaluate(model, test_loader)\n",
    "        test_error = 1 - test_acc\n",
    "        avg_cos = np.mean(epoch_cos) if epoch_cos else 0\n",
    "        \n",
    "        loop_data.append({\n",
    "            'step_idx': step_idx,\n",
    "            'direction': direction,\n",
    "            'lambda': lam,\n",
    "            'test_error': test_error,\n",
    "            'test_acc': test_acc,\n",
    "            'avg_cos': avg_cos,\n",
    "            'global_epoch': global_epoch\n",
    "        })\n",
    "        \n",
    "        if (step_idx + 1) % 10 == 0:\n",
    "            print(f'    Step {step_idx+1}/{total_steps}: {direction} Î»={lam:.3f} â†’ error={test_error:.4f}')\n",
    "    \n",
    "    return loop_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ãƒ‡ãƒ¼ã‚¿æº–å‚™ =====\n",
    "trainset, testset = load_cifar10()\n",
    "clean_labels = np.array(trainset.targets)\n",
    "train_loader, test_loader = get_data_loaders(trainset, testset)\n",
    "\n",
    "print('Data prepared')\n",
    "\n",
    "# GPU warmup\n",
    "warmup_model = get_resnet18().to(device)\n",
    "for _ in range(20):\n",
    "    _ = warmup_model(torch.randn(BATCH_SIZE, 3, 32, 32, device=device))\n",
    "del warmup_model\n",
    "torch.cuda.empty_cache()\n",
    "print('GPU warmed up')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ãƒ¡ã‚¤ãƒ³å®Ÿé¨“ãƒ«ãƒ¼ãƒ— =====\n",
    "results = []\n",
    "checkpoint_file = f'{SAVE_DIR}/{NOTEBOOK_ID}_checkpoint.json'\n",
    "completed = set()\n",
    "\n",
    "if os.path.exists(checkpoint_file):\n",
    "    with open(checkpoint_file, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    for r in results:\n",
    "        completed.add((r['noise_rate'], r['sweep_type'], r['seed']))\n",
    "    print(f'Checkpoint loaded: {len(results)} runs completed')\n",
    "\n",
    "total = len(experiments)\n",
    "\n",
    "for exp in experiments:\n",
    "    seed = exp['seed']\n",
    "    eta = exp['noise_rate']\n",
    "    stype = exp['sweep_type']\n",
    "    \n",
    "    if (eta, stype, seed) in completed:\n",
    "        continue\n",
    "    \n",
    "    run_num = len(completed) + 1\n",
    "    print(f'\\n[{run_num}/{total}] Î·={eta} sweep={stype} seed={seed}')\n",
    "    \n",
    "    set_seed(seed)\n",
    "    noisy_labels = inject_label_noise(clean_labels, eta, seed)\n",
    "    \n",
    "    model = get_resnet18().to(device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    loop_data = train_hysteresis_loop(\n",
    "        model, train_loader, test_loader, \n",
    "        clean_labels, noisy_labels, \n",
    "        LAMBDA_SWEEP, HOLD_EPOCHS\n",
    "    )\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    # Extract up/down branches\n",
    "    up_branch = [d for d in loop_data if d['direction'] == 'up']\n",
    "    down_branch = [d for d in loop_data if d['direction'] == 'down']\n",
    "    \n",
    "    result = {\n",
    "        'experiment_id': f'{NOTEBOOK_ID}-{run_num:03d}',\n",
    "        'experiment': EXP_NAME,\n",
    "        'notebook_id': NOTEBOOK_ID,\n",
    "        'noise_rate': eta,\n",
    "        'sweep_type': stype,\n",
    "        'hold_epochs': HOLD_EPOCHS,\n",
    "        'seed': seed,\n",
    "        'time_seconds': elapsed,\n",
    "        'loop_data': loop_data,\n",
    "        'up_branch': up_branch,\n",
    "        'down_branch': down_branch\n",
    "    }\n",
    "    results.append(result)\n",
    "    completed.add((eta, stype, seed))\n",
    "    \n",
    "    # Summary\n",
    "    up_errors = [d['test_error'] for d in up_branch]\n",
    "    down_errors = [d['test_error'] for d in down_branch]\n",
    "    print(f'  Up branch: min={min(up_errors):.4f}, max={max(up_errors):.4f}')\n",
    "    print(f'  Down branch: min={min(down_errors):.4f}, max={max(down_errors):.4f}')\n",
    "    print(f'  Time: {elapsed/60:.1f} min')\n",
    "    \n",
    "    # Save checkpoint\n",
    "    with open(checkpoint_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    remaining = total - run_num\n",
    "    eta_hours = remaining * elapsed / 3600\n",
    "    print(f'  Progress: {run_num}/{total} | ETA: {eta_hours:.1f} hours')\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print(f'{NOTEBOOK_ID} COMPLETED!')\n",
    "print('='*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== çµæœä¿å­˜ =====\n",
    "import pandas as pd\n",
    "\n",
    "with open(f'{SAVE_DIR}/{NOTEBOOK_ID}_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "# Flatten for CSV\n",
    "rows = []\n",
    "for r in results:\n",
    "    for d in r['loop_data']:\n",
    "        rows.append({\n",
    "            'experiment_id': r['experiment_id'],\n",
    "            'noise_rate': r['noise_rate'],\n",
    "            'sweep_type': r['sweep_type'],\n",
    "            'seed': r['seed'],\n",
    "            'step_idx': d['step_idx'],\n",
    "            'direction': d['direction'],\n",
    "            'lambda': d['lambda'],\n",
    "            'test_error': d['test_error'],\n",
    "            'test_acc': d['test_acc'],\n",
    "            'avg_cos': d['avg_cos']\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(f'{SAVE_DIR}/{NOTEBOOK_ID}_results.csv', index=False)\n",
    "\n",
    "print(f'Results saved to {SAVE_DIR}')\n",
    "print(f'Total data points: {len(rows)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹ãƒ»ãƒ«ãƒ¼ãƒ—å¯è¦–åŒ– =====\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Individual loops\n",
    "ax = axes[0]\n",
    "for r in results:\n",
    "    up_lam = [d['lambda'] for d in r['up_branch']]\n",
    "    up_err = [d['test_error'] for d in r['up_branch']]\n",
    "    down_lam = [d['lambda'] for d in r['down_branch']]\n",
    "    down_err = [d['test_error'] for d in r['down_branch']]\n",
    "    \n",
    "    ax.plot(up_lam, up_err, 'b-', alpha=0.3, linewidth=0.8)\n",
    "    ax.plot(down_lam, down_err, 'r-', alpha=0.3, linewidth=0.8)\n",
    "\n",
    "ax.set_xlabel('Î»', fontsize=12)\n",
    "ax.set_ylabel('Test Error', fontsize=12)\n",
    "ax.set_title(f'Hysteresis Loop (Î·={NOISE_RATE}, {SWEEP_TYPE})', fontsize=12)\n",
    "ax.legend(['Up sweep', 'Down sweep'], loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Mean loop with std\n",
    "ax = axes[1]\n",
    "\n",
    "# Aggregate by Î» and direction\n",
    "df_up = df[df['direction'] == 'up'].groupby('lambda')['test_error'].agg(['mean', 'std']).reset_index()\n",
    "df_down = df[df['direction'] == 'down'].groupby('lambda')['test_error'].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "ax.errorbar(df_up['lambda'], df_up['mean'], yerr=df_up['std'], \n",
    "            fmt='b-o', markersize=3, capsize=2, label='Up sweep', alpha=0.8)\n",
    "ax.errorbar(df_down['lambda'], df_down['mean'], yerr=df_down['std'], \n",
    "            fmt='r-o', markersize=3, capsize=2, label='Down sweep', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Î»', fontsize=12)\n",
    "ax.set_ylabel('Test Error (mean Â± std)', fontsize=12)\n",
    "ax.set_title(f'Mean Hysteresis Loop (Î·={NOISE_RATE}, {SWEEP_TYPE}, n={len(SEEDS)})', fontsize=12)\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR}/figures/{NOTEBOOK_ID}_hysteresis_loop.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Hysteresis area estimate\n",
    "print('\\n--- Hysteresis Analysis ---')\n",
    "common_lambdas = set(df_up['lambda']) & set(df_down['lambda'])\n",
    "if common_lambdas:\n",
    "    area = 0\n",
    "    sorted_lambdas = sorted(common_lambdas)\n",
    "    for i in range(len(sorted_lambdas) - 1):\n",
    "        lam1, lam2 = sorted_lambdas[i], sorted_lambdas[i+1]\n",
    "        up1 = df_up[df_up['lambda'] == lam1]['mean'].values[0]\n",
    "        up2 = df_up[df_up['lambda'] == lam2]['mean'].values[0]\n",
    "        down1 = df_down[df_down['lambda'] == lam1]['mean'].values[0]\n",
    "        down2 = df_down[df_down['lambda'] == lam2]['mean'].values[0]\n",
    "        # Trapezoidal area\n",
    "        area += abs((down1 - up1) + (down2 - up2)) / 2 * (lam2 - lam1)\n",
    "    print(f'Estimated loop area: {area:.6f}')\n",
    "\n",
    "print(f'\\nNext: Run R1b (slow sweep) for comparison')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
