{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp J: Mechanism Decomposition - Why Misaligned Gradients Improve\n",
    "\n",
    "## 目的\n",
    "Misaligned（random/anti-label）が低λで改善する「パラドックス」の機構を分解・特定する。\n",
    "\n",
    "## 仮説\n",
    "1. **暗黙のLR低下仮説**: ||g_mix|| < 1 による効果的学習率低下\n",
    "2. **直交ノイズ仮説**: g_structに直交する成分が正則化/探索効果を持つ\n",
    "3. **幾何効果仮説**: 上記では説明できない表現学習の相互作用\n",
    "\n",
    "## 実験設計\n",
    "- **ノイズ率**: η = 0.4\n",
    "- **λ**: 0.1, 0.2, 0.3\n",
    "- **Seeds**: 0, 1, 2\n",
    "\n",
    "### 比較条件（5種類）\n",
    "1. **CE_baseline**: 通常のCE学習\n",
    "2. **Misaligned_raw**: Random label mix（renorm=False）\n",
    "3. **Misaligned_renorm**: Random label mix（renorm=True）\n",
    "4. **LR_matched**: ||g_mix||に合わせてLRを下げたCE\n",
    "5. **Orthogonal_noise**: g_structに直交ノイズを同強度で加える\n",
    "\n",
    "## Runs計算\n",
    "5条件 × 3λ × 3seeds = **45 runs** + baseline 3 = **48 runs**\n",
    "\n",
    "## 判定ロジック\n",
    "- Misaligned_renorm で改善が消える → 暗黙LR低下が主因\n",
    "- Misaligned_renorm でも改善が残り、Orthogonal_noise で再現 → 直交ノイズ効果が主因\n",
    "- どれでも再現しない → 幾何効果（未解明の相互作用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== セットアップ =====\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "EXP_NAME = 'exp_J_mechanism_decomposition'\n",
    "TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "BASE_DIR = '/content/drive/MyDrive/dual-gradient-learning/Paper-A'\n",
    "SAVE_DIR = f'{BASE_DIR}/{EXP_NAME}_{TIMESTAMP}'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR}/figures', exist_ok=True)\n",
    "\n",
    "print(f'Experiment: {EXP_NAME}')\n",
    "print(f'Timestamp: {TIMESTAMP}')\n",
    "print(f'Save directory: {SAVE_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils import parameters_to_vector\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== モデル定義 =====\n",
    "class IndexedDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        return img, label, idx\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, 3, stride, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, 3, 1, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(planes))\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        return F.relu(out)\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.in_planes = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, 1, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(64, 2, 1)\n",
    "        self.layer2 = self._make_layer(128, 2, 2)\n",
    "        self.layer3 = self._make_layer(256, 2, 2)\n",
    "        self.layer4 = self._make_layer(512, 2, 2)\n",
    "        self.linear = nn.Linear(512, num_classes)\n",
    "    def _make_layer(self, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for s in strides:\n",
    "            layers.append(BasicBlock(self.in_planes, planes, s))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.adaptive_avg_pool2d(out, 1)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        return self.linear(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 実験パラメータ =====\n",
    "BATCH_SIZE = 256\n",
    "NUM_WORKERS = 4\n",
    "EPOCHS = 100\n",
    "LR = 0.1\n",
    "K = 16\n",
    "\n",
    "NOISE_RATE = 0.4\n",
    "LAMBDAS = [0.1, 0.2, 0.3]\n",
    "SEEDS = [0, 1, 2]\n",
    "\n",
    "# 実験条件\n",
    "CONDITIONS = [\n",
    "    {'name': 'CE_baseline',      'method': 'ce'},\n",
    "    {'name': 'Misaligned_raw',   'method': 'misaligned', 'renorm': False},\n",
    "    {'name': 'Misaligned_renorm','method': 'misaligned', 'renorm': True},\n",
    "    {'name': 'LR_matched',       'method': 'lr_matched'},\n",
    "    {'name': 'Orthogonal_noise', 'method': 'orthogonal_noise'},\n",
    "]\n",
    "\n",
    "# 実験条件リスト生成\n",
    "experiments = []\n",
    "for cond in CONDITIONS:\n",
    "    if cond['method'] == 'ce':\n",
    "        # baselineはλ不要、seed分だけ\n",
    "        for seed in SEEDS:\n",
    "            experiments.append({**cond, 'lambda': None, 'seed': seed})\n",
    "    else:\n",
    "        for lam in LAMBDAS:\n",
    "            for seed in SEEDS:\n",
    "                experiments.append({**cond, 'lambda': lam, 'seed': seed})\n",
    "\n",
    "total_runs = len(experiments)\n",
    "print(f'Total runs: {total_runs}')\n",
    "print(f'Estimated time: {total_runs * 9.5 / 60:.1f} hours')\n",
    "\n",
    "# config保存\n",
    "config = {\n",
    "    'experiment': EXP_NAME,\n",
    "    'timestamp': TIMESTAMP,\n",
    "    'parameters': {\n",
    "        'conditions': [c['name'] for c in CONDITIONS],\n",
    "        'lambdas': LAMBDAS,\n",
    "        'seeds': SEEDS,\n",
    "        'noise_rate': NOISE_RATE,\n",
    "        'epochs': EPOCHS,\n",
    "        'K': K\n",
    "    },\n",
    "    'total_runs': total_runs\n",
    "}\n",
    "with open(f'{SAVE_DIR}/config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print(f'Config saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ユーティリティ関数 =====\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def load_cifar10():\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "    return trainset, testset\n",
    "\n",
    "def get_data_loaders(trainset, testset):\n",
    "    indexed_trainset = IndexedDataset(trainset)\n",
    "    train_loader = DataLoader(indexed_trainset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                              num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, drop_last=True)\n",
    "    test_loader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                             num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def inject_noise(labels, noise_rate, seed):\n",
    "    np.random.seed(seed + 1000)\n",
    "    noisy_labels = labels.copy()\n",
    "    n_noisy = int(noise_rate * len(labels))\n",
    "    noisy_indices = np.random.choice(len(labels), n_noisy, replace=False)\n",
    "    for idx in noisy_indices:\n",
    "        choices = [i for i in range(10) if i != labels[idx]]\n",
    "        noisy_labels[idx] = np.random.choice(choices)\n",
    "    return noisy_labels\n",
    "\n",
    "def generate_random_labels(labels, seed):\n",
    "    \"\"\"完全ランダムなラベルを生成（misaligned用）\"\"\"\n",
    "    np.random.seed(seed + 2000)\n",
    "    return np.random.randint(0, 10, size=len(labels))\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 学習関数群 =====\n",
    "\n",
    "def train_ce_baseline(model, train_loader, test_loader, noisy_labels):\n",
    "    \"\"\"通常のCE学習（baseline）\"\"\"\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 75], gamma=0.1)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    noisy_labels_tensor = torch.tensor(noisy_labels, device=device)\n",
    "    \n",
    "    best_acc = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        for inputs, _, indices in train_loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            indices = indices.to(device, non_blocking=True)\n",
    "            batch_labels = noisy_labels_tensor[indices]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            best_acc = max(best_acc, evaluate(model, test_loader))\n",
    "    \n",
    "    final_acc = evaluate(model, test_loader)\n",
    "    return final_acc, max(best_acc, final_acc), {'avg_gmix_norm': 1.0}\n",
    "\n",
    "\n",
    "def train_misaligned(model, train_loader, test_loader, noisy_labels, random_labels, lam, renorm):\n",
    "    \"\"\"Misaligned gradient mixing（random labels）\"\"\"\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 75], gamma=0.1)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    noisy_labels_tensor = torch.tensor(noisy_labels, device=device)\n",
    "    random_labels_tensor = torch.tensor(random_labels, device=device)\n",
    "    \n",
    "    cached_value_grad = None\n",
    "    global_step = 0\n",
    "    best_acc = 0\n",
    "    gmix_norms = []\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        for inputs, _, indices in train_loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            indices = indices.to(device, non_blocking=True)\n",
    "            batch_noisy = noisy_labels_tensor[indices]\n",
    "            batch_random = random_labels_tensor[indices]\n",
    "            \n",
    "            # Structure gradient\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss_struct = criterion(outputs, batch_noisy)\n",
    "            loss_struct.backward(retain_graph=True)\n",
    "            g_struct = parameters_to_vector([p.grad for p in model.parameters()]).clone()\n",
    "            \n",
    "            # Value gradient (from random labels)\n",
    "            if global_step % K == 0 or cached_value_grad is None:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss_value = criterion(outputs, batch_random)\n",
    "                loss_value.backward()\n",
    "                cached_value_grad = parameters_to_vector([p.grad for p in model.parameters()]).clone()\n",
    "            \n",
    "            # Normalize and mix\n",
    "            g_struct_norm = g_struct / (g_struct.norm() + 1e-12)\n",
    "            g_value_norm = cached_value_grad / (cached_value_grad.norm() + 1e-12)\n",
    "            g_mix = (1 - lam) * g_struct_norm + lam * g_value_norm\n",
    "            \n",
    "            gmix_norms.append(g_mix.norm().item())\n",
    "            \n",
    "            if renorm:\n",
    "                g_mix = g_mix / (g_mix.norm() + 1e-12)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            idx = 0\n",
    "            for p in model.parameters():\n",
    "                numel = p.numel()\n",
    "                p.grad = g_mix[idx:idx+numel].view(p.shape).clone()\n",
    "                idx += numel\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "        \n",
    "        scheduler.step()\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            best_acc = max(best_acc, evaluate(model, test_loader))\n",
    "    \n",
    "    final_acc = evaluate(model, test_loader)\n",
    "    return final_acc, max(best_acc, final_acc), {'avg_gmix_norm': np.mean(gmix_norms)}\n",
    "\n",
    "\n",
    "def train_lr_matched(model, train_loader, test_loader, noisy_labels, lr_scale):\n",
    "    \"\"\"LRを下げたCE学習（暗黙LR低下仮説の検証）\"\"\"\n",
    "    adjusted_lr = LR * lr_scale\n",
    "    optimizer = optim.SGD(model.parameters(), lr=adjusted_lr, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 75], gamma=0.1)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    noisy_labels_tensor = torch.tensor(noisy_labels, device=device)\n",
    "    \n",
    "    best_acc = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        for inputs, _, indices in train_loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            indices = indices.to(device, non_blocking=True)\n",
    "            batch_labels = noisy_labels_tensor[indices]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            best_acc = max(best_acc, evaluate(model, test_loader))\n",
    "    \n",
    "    final_acc = evaluate(model, test_loader)\n",
    "    return final_acc, max(best_acc, final_acc), {'lr_scale': lr_scale}\n",
    "\n",
    "\n",
    "def train_orthogonal_noise(model, train_loader, test_loader, noisy_labels, lam):\n",
    "    \"\"\"構造勾配に直交ノイズを加える（直交ノイズ仮説の検証）\"\"\"\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 75], gamma=0.1)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    noisy_labels_tensor = torch.tensor(noisy_labels, device=device)\n",
    "    \n",
    "    param_dim = sum(p.numel() for p in model.parameters())\n",
    "    best_acc = 0\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        for inputs, _, indices in train_loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            indices = indices.to(device, non_blocking=True)\n",
    "            batch_labels = noisy_labels_tensor[indices]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            g_struct = parameters_to_vector([p.grad for p in model.parameters()]).clone()\n",
    "            g_struct_norm = g_struct / (g_struct.norm() + 1e-12)\n",
    "            \n",
    "            # 直交ノイズを生成\n",
    "            random_vec = torch.randn(param_dim, device=device)\n",
    "            proj = (random_vec @ g_struct_norm) * g_struct_norm\n",
    "            orthogonal = random_vec - proj\n",
    "            orthogonal_norm = orthogonal / (orthogonal.norm() + 1e-12)\n",
    "            \n",
    "            # 混合（同じλで）\n",
    "            g_mix = (1 - lam) * g_struct_norm + lam * orthogonal_norm\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            idx = 0\n",
    "            for p in model.parameters():\n",
    "                numel = p.numel()\n",
    "                p.grad = g_mix[idx:idx+numel].view(p.shape).clone()\n",
    "                idx += numel\n",
    "            optimizer.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            best_acc = max(best_acc, evaluate(model, test_loader))\n",
    "    \n",
    "    final_acc = evaluate(model, test_loader)\n",
    "    return final_acc, max(best_acc, final_acc), {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== データ準備 =====\n",
    "trainset, testset = load_cifar10()\n",
    "clean_labels = np.array(trainset.targets)\n",
    "train_loader, test_loader = get_data_loaders(trainset, testset)\n",
    "\n",
    "# ノイズラベルとランダムラベルを事前生成\n",
    "noisy_labels = inject_noise(clean_labels, NOISE_RATE, seed=0)\n",
    "random_labels = generate_random_labels(clean_labels, seed=0)\n",
    "\n",
    "print('Data prepared')\n",
    "print(f'  Noisy labels: {np.mean(noisy_labels != clean_labels)*100:.1f}% corrupted')\n",
    "print(f'  Random labels: {np.mean(random_labels != clean_labels)*100:.1f}% different from clean')\n",
    "\n",
    "# GPU warmup\n",
    "warmup_model = ResNet18().to(device)\n",
    "for _ in range(20):\n",
    "    _ = warmup_model(torch.randn(BATCH_SIZE, 3, 32, 32, device=device))\n",
    "del warmup_model\n",
    "torch.cuda.empty_cache()\n",
    "print('Warmup complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ||g_mix|| の事前測定（LR_matched用） =====\n",
    "# Misaligned_rawでの平均||g_mix||を測定して、LR_matchedのスケールを決定\n",
    "\n",
    "print('Measuring ||g_mix|| for LR matching...')\n",
    "gmix_norm_by_lambda = {}\n",
    "\n",
    "for lam in LAMBDAS:\n",
    "    set_seed(0)\n",
    "    model = ResNet18().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    noisy_labels_tensor = torch.tensor(noisy_labels, device=device)\n",
    "    random_labels_tensor = torch.tensor(random_labels, device=device)\n",
    "    \n",
    "    norms = []\n",
    "    for batch_idx, (inputs, _, indices) in enumerate(train_loader):\n",
    "        if batch_idx >= 10:  # 最初の10バッチで推定\n",
    "            break\n",
    "        inputs = inputs.to(device)\n",
    "        indices = indices.to(device)\n",
    "        batch_noisy = noisy_labels_tensor[indices]\n",
    "        batch_random = random_labels_tensor[indices]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        criterion(outputs, batch_noisy).backward(retain_graph=True)\n",
    "        g_struct = parameters_to_vector([p.grad for p in model.parameters()]).clone()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        criterion(outputs, batch_random).backward()\n",
    "        g_value = parameters_to_vector([p.grad for p in model.parameters()]).clone()\n",
    "        \n",
    "        g_struct_norm = g_struct / (g_struct.norm() + 1e-12)\n",
    "        g_value_norm = g_value / (g_value.norm() + 1e-12)\n",
    "        g_mix = (1 - lam) * g_struct_norm + lam * g_value_norm\n",
    "        norms.append(g_mix.norm().item())\n",
    "    \n",
    "    gmix_norm_by_lambda[lam] = np.mean(norms)\n",
    "    print(f'  λ={lam}: avg ||g_mix|| = {gmix_norm_by_lambda[lam]:.4f}')\n",
    "    del model\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== メイン実験ループ =====\n",
    "results = []\n",
    "checkpoint_file = f'{SAVE_DIR}/checkpoint.json'\n",
    "completed = set()\n",
    "\n",
    "if os.path.exists(checkpoint_file):\n",
    "    with open(checkpoint_file, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    for r in results:\n",
    "        completed.add((r['condition'], r.get('lambda'), r['seed']))\n",
    "    print(f'Checkpoint loaded: {len(completed)} runs completed')\n",
    "\n",
    "run_counter = 0\n",
    "exp_start = time.time()\n",
    "\n",
    "for exp in experiments:\n",
    "    run_counter += 1\n",
    "    cond_name = exp['name']\n",
    "    method = exp['method']\n",
    "    lam = exp.get('lambda')\n",
    "    seed = exp['seed']\n",
    "    \n",
    "    key = (cond_name, lam, seed)\n",
    "    if key in completed:\n",
    "        continue\n",
    "    \n",
    "    lam_str = f'λ={lam:.1f}' if lam else 'N/A'\n",
    "    print(f'\\n[{run_counter}/{total_runs}] {cond_name} {lam_str} seed={seed}')\n",
    "    t0 = time.time()\n",
    "    \n",
    "    set_seed(seed)\n",
    "    model = ResNet18().to(device)\n",
    "    \n",
    "    # 条件に応じて学習関数を選択\n",
    "    if method == 'ce':\n",
    "        final_acc, best_acc, extra = train_ce_baseline(model, train_loader, test_loader, noisy_labels)\n",
    "    elif method == 'misaligned':\n",
    "        renorm = exp.get('renorm', False)\n",
    "        final_acc, best_acc, extra = train_misaligned(\n",
    "            model, train_loader, test_loader, noisy_labels, random_labels, lam, renorm\n",
    "        )\n",
    "    elif method == 'lr_matched':\n",
    "        lr_scale = gmix_norm_by_lambda[lam]\n",
    "        final_acc, best_acc, extra = train_lr_matched(\n",
    "            model, train_loader, test_loader, noisy_labels, lr_scale\n",
    "        )\n",
    "    elif method == 'orthogonal_noise':\n",
    "        final_acc, best_acc, extra = train_orthogonal_noise(\n",
    "            model, train_loader, test_loader, noisy_labels, lam\n",
    "        )\n",
    "    \n",
    "    elapsed = time.time() - t0\n",
    "    \n",
    "    result = {\n",
    "        'experiment': EXP_NAME,\n",
    "        'condition': cond_name,\n",
    "        'method': method,\n",
    "        'lambda': lam,\n",
    "        'seed': seed,\n",
    "        'noise_rate': NOISE_RATE,\n",
    "        'test_acc': final_acc,\n",
    "        'test_error': 1 - final_acc,\n",
    "        'best_acc': best_acc,\n",
    "        'best_error': 1 - best_acc,\n",
    "        'time_seconds': elapsed,\n",
    "        **extra\n",
    "    }\n",
    "    results.append(result)\n",
    "    \n",
    "    with open(checkpoint_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f'  Error: {1-final_acc:.4f} | Time: {elapsed/60:.1f} min')\n",
    "    \n",
    "    completed_count = len(results)\n",
    "    avg_time = sum([r['time_seconds'] for r in results]) / completed_count\n",
    "    remaining = total_runs - completed_count\n",
    "    eta_hours = (remaining * avg_time) / 3600\n",
    "    print(f'  Progress: {completed_count}/{total_runs} | ETA: {eta_hours:.1f} hours')\n",
    "\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'EXPERIMENT COMPLETE')\n",
    "print(f'Total time: {(time.time()-exp_start)/3600:.2f} hours')\n",
    "print(f'{\"=\"*60}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 結果保存 =====\n",
    "import pandas as pd\n",
    "\n",
    "with open(f'{SAVE_DIR}/results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(f'{SAVE_DIR}/results.csv', index=False)\n",
    "\n",
    "print(f'Results saved to {SAVE_DIR}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 機構分析 =====\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "print('='*70)\n",
    "print('MECHANISM DECOMPOSITION ANALYSIS')\n",
    "print('='*70)\n",
    "\n",
    "# CE baseline\n",
    "ce_error = df[df['condition'] == 'CE_baseline']['test_error'].mean()\n",
    "print(f'\\nCE Baseline Error: {ce_error:.4f}')\n",
    "\n",
    "print('\\n--- Comparison by λ ---')\n",
    "for lam in LAMBDAS:\n",
    "    print(f'\\nλ = {lam}:')\n",
    "    \n",
    "    df_l = df[df['lambda'] == lam]\n",
    "    \n",
    "    mis_raw = df_l[df_l['condition'] == 'Misaligned_raw']['test_error'].mean()\n",
    "    mis_renorm = df_l[df_l['condition'] == 'Misaligned_renorm']['test_error'].mean()\n",
    "    lr_matched = df_l[df_l['condition'] == 'LR_matched']['test_error'].mean()\n",
    "    orth_noise = df_l[df_l['condition'] == 'Orthogonal_noise']['test_error'].mean()\n",
    "    \n",
    "    print(f'  Misaligned_raw:   {mis_raw:.4f} (Δ vs CE: {mis_raw - ce_error:+.4f})')\n",
    "    print(f'  Misaligned_renorm: {mis_renorm:.4f} (Δ vs CE: {mis_renorm - ce_error:+.4f})')\n",
    "    print(f'  LR_matched:       {lr_matched:.4f} (Δ vs CE: {lr_matched - ce_error:+.4f})')\n",
    "    print(f'  Orthogonal_noise: {orth_noise:.4f} (Δ vs CE: {orth_noise - ce_error:+.4f})')\n",
    "    \n",
    "    # 判定ロジック\n",
    "    print('\\n  Interpretation:')\n",
    "    if mis_raw < ce_error and mis_renorm >= ce_error:\n",
    "        print('  → 暗黙LR低下が主因（renormで改善が消えた）')\n",
    "    elif mis_raw < ce_error and mis_renorm < ce_error:\n",
    "        if orth_noise < ce_error:\n",
    "            print('  → 直交ノイズ効果が主因（orthogonal_noiseで再現）')\n",
    "        else:\n",
    "            print('  → 幾何効果（未解明の相互作用）')\n",
    "    elif mis_raw >= ce_error:\n",
    "        print('  → Misalignedによる改善なし')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 可視化 =====\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# CE baseline\n",
    "ce_error = df[df['condition'] == 'CE_baseline']['test_error'].mean()\n",
    "ce_std = df[df['condition'] == 'CE_baseline']['test_error'].std()\n",
    "\n",
    "conditions_to_plot = ['Misaligned_raw', 'Misaligned_renorm', 'LR_matched', 'Orthogonal_noise']\n",
    "colors = ['C0', 'C1', 'C2', 'C3']\n",
    "x_positions = np.arange(len(LAMBDAS))\n",
    "width = 0.2\n",
    "\n",
    "for i, cond in enumerate(conditions_to_plot):\n",
    "    errors = []\n",
    "    stds = []\n",
    "    for lam in LAMBDAS:\n",
    "        df_subset = df[(df['condition'] == cond) & (df['lambda'] == lam)]\n",
    "        errors.append(df_subset['test_error'].mean())\n",
    "        stds.append(df_subset['test_error'].std())\n",
    "    \n",
    "    ax.bar(x_positions + i*width, errors, width, yerr=stds, \n",
    "           label=cond, color=colors[i], capsize=3, alpha=0.8)\n",
    "\n",
    "# CE baseline line\n",
    "ax.axhline(y=ce_error, color='red', linestyle='--', linewidth=2, label=f'CE baseline ({ce_error:.3f})')\n",
    "ax.fill_between([-0.5, len(LAMBDAS)], ce_error-ce_std, ce_error+ce_std, color='red', alpha=0.1)\n",
    "\n",
    "ax.set_xlabel('λ', fontsize=12)\n",
    "ax.set_ylabel('Test Error', fontsize=12)\n",
    "ax.set_title('Mechanism Decomposition: Why Misaligned Improves?', fontsize=13, fontweight='bold')\n",
    "ax.set_xticks(x_positions + 1.5*width)\n",
    "ax.set_xticklabels([str(l) for l in LAMBDAS])\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR}/figures/mechanism_decomposition.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Figure saved: {SAVE_DIR}/figures/mechanism_decomposition.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== サマリー =====\n",
    "print('\\n' + '='*70)\n",
    "print('EXPERIMENT J: MECHANISM DECOMPOSITION - SUMMARY')\n",
    "print('='*70)\n",
    "print(f'\\nSave directory: {SAVE_DIR}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
