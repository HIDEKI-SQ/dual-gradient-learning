{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TŒ¥2: Escape from 90% Trap\n",
    "\n",
    "**Purpose**: Test interventions to escape from the 90% trapped state\n",
    "\n",
    "**Background**: Some seeds get trapped at ~90% error (near random). Can we rescue them?\n",
    "\n",
    "**Interventions to test**:\n",
    "1. **Œª‚Üí0**: Pure structure gradient (ignore value completely)\n",
    "2. **LR Boost**: Increase learning rate significantly\n",
    "3. **Œª‚Üí0 + LR Boost**: Combined intervention\n",
    "4. **Warm Restart**: Reset optimizer momentum\n",
    "\n",
    "**Key Question**: What breaks the 90% trap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os, glob, json, time\n",
    "from datetime import datetime\n",
    "\n",
    "EXP_NAME = 'exp_Td2_escape_trap'\n",
    "NOTEBOOK_ID = 'Td2'\n",
    "BASE_DIR = '/content/drive/MyDrive/dual-gradient-learning/Paper-A'\n",
    "\n",
    "existing = glob.glob(f'{BASE_DIR}/{EXP_NAME}_*')\n",
    "if existing:\n",
    "    SAVE_DIR = sorted(existing)[-1]\n",
    "    print(f'üîÑ Resuming: {SAVE_DIR}')\n",
    "else:\n",
    "    TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    SAVE_DIR = f'{BASE_DIR}/{EXP_NAME}_{TIMESTAMP}'\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    print(f'üÜï New: {SAVE_DIR}')\n",
    "\n",
    "os.makedirs(f'{SAVE_DIR}/checkpoints', exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR}/figures', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils import parameters_to_vector\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core parameters\n",
    "BATCH_SIZE = 256\n",
    "NUM_WORKERS = 4\n",
    "LR = 0.1\n",
    "K = 16\n",
    "NOISE_RATE = 0.4\n",
    "\n",
    "# Create trapped state\n",
    "TRAP_LAMBDA = 0.60  # Œª that tends to produce 90% traps\n",
    "TRAP_EPOCHS = 100   # Long enough to get trapped\n",
    "TRAP_THRESHOLD = 0.85  # Consider >85% as \"trapped\"\n",
    "\n",
    "# Escape interventions\n",
    "ESCAPE_EPOCHS = 50  # Epochs to attempt escape\n",
    "EVAL_FREQ = 5\n",
    "\n",
    "# Intervention configurations\n",
    "INTERVENTIONS = [\n",
    "    {'name': 'baseline', 'lambda': 0.60, 'lr_mult': 0.01, 'reset_opt': False},  # Continue as-is (control)\n",
    "    {'name': 'lambda_zero', 'lambda': 0.00, 'lr_mult': 0.01, 'reset_opt': False},  # Pure structure\n",
    "    {'name': 'lambda_low', 'lambda': 0.20, 'lr_mult': 0.01, 'reset_opt': False},  # Low Œª\n",
    "    {'name': 'lr_boost', 'lambda': 0.60, 'lr_mult': 0.1, 'reset_opt': False},  # 10x LR\n",
    "    {'name': 'lr_boost_high', 'lambda': 0.60, 'lr_mult': 1.0, 'reset_opt': True},  # Full LR + reset\n",
    "    {'name': 'combined', 'lambda': 0.00, 'lr_mult': 0.1, 'reset_opt': True},  # Œª=0 + LR boost + reset\n",
    "]\n",
    "\n",
    "# Seeds known to produce 90% traps (from previous experiments)\n",
    "# We'll also try to create new traps\n",
    "N_TRAP_ATTEMPTS = 10  # Try this many seeds to find trapped states\n",
    "\n",
    "print(f'Trap Œª: {TRAP_LAMBDA}')\n",
    "print(f'Interventions: {[i[\"name\"] for i in INTERVENTIONS]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet18():\n",
    "    model = resnet18(weights=None, num_classes=10)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "    return model\n",
    "\n",
    "class IndexedDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        return img, label, idx\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def inject_label_noise(labels, noise_rate, seed):\n",
    "    np.random.seed(seed)\n",
    "    noisy = labels.copy()\n",
    "    n_noisy = int(noise_rate * len(labels))\n",
    "    idx = np.random.choice(len(labels), n_noisy, replace=False)\n",
    "    for i in idx:\n",
    "        noisy[i] = np.random.choice([l for l in range(10) if l != labels[i]])\n",
    "    return noisy\n",
    "\n",
    "def load_cifar10():\n",
    "    tr = transforms.Compose([transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(),\n",
    "                             transforms.ToTensor(), transforms.Normalize((0.4914,0.4822,0.4465),(0.2023,0.1994,0.2010))])\n",
    "    te = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914,0.4822,0.4465),(0.2023,0.1994,0.2010))])\n",
    "    return torchvision.datasets.CIFAR10('./data', True, tr, download=True), torchvision.datasets.CIFAR10('./data', False, te, download=True)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            correct += (model(x).argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, opt, clean_t, noisy_t, lam, state):\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    step = state['step']\n",
    "    cached_gv = state['gv']\n",
    "    \n",
    "    for x, _, idx in train_loader:\n",
    "        x, idx = x.to(device), idx.to(device)\n",
    "        bn, bc = noisy_t[idx], clean_t[idx]\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss_s = crit(model(x), bn)\n",
    "        loss_s.backward(retain_graph=True)\n",
    "        g_s = parameters_to_vector([p.grad for p in model.parameters()]).clone()\n",
    "        \n",
    "        if step % K == 0 or cached_gv is None:\n",
    "            opt.zero_grad()\n",
    "            loss_v = crit(model(x), bc)\n",
    "            loss_v.backward()\n",
    "            cached_gv = parameters_to_vector([p.grad for p in model.parameters()]).clone()\n",
    "        \n",
    "        g_s_n = g_s / (g_s.norm() + 1e-12)\n",
    "        g_v_n = cached_gv / (cached_gv.norm() + 1e-12)\n",
    "        g_mix = (1 - lam) * g_s_n + lam * g_v_n\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        i = 0\n",
    "        for p in model.parameters():\n",
    "            n = p.numel()\n",
    "            p.grad = g_mix[i:i+n].view(p.shape).clone()\n",
    "            i += n\n",
    "        opt.step()\n",
    "        step += 1\n",
    "    \n",
    "    state['step'] = step\n",
    "    state['gv'] = cached_gv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = load_cifar10()\n",
    "clean_labels = np.array(trainset.targets)\n",
    "train_loader = DataLoader(IndexedDataset(trainset), BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader = DataLoader(testset, BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "m = get_resnet18().to(device)\n",
    "for _ in range(5): _ = m(torch.randn(BATCH_SIZE,3,32,32,device=device))\n",
    "del m; torch.cuda.empty_cache()\n",
    "print('Ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trapped_state(seed, clean_t, noisy_t):\n",
    "    \"\"\"Attempt to create a 90% trapped state\"\"\"\n",
    "    set_seed(seed)\n",
    "    model = get_resnet18().to(device)\n",
    "    opt = optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "    sched = optim.lr_scheduler.MultiStepLR(opt, [50, 80], gamma=0.1)\n",
    "    state = {'step': 0, 'gv': None}\n",
    "    \n",
    "    error_history = []\n",
    "    \n",
    "    for ep in range(TRAP_EPOCHS):\n",
    "        train_one_epoch(model, train_loader, opt, clean_t, noisy_t, TRAP_LAMBDA, state)\n",
    "        sched.step()\n",
    "        \n",
    "        if (ep + 1) % 20 == 0:\n",
    "            err = 1 - evaluate(model, test_loader)\n",
    "            error_history.append({'epoch': ep + 1, 'error': err})\n",
    "            print(f'    Epoch {ep+1}: {err:.4f}')\n",
    "    \n",
    "    final_error = 1 - evaluate(model, test_loader)\n",
    "    is_trapped = final_error >= TRAP_THRESHOLD\n",
    "    \n",
    "    if is_trapped:\n",
    "        trapped_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "    else:\n",
    "        trapped_state = None\n",
    "    \n",
    "    del model; torch.cuda.empty_cache()\n",
    "    \n",
    "    return {\n",
    "        'seed': seed,\n",
    "        'final_error': final_error,\n",
    "        'is_trapped': is_trapped,\n",
    "        'error_history': error_history,\n",
    "        'state': trapped_state\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_escape_intervention(trapped_state, intervention, seed, clean_t, noisy_t, init_error):\n",
    "    \"\"\"Run escape intervention and track trajectory\"\"\"\n",
    "    set_seed(seed + 1000 + hash(intervention['name']) % 1000)\n",
    "    \n",
    "    model = get_resnet18().to(device)\n",
    "    model.load_state_dict({k: v.to(device) for k, v in trapped_state.items()})\n",
    "    \n",
    "    # Setup optimizer based on intervention\n",
    "    actual_lr = LR * intervention['lr_mult']\n",
    "    opt = optim.SGD(model.parameters(), lr=actual_lr, momentum=0.9, weight_decay=5e-4)\n",
    "    \n",
    "    state = {'step': 0, 'gv': None}\n",
    "    trajectory = [{'epoch': 0, 'error': init_error}]\n",
    "    \n",
    "    for ep in range(ESCAPE_EPOCHS):\n",
    "        train_one_epoch(model, train_loader, opt, clean_t, noisy_t, intervention['lambda'], state)\n",
    "        \n",
    "        if (ep + 1) % EVAL_FREQ == 0:\n",
    "            err = 1 - evaluate(model, test_loader)\n",
    "            trajectory.append({'epoch': ep + 1, 'error': err})\n",
    "    \n",
    "    final_error = 1 - evaluate(model, test_loader)\n",
    "    escaped = final_error < 0.60  # Escaped if below 60%\n",
    "    \n",
    "    del model; torch.cuda.empty_cache()\n",
    "    \n",
    "    return {\n",
    "        'intervention': intervention['name'],\n",
    "        'config': intervention,\n",
    "        'init_error': init_error,\n",
    "        'final_error': final_error,\n",
    "        'escaped': escaped,\n",
    "        'improvement': init_error - final_error,\n",
    "        'trajectory': trajectory\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Find trapped states\n",
    "print('='*60)\n",
    "print('PHASE 1: Finding Trapped States')\n",
    "print('='*60)\n",
    "\n",
    "trapped_states = []\n",
    "\n",
    "for seed in range(N_TRAP_ATTEMPTS):\n",
    "    print(f'\\nSeed {seed}:')\n",
    "    \n",
    "    noisy_labels = inject_label_noise(clean_labels, NOISE_RATE, seed)\n",
    "    clean_t = torch.tensor(clean_labels, device=device)\n",
    "    noisy_t = torch.tensor(noisy_labels, device=device)\n",
    "    \n",
    "    result = create_trapped_state(seed, clean_t, noisy_t)\n",
    "    \n",
    "    if result['is_trapped']:\n",
    "        print(f'  üî¥ TRAPPED at {result[\"final_error\"]:.2%}')\n",
    "        trapped_states.append({\n",
    "            'seed': seed,\n",
    "            'final_error': result['final_error'],\n",
    "            'state': result['state'],\n",
    "            'noisy_labels': noisy_labels\n",
    "        })\n",
    "    else:\n",
    "        print(f'  üü¢ Not trapped ({result[\"final_error\"]:.2%})')\n",
    "    \n",
    "    # Stop if we have enough trapped states\n",
    "    if len(trapped_states) >= 3:\n",
    "        print(f'\\nFound {len(trapped_states)} trapped states, proceeding to interventions')\n",
    "        break\n",
    "\n",
    "print(f'\\nTotal trapped states found: {len(trapped_states)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: Run escape interventions\n",
    "print('\\n' + '='*60)\n",
    "print('PHASE 2: Escape Interventions')\n",
    "print('='*60)\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for trap_info in trapped_states:\n",
    "    seed = trap_info['seed']\n",
    "    print(f'\\n{\"=\"*50}')\n",
    "    print(f'Trapped Seed {seed} (error={trap_info[\"final_error\"]:.2%})')\n",
    "    print(f'{\"=\"*50}')\n",
    "    \n",
    "    clean_t = torch.tensor(clean_labels, device=device)\n",
    "    noisy_t = torch.tensor(trap_info['noisy_labels'], device=device)\n",
    "    \n",
    "    seed_results = {\n",
    "        'seed': seed,\n",
    "        'trap_error': trap_info['final_error'],\n",
    "        'interventions': []\n",
    "    }\n",
    "    \n",
    "    for intervention in INTERVENTIONS:\n",
    "        print(f'\\n  Intervention: {intervention[\"name\"]}')\n",
    "        print(f'    Œª={intervention[\"lambda\"]}, LR√ó{intervention[\"lr_mult\"]}, reset={intervention[\"reset_opt\"]}')\n",
    "        \n",
    "        result = run_escape_intervention(\n",
    "            trap_info['state'],\n",
    "            intervention,\n",
    "            seed,\n",
    "            clean_t,\n",
    "            noisy_t,\n",
    "            trap_info['final_error']\n",
    "        )\n",
    "        \n",
    "        status = '‚úÖ ESCAPED' if result['escaped'] else '‚ùå Still trapped'\n",
    "        print(f'    {trap_info[\"final_error\"]:.2%} ‚Üí {result[\"final_error\"]:.2%} ({status})')\n",
    "        print(f'    Improvement: {result[\"improvement\"]*100:.1f}%')\n",
    "        \n",
    "        seed_results['interventions'].append(result)\n",
    "    \n",
    "    seed_results['experiment_id'] = f'{NOTEBOOK_ID}-seed{seed:02d}'\n",
    "    all_results.append(seed_results)\n",
    "\n",
    "# Save results\n",
    "json.dump(all_results, open(f'{SAVE_DIR}/{NOTEBOOK_ID}_results.json', 'w'), indent=2, default=str)\n",
    "\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'{NOTEBOOK_ID} COMPLETE')\n",
    "print(f'{\"=\"*60}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualization: Escape trajectories by intervention\n",
    "n_interventions = len(INTERVENTIONS)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(trapped_states)))\n",
    "\n",
    "for i, intervention in enumerate(INTERVENTIONS):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    for j, seed_result in enumerate(all_results):\n",
    "        intv_result = next((r for r in seed_result['interventions'] if r['intervention'] == intervention['name']), None)\n",
    "        if intv_result:\n",
    "            epochs = [t['epoch'] for t in intv_result['trajectory']]\n",
    "            errors = [t['error'] for t in intv_result['trajectory']]\n",
    "            ax.plot(epochs, errors, 'o-', color=colors[j], linewidth=2, markersize=5,\n",
    "                    label=f'Seed {seed_result[\"seed\"]}')\n",
    "    \n",
    "    ax.axhline(0.60, color='orange', linestyle='--', alpha=0.5, label='Escape threshold')\n",
    "    ax.axhline(0.90, color='red', linestyle='--', alpha=0.3)\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Test Error')\n",
    "    ax.set_title(f'{intervention[\"name\"]}\\n(Œª={intervention[\"lambda\"]}, LR√ó{intervention[\"lr_mult\"]})', fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0, 1)\n",
    "    if i == 0:\n",
    "        ax.legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR}/figures/{NOTEBOOK_ID}_escape_trajectories.png', dpi=150)\n",
    "plt.savefig(f'{SAVE_DIR}/figures/{NOTEBOOK_ID}_escape_trajectories.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot: Success rate by intervention\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Escape rate\n",
    "ax = axes[0]\n",
    "escape_rates = []\n",
    "for intervention in INTERVENTIONS:\n",
    "    escaped = sum(1 for r in all_results \n",
    "                  for intv in r['interventions'] \n",
    "                  if intv['intervention'] == intervention['name'] and intv['escaped'])\n",
    "    total = sum(1 for r in all_results \n",
    "                for intv in r['interventions'] \n",
    "                if intv['intervention'] == intervention['name'])\n",
    "    rate = escaped / total if total > 0 else 0\n",
    "    escape_rates.append(rate)\n",
    "\n",
    "x = np.arange(len(INTERVENTIONS))\n",
    "bars = ax.bar(x, [r * 100 for r in escape_rates], color='steelblue', alpha=0.8, edgecolor='navy')\n",
    "ax.axhline(50, color='orange', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Intervention')\n",
    "ax.set_ylabel('Escape Rate (%)')\n",
    "ax.set_title('Escape Success Rate by Intervention')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([i['name'] for i in INTERVENTIONS], rotation=45, ha='right')\n",
    "ax.set_ylim(0, 100)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Average improvement\n",
    "ax = axes[1]\n",
    "improvements = []\n",
    "for intervention in INTERVENTIONS:\n",
    "    imps = [intv['improvement'] for r in all_results \n",
    "            for intv in r['interventions'] \n",
    "            if intv['intervention'] == intervention['name']]\n",
    "    improvements.append(np.mean(imps) if imps else 0)\n",
    "\n",
    "bars = ax.bar(x, [imp * 100 for imp in improvements], color='green', alpha=0.8, edgecolor='darkgreen')\n",
    "ax.axhline(0, color='black', linestyle='-', alpha=0.3)\n",
    "ax.set_xlabel('Intervention')\n",
    "ax.set_ylabel('Average Improvement (%)')\n",
    "ax.set_title('Average Error Reduction by Intervention')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([i['name'] for i in INTERVENTIONS], rotation=45, ha='right')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR}/figures/{NOTEBOOK_ID}_intervention_comparison.png', dpi=150)\n",
    "plt.savefig(f'{SAVE_DIR}/figures/{NOTEBOOK_ID}_intervention_comparison.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print('='*60)\n",
    "print(f'{NOTEBOOK_ID} SUMMARY: Escape from 90% Trap')\n",
    "print('='*60)\n",
    "\n",
    "print(f'\\nüìä Trapped states tested: {len(all_results)}')\n",
    "\n",
    "print(f'\\nüìä Results by Intervention:')\n",
    "print(f'{\"Intervention\":<20} {\"Escape Rate\":<15} {\"Avg Improvement\":<15} {\"Best Final\":<12}')\n",
    "print('-' * 62)\n",
    "\n",
    "best_intervention = None\n",
    "best_escape_rate = 0\n",
    "\n",
    "for i, intervention in enumerate(INTERVENTIONS):\n",
    "    results_for_intv = [intv for r in all_results \n",
    "                        for intv in r['interventions'] \n",
    "                        if intv['intervention'] == intervention['name']]\n",
    "    \n",
    "    if results_for_intv:\n",
    "        escaped = sum(1 for r in results_for_intv if r['escaped'])\n",
    "        escape_rate = escaped / len(results_for_intv)\n",
    "        avg_improvement = np.mean([r['improvement'] for r in results_for_intv])\n",
    "        best_final = min(r['final_error'] for r in results_for_intv)\n",
    "        \n",
    "        print(f'{intervention[\"name\"]:<20} {escape_rate*100:>6.1f}%        {avg_improvement*100:>6.1f}%          {best_final*100:>5.1f}%')\n",
    "        \n",
    "        if escape_rate > best_escape_rate:\n",
    "            best_escape_rate = escape_rate\n",
    "            best_intervention = intervention['name']\n",
    "\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'CONCLUSION:')\n",
    "if best_escape_rate > 0.5:\n",
    "    print(f'  ‚úÖ Best intervention: {best_intervention}')\n",
    "    print(f'  ‚úÖ Escape rate: {best_escape_rate*100:.0f}%')\n",
    "    print(f'  ‚úÖ 90% trap CAN be escaped with proper intervention')\n",
    "elif best_escape_rate > 0:\n",
    "    print(f'  ‚ö†Ô∏è Best intervention: {best_intervention}')\n",
    "    print(f'  ‚ö†Ô∏è Escape rate: {best_escape_rate*100:.0f}% (partial success)')\n",
    "    print(f'  ‚ö†Ô∏è 90% trap is difficult but not impossible to escape')\n",
    "else:\n",
    "    print(f'  ‚ùå No intervention successfully escaped the trap')\n",
    "    print(f'  ‚ùå 90% trap is a stable attractor')\n",
    "print(f'{\"=\"*60}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame\n",
    "summary_data = []\n",
    "for r in all_results:\n",
    "    for intv in r['interventions']:\n",
    "        summary_data.append({\n",
    "            'seed': r['seed'],\n",
    "            'trap_error': r['trap_error'],\n",
    "            'intervention': intv['intervention'],\n",
    "            'lambda': intv['config']['lambda'],\n",
    "            'lr_mult': intv['config']['lr_mult'],\n",
    "            'final_error': intv['final_error'],\n",
    "            'improvement': intv['improvement'],\n",
    "            'escaped': intv['escaped']\n",
    "        })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "df_summary.to_csv(f'{SAVE_DIR}/{NOTEBOOK_ID}_summary.csv', index=False)\n",
    "print('Summary saved')\n",
    "print(df_summary.to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
