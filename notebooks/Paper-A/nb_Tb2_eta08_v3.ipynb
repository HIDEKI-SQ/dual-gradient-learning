{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TŒ≤2_v3: Two-Branch Existence Test at Œ∑=0.8\n",
    "\n",
    "**Purpose**: Test whether bistability exists under extreme noise (80%)\n",
    "\n",
    "**Version History**:\n",
    "- v1: threshold 25% ‚Üí All seeds failed\n",
    "- v2: threshold 45% ‚Üí Seeds reached 46-65% but still failed\n",
    "- **v3: No absolute threshold** (Sofia's recommendation)\n",
    "\n",
    "**Key Design Change** (per Sofia):\n",
    "> „Äåordered-initÔºàÔºùlow-error initÔºâÁµ∂ÂØæÈñæÂÄ§„ÅØ‰Ωø„Çè„Å™„ÅÑ„Äç\n",
    "> „Äåordered„Åå‰Ωú„Çå„Å™„ÅÑÔºùÁõ∏„ÅåÊ∂à„Åà„Çã„Äç„Å®„ÅÑ„ÅÜÁµêÊûú„Å®„Åó„Å¶„ÄÅË®≠Ë®àÁõÆÁöÑ„ÇíÂàá„ÇäÊõø„Åà„Çå„Å∞Ë´ñÊñáÂåñ„Åß„Åç„Çã\n",
    "\n",
    "**Protocol**:\n",
    "1. Train multiple seeds at Œª=0.50 for 100 epochs\n",
    "2. Take the **best performing seed** as \"low-error init\" (whatever error it achieves)\n",
    "3. Create collapse init (90%) as usual\n",
    "4. Compare the two branches - do they separate?\n",
    "\n",
    "**Possible Outcomes**:\n",
    "- Branches separate ‚Üí Bistability persists at Œ∑=0.8\n",
    "- Branches converge ‚Üí Bistability breaks at Œ∑=0.8 (boundary discovery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os, glob, json, time\n",
    "from datetime import datetime\n",
    "\n",
    "EXP_NAME = 'exp_Tb2_eta08_v3'\n",
    "NOTEBOOK_ID = 'Tb2_v3'\n",
    "BASE_DIR = '/content/drive/MyDrive/dual-gradient-learning/Paper-A'\n",
    "\n",
    "existing = glob.glob(f'{BASE_DIR}/{EXP_NAME}_*')\n",
    "if existing:\n",
    "    SAVE_DIR = sorted(existing)[-1]\n",
    "    print(f'üîÑ Resuming: {SAVE_DIR}')\n",
    "else:\n",
    "    TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    SAVE_DIR = f'{BASE_DIR}/{EXP_NAME}_{TIMESTAMP}'\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    print(f'üÜï New: {SAVE_DIR}')\n",
    "\n",
    "os.makedirs(f'{SAVE_DIR}/checkpoints', exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR}/figures', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils import parameters_to_vector\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core parameters\n",
    "BATCH_SIZE = 256\n",
    "NUM_WORKERS = 4\n",
    "LR = 0.1\n",
    "K = 16\n",
    "\n",
    "# HIGH NOISE\n",
    "NOISE_RATE = 0.8  # 80% noise\n",
    "\n",
    "# v3: NO ABSOLUTE THRESHOLD for low-error init\n",
    "# Instead, we take the best we can get\n",
    "LOW_ERROR_LAMBDA = 0.50  # Œª for creating low-error state\n",
    "LOW_ERROR_EPOCHS = 100\n",
    "\n",
    "# Collapse init (high-error)\n",
    "COLLAPSE_LAMBDA = 0.70\n",
    "COLLAPSE_EPOCHS = 100\n",
    "\n",
    "# Sweep settings\n",
    "LAMBDA_START = 0.40\n",
    "LAMBDA_END = 0.80\n",
    "LAMBDA_STEP = 0.05\n",
    "EPOCHS_PER_LAMBDA = 5\n",
    "\n",
    "LAMBDA_GRID_UP = np.round(np.arange(LAMBDA_START, LAMBDA_END + LAMBDA_STEP/2, LAMBDA_STEP), 2)\n",
    "LAMBDA_GRID_DOWN = np.round(np.arange(LAMBDA_END, LAMBDA_START - LAMBDA_STEP/2, -LAMBDA_STEP), 2)\n",
    "\n",
    "# Phase 1: Find best low-error seed\n",
    "N_SEARCH_SEEDS = 10  # Search this many seeds to find the best\n",
    "\n",
    "# Phase 2: Run comparison with best seed\n",
    "N_COMPARISON_SEEDS = 3  # How many seeds to run full comparison\n",
    "\n",
    "print(f'Noise rate: {NOISE_RATE*100:.0f}%')\n",
    "print(f'Strategy: No threshold, take best achievable error as low-error init')\n",
    "print(f'Sweep range: Œª ‚àà [{LAMBDA_START}, {LAMBDA_END}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet18():\n",
    "    model = resnet18(weights=None, num_classes=10)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "    return model\n",
    "\n",
    "class IndexedDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        return img, label, idx\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def inject_label_noise(labels, noise_rate, seed):\n",
    "    np.random.seed(seed)\n",
    "    noisy = labels.copy()\n",
    "    n_noisy = int(noise_rate * len(labels))\n",
    "    idx = np.random.choice(len(labels), n_noisy, replace=False)\n",
    "    for i in idx:\n",
    "        noisy[i] = np.random.choice([l for l in range(10) if l != labels[i]])\n",
    "    return noisy\n",
    "\n",
    "def load_cifar10():\n",
    "    tr = transforms.Compose([transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(),\n",
    "                             transforms.ToTensor(), transforms.Normalize((0.4914,0.4822,0.4465),(0.2023,0.1994,0.2010))])\n",
    "    te = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914,0.4822,0.4465),(0.2023,0.1994,0.2010))])\n",
    "    return torchvision.datasets.CIFAR10('./data', True, tr, download=True), torchvision.datasets.CIFAR10('./data', False, te, download=True)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            correct += (model(x).argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, opt, clean_t, noisy_t, lam, state):\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    step = state['step']\n",
    "    cached_gv = state['gv']\n",
    "    \n",
    "    for x, _, idx in train_loader:\n",
    "        x, idx = x.to(device), idx.to(device)\n",
    "        bn, bc = noisy_t[idx], clean_t[idx]\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss_s = crit(model(x), bn)\n",
    "        loss_s.backward(retain_graph=True)\n",
    "        g_s = parameters_to_vector([p.grad for p in model.parameters()]).clone()\n",
    "        \n",
    "        if step % K == 0 or cached_gv is None:\n",
    "            opt.zero_grad()\n",
    "            loss_v = crit(model(x), bc)\n",
    "            loss_v.backward()\n",
    "            cached_gv = parameters_to_vector([p.grad for p in model.parameters()]).clone()\n",
    "        \n",
    "        g_s_n = g_s / (g_s.norm() + 1e-12)\n",
    "        g_v_n = cached_gv / (cached_gv.norm() + 1e-12)\n",
    "        g_mix = (1 - lam) * g_s_n + lam * g_v_n\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        i = 0\n",
    "        for p in model.parameters():\n",
    "            n = p.numel()\n",
    "            p.grad = g_mix[i:i+n].view(p.shape).clone()\n",
    "            i += n\n",
    "        opt.step()\n",
    "        step += 1\n",
    "    \n",
    "    state['step'] = step\n",
    "    state['gv'] = cached_gv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = load_cifar10()\n",
    "clean_labels = np.array(trainset.targets)\n",
    "train_loader = DataLoader(IndexedDataset(trainset), BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader = DataLoader(testset, BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "m = get_resnet18().to(device)\n",
    "for _ in range(5): _ = m(torch.randn(BATCH_SIZE,3,32,32,device=device))\n",
    "del m; torch.cuda.empty_cache()\n",
    "print('Ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PHASE 1: Find best low-error seeds\n",
    "# ============================================================\n",
    "print('='*60)\n",
    "print('PHASE 1: Finding Best Low-Error Seeds')\n",
    "print('='*60)\n",
    "\n",
    "seed_results = []\n",
    "\n",
    "for seed in range(N_SEARCH_SEEDS):\n",
    "    print(f'\\nSeed {seed}...')\n",
    "    \n",
    "    noisy_labels = inject_label_noise(clean_labels, NOISE_RATE, seed)\n",
    "    clean_t = torch.tensor(clean_labels, device=device)\n",
    "    noisy_t = torch.tensor(noisy_labels, device=device)\n",
    "    \n",
    "    set_seed(seed)\n",
    "    model = get_resnet18().to(device)\n",
    "    opt = optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "    sched = optim.lr_scheduler.MultiStepLR(opt, [50, 80], gamma=0.1)\n",
    "    state = {'step': 0, 'gv': None}\n",
    "    \n",
    "    best_error = 1.0\n",
    "    for ep in range(LOW_ERROR_EPOCHS):\n",
    "        train_one_epoch(model, train_loader, opt, clean_t, noisy_t, LOW_ERROR_LAMBDA, state)\n",
    "        sched.step()\n",
    "        if (ep + 1) % 20 == 0:\n",
    "            err = 1 - evaluate(model, test_loader)\n",
    "            best_error = min(best_error, err)\n",
    "            print(f'  Epoch {ep+1}: error={err:.4f}')\n",
    "    \n",
    "    final_error = 1 - evaluate(model, test_loader)\n",
    "    seed_results.append({\n",
    "        'seed': seed,\n",
    "        'final_error': final_error,\n",
    "        'state': {k: v.cpu().clone() for k, v in model.state_dict().items()},\n",
    "        'noisy_labels': noisy_labels\n",
    "    })\n",
    "    print(f'  Final: {final_error:.4f}')\n",
    "    \n",
    "    del model; torch.cuda.empty_cache()\n",
    "\n",
    "# Sort by error (lowest first)\n",
    "seed_results.sort(key=lambda x: x['final_error'])\n",
    "\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print('Phase 1 Results (sorted by error):')\n",
    "for i, r in enumerate(seed_results):\n",
    "    marker = '‚≠ê' if i < N_COMPARISON_SEEDS else '  '\n",
    "    print(f'{marker} Seed {r[\"seed\"]}: {r[\"final_error\"]*100:.1f}%')\n",
    "print(f'{\"=\"*60}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PHASE 2: Two-Branch Comparison\n",
    "# ============================================================\n",
    "print('\\n' + '='*60)\n",
    "print('PHASE 2: Two-Branch Comparison')\n",
    "print('='*60)\n",
    "\n",
    "# Take best N seeds\n",
    "selected_seeds = seed_results[:N_COMPARISON_SEEDS]\n",
    "\n",
    "results = []\n",
    "\n",
    "for seed_info in selected_seeds:\n",
    "    seed = seed_info['seed']\n",
    "    low_error_init = seed_info['final_error']\n",
    "    \n",
    "    print(f'\\n{\"=\"*50}')\n",
    "    print(f'Seed {seed} (low-error init: {low_error_init*100:.1f}%)')\n",
    "    print(f'{\"=\"*50}')\n",
    "    \n",
    "    clean_t = torch.tensor(clean_labels, device=device)\n",
    "    noisy_t = torch.tensor(seed_info['noisy_labels'], device=device)\n",
    "    \n",
    "    # === Create Collapse Init (high-error) ===\n",
    "    print(f'\\n[Creating Collapse Init (Œª={COLLAPSE_LAMBDA})]...')\n",
    "    set_seed(seed + 100)\n",
    "    model = get_resnet18().to(device)\n",
    "    opt = optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "    sched = optim.lr_scheduler.MultiStepLR(opt, [50, 80], gamma=0.1)\n",
    "    state = {'step': 0, 'gv': None}\n",
    "    \n",
    "    for ep in range(COLLAPSE_EPOCHS):\n",
    "        train_one_epoch(model, train_loader, opt, clean_t, noisy_t, COLLAPSE_LAMBDA, state)\n",
    "        sched.step()\n",
    "        if (ep + 1) % 25 == 0:\n",
    "            err = 1 - evaluate(model, test_loader)\n",
    "            print(f'  Epoch {ep+1}: error={err:.4f}')\n",
    "    \n",
    "    collapse_error = 1 - evaluate(model, test_loader)\n",
    "    collapse_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "    print(f'  Collapse init: {collapse_error:.4f}')\n",
    "    del model; torch.cuda.empty_cache()\n",
    "    \n",
    "    # === Low-Error Branch Sweep (Œª‚Üë) ===\n",
    "    print(f'\\n[Low-Error Branch Sweep (Œª‚Üë)]...')\n",
    "    set_seed(seed + 200)\n",
    "    model = get_resnet18().to(device)\n",
    "    model.load_state_dict({k: v.to(device) for k, v in seed_info['state'].items()})\n",
    "    opt = optim.SGD(model.parameters(), lr=LR * 0.01, momentum=0.9, weight_decay=5e-4)\n",
    "    state = {'step': 0, 'gv': None}\n",
    "    \n",
    "    low_error_traj = []\n",
    "    for lam in LAMBDA_GRID_UP:\n",
    "        for _ in range(EPOCHS_PER_LAMBDA):\n",
    "            train_one_epoch(model, train_loader, opt, clean_t, noisy_t, lam, state)\n",
    "        err = 1 - evaluate(model, test_loader)\n",
    "        low_error_traj.append({'lambda': float(lam), 'error': err})\n",
    "        print(f'  Œª={lam:.2f}: {err:.4f}')\n",
    "    \n",
    "    del model; torch.cuda.empty_cache()\n",
    "    \n",
    "    # === Collapse Branch Sweep (Œª‚Üì) ===\n",
    "    print(f'\\n[Collapse Branch Sweep (Œª‚Üì)]...')\n",
    "    set_seed(seed + 300)\n",
    "    model = get_resnet18().to(device)\n",
    "    model.load_state_dict({k: v.to(device) for k, v in collapse_state.items()})\n",
    "    opt = optim.SGD(model.parameters(), lr=LR * 0.01, momentum=0.9, weight_decay=5e-4)\n",
    "    state = {'step': 0, 'gv': None}\n",
    "    \n",
    "    collapse_traj = []\n",
    "    for lam in LAMBDA_GRID_DOWN:\n",
    "        for _ in range(EPOCHS_PER_LAMBDA):\n",
    "            train_one_epoch(model, train_loader, opt, clean_t, noisy_t, lam, state)\n",
    "        err = 1 - evaluate(model, test_loader)\n",
    "        collapse_traj.append({'lambda': float(lam), 'error': err})\n",
    "        print(f'  Œª={lam:.2f}: {err:.4f}')\n",
    "    \n",
    "    del model; torch.cuda.empty_cache()\n",
    "    \n",
    "    results.append({\n",
    "        'seed': seed,\n",
    "        'eta': NOISE_RATE,\n",
    "        'low_error_init': low_error_init,\n",
    "        'collapse_init': collapse_error,\n",
    "        'low_error_trajectory': low_error_traj,\n",
    "        'collapse_trajectory': collapse_traj,\n",
    "        'experiment_id': f'{NOTEBOOK_ID}-seed{seed:02d}'\n",
    "    })\n",
    "\n",
    "# Save results\n",
    "json.dump(results, open(f'{SAVE_DIR}/{NOTEBOOK_ID}_results.json', 'w'), indent=2, default=str)\n",
    "\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'{NOTEBOOK_ID} COMPLETE')\n",
    "print(f'{\"=\"*60}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create DataFrame\n",
    "all_data = []\n",
    "for r in results:\n",
    "    for t in r['low_error_trajectory']:\n",
    "        all_data.append({'seed': r['seed'], 'branch': 'low_error', 'lambda': t['lambda'], 'error': t['error']})\n",
    "    for t in r['collapse_trajectory']:\n",
    "        all_data.append({'seed': r['seed'], 'branch': 'collapse', 'lambda': t['lambda'], 'error': t['error']})\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(f'{SAVE_DIR}/{NOTEBOOK_ID}_results.csv', index=False)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Individual trajectories\n",
    "ax = axes[0]\n",
    "for r in results:\n",
    "    lams_l = [t['lambda'] for t in r['low_error_trajectory']]\n",
    "    errs_l = [t['error'] for t in r['low_error_trajectory']]\n",
    "    lams_c = [t['lambda'] for t in r['collapse_trajectory']]\n",
    "    errs_c = [t['error'] for t in r['collapse_trajectory']]\n",
    "    ax.plot(lams_l, errs_l, 'b-o', alpha=0.6, linewidth=2, markersize=5)\n",
    "    ax.plot(lams_c, errs_c, 'r-s', alpha=0.6, linewidth=2, markersize=5)\n",
    "\n",
    "ax.axhline(0.90, color='red', linestyle='--', alpha=0.3, label='Random')\n",
    "ax.set_xlabel('Œª', fontsize=12)\n",
    "ax.set_ylabel('Test Error', fontsize=12)\n",
    "ax.set_title(f'Œ∑={NOISE_RATE}: Individual Trajectories', fontsize=14)\n",
    "ax.legend(['Low-Error (Œª‚Üë)', 'Collapse (Œª‚Üì)'], fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(LAMBDA_START - 0.02, LAMBDA_END + 0.02)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Mean with std\n",
    "ax = axes[1]\n",
    "df_low = df[df['branch'] == 'low_error']\n",
    "df_col = df[df['branch'] == 'collapse']\n",
    "\n",
    "if len(df_low) > 0:\n",
    "    m = df_low.groupby('lambda')['error'].agg(['mean', 'std']).reset_index()\n",
    "    ax.fill_between(m['lambda'], m['mean']-m['std'], m['mean']+m['std'], alpha=0.3, color='blue')\n",
    "    ax.plot(m['lambda'], m['mean'], 'b-o', linewidth=2, markersize=6, label='Low-Error (Œª‚Üë)')\n",
    "\n",
    "if len(df_col) > 0:\n",
    "    m = df_col.groupby('lambda')['error'].agg(['mean', 'std']).reset_index()\n",
    "    ax.fill_between(m['lambda'], m['mean']-m['std'], m['mean']+m['std'], alpha=0.3, color='red')\n",
    "    ax.plot(m['lambda'], m['mean'], 'r-s', linewidth=2, markersize=6, label='Collapse (Œª‚Üì)')\n",
    "\n",
    "ax.axhline(0.90, color='red', linestyle='--', alpha=0.3)\n",
    "ax.set_xlabel('Œª', fontsize=12)\n",
    "ax.set_ylabel('Test Error', fontsize=12)\n",
    "ax.set_title(f'Two-Branch Test at Œ∑={NOISE_RATE}', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(LAMBDA_START - 0.02, LAMBDA_END + 0.02)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR}/figures/{NOTEBOOK_ID}_two_branch_eta08.png', dpi=150)\n",
    "plt.savefig(f'{SAVE_DIR}/figures/{NOTEBOOK_ID}_two_branch_eta08.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and Conclusion\n",
    "print('='*60)\n",
    "print(f'{NOTEBOOK_ID} SUMMARY: Two-Branch Test at Œ∑={NOISE_RATE}')\n",
    "print('='*60)\n",
    "\n",
    "# Initial conditions\n",
    "print(f'\\nüìä Initial Conditions:')\n",
    "for r in results:\n",
    "    print(f'   Seed {r[\"seed\"]}: Low-error={r[\"low_error_init\"]*100:.1f}%, Collapse={r[\"collapse_init\"]*100:.1f}%')\n",
    "\n",
    "# Gap analysis at multiple Œª points\n",
    "print(f'\\nüìä Gap Analysis:')\n",
    "gaps_at_lambda = {}\n",
    "\n",
    "for check_lam in [0.50, 0.60, 0.70]:\n",
    "    low_at_lam = df_low[df_low['lambda'] == check_lam]['error']\n",
    "    col_at_lam = df_col[df_col['lambda'] == check_lam]['error']\n",
    "    \n",
    "    if len(low_at_lam) > 0 and len(col_at_lam) > 0:\n",
    "        low_err = low_at_lam.mean()\n",
    "        col_err = col_at_lam.mean()\n",
    "        gap = col_err - low_err\n",
    "        gaps_at_lambda[check_lam] = gap\n",
    "        \n",
    "        print(f'\\n   At Œª={check_lam}:')\n",
    "        print(f'     Low-error: {low_err*100:.1f}%')\n",
    "        print(f'     Collapse:  {col_err*100:.1f}%')\n",
    "        print(f'     Gap:       {gap*100:.1f}%')\n",
    "\n",
    "# Conclusion\n",
    "if gaps_at_lambda:\n",
    "    avg_gap = np.mean(list(gaps_at_lambda.values()))\n",
    "    \n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'CONCLUSION (Œ∑={NOISE_RATE}):')\n",
    "    \n",
    "    if avg_gap > 0.15:\n",
    "        print(f'  ‚úÖ TWO-BRANCH STRUCTURE PERSISTS at Œ∑=0.8')\n",
    "        print(f'  ‚úÖ Average gap: {avg_gap*100:.1f}%')\n",
    "        print(f'  ‚úÖ Bistability is robust to extreme noise')\n",
    "    elif avg_gap > 0.05:\n",
    "        print(f'  ‚ö†Ô∏è WEAK TWO-BRANCH STRUCTURE at Œ∑=0.8')\n",
    "        print(f'  ‚ö†Ô∏è Average gap: {avg_gap*100:.1f}%')\n",
    "        print(f'  ‚ö†Ô∏è Bistability partially persists')\n",
    "    else:\n",
    "        print(f'  üîç NO CLEAR TWO-BRANCH STRUCTURE at Œ∑=0.8')\n",
    "        print(f'  üîç Average gap: {avg_gap*100:.1f}%')\n",
    "        print(f'  üîç This suggests Œ∑=0.8 is at or beyond the bistability boundary')\n",
    "        print(f'  üîç ‚Üí This is a FINDING: noise-bistability phase boundary')\n",
    "    \n",
    "    print(f'{\"=\"*60}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
