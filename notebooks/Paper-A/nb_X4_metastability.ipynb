{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# X4: Metastability / Survival Test (NLP)\n\n**Paper-A (P0009) \u2014 X Series: Cross-Domain Generality**\n\n| Item | Value |\n|------|-------|\n| Dataset | SST-2 (binary sentiment) |\n| Model | DistilBERT (pretrained) |\n| Noise | Symmetric, \u03b7 = 0.8 |\n| Trusted ratio | 5% |\n| \u03bb values | **0.34** (bad side), **0.36** (boundary), **0.42** (good side, control) |\n| Epochs | **12** (4\u00d7 baseline, long survival test) |\n| Seeds | 0\u201329 (n=30 per \u03bb) |\n| Total runs | **90** |\n| Estimated time | ~3\u20134 hours (A100) |\n\n**Purpose**: Test how long a \"good\" learning trajectory survives  \nwhen \u03bb is set near or slightly below the phase boundary (\u03bb\u22480.37).\n\n**Motivation**: X2 established the boundary at \u03bb\u22480.37. X3-fast showed  \nirreversible trapping. X4 asks: at boundary-adjacent \u03bb, do some runs  \nstay \"good\" while others collapse? How does survival fraction decay over epochs?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install -q transformers datasets accelerate\n\nimport os\nimport json\nimport time\nimport copy\nimport random\nimport warnings\nimport logging\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import (\n    DistilBertTokenizer, DistilBertForSequenceClassification\n)\nfrom datasets import load_dataset\n\nwarnings.filterwarnings(\"ignore\")\nlogging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\nos.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\nos.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    gpu = torch.cuda.get_device_properties(0)\n    print(f\"GPU: {gpu.name} ({gpu.total_mem/1e9:.1f} GB)\" if hasattr(gpu, 'total_mem') else f\"GPU: {gpu.name}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "CONFIG = {\n    \"experiment\": \"exp_X4_metastability\",\n    \"series\": \"X\",\n    \"experiment_id_prefix\": \"X4\",\n    \"dataset\": \"SST-2\",\n    \"num_classes\": 2,\n    \"model_name\": \"distilbert-base-uncased\",\n    \"noise_type\": \"symmetric\",\n    \"noise_rate\": 0.8,\n    \"trusted_ratio\": 0.05,\n    \"lambda_values\": [0.34, 0.36, 0.42],\n    \"seeds\": list(range(30)),\n    \"epochs\": 12,\n    \"batch_size\": 64,\n    \"learning_rate\": 2e-5,\n    \"weight_decay\": 0.01,\n    \"warmup_ratio\": 0.1,\n    \"max_seq_length\": 128,\n    \"use_bf16\": True,\n    \"max_train_samples\": 16000,\n    \"csv_measure_every_n_steps\": 25,\n    \"output_dir\": \"X4_results\",\n}\n\nos.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n\nn_used = min(int(67349 * 0.95), CONFIG[\"max_train_samples\"])\nsteps_per_epoch = n_used // CONFIG[\"batch_size\"]\ntotal_steps = steps_per_epoch * CONFIG[\"epochs\"]\nn_runs = len(CONFIG[\"lambda_values\"]) * len(CONFIG[\"seeds\"])\nest_per_run = total_steps * 25 / 1000 + 15\n\nprint(f\"Experiment: {CONFIG['experiment']}\")\nprint(f\"ID prefix: {CONFIG['experiment_id_prefix']}\")\nprint(f\"Total runs: {n_runs} ({len(CONFIG['lambda_values'])} \u03bb \u00d7 {len(CONFIG['seeds'])} seeds)\")\nprint(f\"\u03bb values: {CONFIG['lambda_values']}\")\nprint(f\"Epochs: {CONFIG['epochs']} (4\u00d7 baseline)\")\nprint(f\"Steps/epoch: ~{steps_per_epoch}, Total steps/run: ~{total_steps}\")\nprint(f\"Est. time/run: ~{est_per_run:.0f}s\")\nprint(f\"Est. total: ~{n_runs * est_per_run / 3600:.1f}h\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Google Drive Mount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from google.colab import drive\ndrive.mount('/content/drive')\n\nDRIVE_BASE = \"/content/drive/MyDrive/Paper-A_X_series\"\nDRIVE_OUTPUT = f\"{DRIVE_BASE}/{CONFIG['experiment']}\"\nos.makedirs(DRIVE_OUTPUT, exist_ok=True)\nLOCAL_OUTPUT = CONFIG[\"output_dir\"]\n\ndef save_to_drive(filename, data):\n    \"\"\"Save JSON to both local and Drive.\"\"\"\n    for d in [LOCAL_OUTPUT, DRIVE_OUTPUT]:\n        path = os.path.join(d, filename)\n        with open(path, \"w\") as f:\n            json.dump(data, f, indent=2)\n\ndef save_figure_to_drive(fig, filename, dpi=300):\n    \"\"\"Save figure to both local and Drive.\"\"\"\n    for d in [LOCAL_OUTPUT, DRIVE_OUTPUT]:\n        fig.savefig(os.path.join(d, filename), dpi=dpi, bbox_inches=\"tight\", facecolor=\"white\")\n\nprint(f\"Local: {LOCAL_OUTPUT}\")\nprint(f\"Drive: {DRIVE_OUTPUT}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Deterministic Seeding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def set_seed(seed):\n    \"\"\"Set all random seeds for full reproducibility.\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Loading SST-2...\")\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\nraw_dataset = load_dataset(\"glue\", \"sst2\")\n\ntrain_texts = list(raw_dataset[\"train\"][\"sentence\"])\ntrain_labels = list(raw_dataset[\"train\"][\"label\"])\ntest_texts = list(raw_dataset[\"validation\"][\"sentence\"])\ntest_labels = list(raw_dataset[\"validation\"][\"label\"])\n\nprint(f\"Train: {len(train_texts)}, Test: {len(test_texts)}\")\n\nprint(\"Tokenizing...\")\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=\"max_length\",\n                            max_length=CONFIG[\"max_seq_length\"], return_tensors=\"pt\")\ntest_encodings = tokenizer(test_texts, truncation=True, padding=\"max_length\",\n                           max_length=CONFIG[\"max_seq_length\"], return_tensors=\"pt\")\nprint(\"Done.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class TextDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n    def __len__(self):\n        return len(self.labels)\n    def __getitem__(self, idx):\n        item = {k: v[idx] for k, v in self.encodings.items()}\n        item[\"labels\"] = self.labels[idx]\n        return item\n\ntest_dataset = TextDataset(test_encodings, torch.tensor(test_labels, dtype=torch.long))\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\nprint(f\"Test loader: {len(test_dataset)} samples\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Noise Injection & Trusted Subset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def prepare_noisy_and_trusted(train_labels, noise_rate, trusted_ratio,\n                              num_classes, seed, max_noisy_samples=None):\n    rng = np.random.RandomState(seed + 10000)\n    n = len(train_labels)\n    labels = np.array(train_labels)\n    indices = np.arange(n)\n\n    # Stratified trusted selection\n    trusted_indices = []\n    for c in range(num_classes):\n        class_idx = indices[labels == c]\n        n_sel = max(1, int(len(class_idx) * trusted_ratio))\n        trusted_indices.extend(rng.choice(class_idx, size=n_sel, replace=False))\n    trusted_indices = np.array(sorted(trusted_indices))\n\n    # Noisy pool\n    noisy_mask = np.ones(n, dtype=bool)\n    noisy_mask[trusted_indices] = False\n    noisy_indices = indices[noisy_mask]\n\n    # Flip labels\n    noisy_labels = labels.copy()\n    n_flip = int(len(noisy_indices) * noise_rate)\n    flip_idx = rng.choice(noisy_indices, size=n_flip, replace=False)\n    for idx in flip_idx:\n        candidates = [c for c in range(num_classes) if c != noisy_labels[idx]]\n        noisy_labels[idx] = rng.choice(candidates)\n\n    # Subsample noisy pool\n    if max_noisy_samples and len(noisy_indices) > max_noisy_samples:\n        noisy_indices = np.sort(rng.choice(noisy_indices, size=max_noisy_samples, replace=False))\n\n    actual = np.mean(noisy_labels[noisy_indices] != labels[noisy_indices])\n    print(f\"  Trusted: {len(trusted_indices)} | Noisy: {len(noisy_indices)} (\u03b7={actual:.3f})\")\n    return torch.tensor(noisy_labels, dtype=torch.long), trusted_indices, noisy_indices\n\n# Quick test\nprint(\"Noise test:\")\n_, ti, ni = prepare_noisy_and_trusted(train_labels, 0.8, 0.05, 2, 0, 16000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Dual-Gradient Core"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_head_param_names(model):\n    \"\"\"Return parameter names of the classification head.\"\"\"\n    return [n for n, _ in model.named_parameters()\n            if \"classifier\" in n or \"pre_classifier\" in n]\n\ndef extract_grads(model):\n    \"\"\"Extract gradients from all parameters.\"\"\"\n    return {n: p.grad.clone() for n, p in model.named_parameters() if p.grad is not None}\n\ndef normalize_grad_dict(grads):\n    \"\"\"Normalize gradient dictionary to unit norm.\"\"\"\n    flat = torch.cat([g.flatten() for g in grads.values()])\n    norm = flat.norm()\n    if norm > 0:\n        return {n: g / norm for n, g in grads.items()}, norm.item()\n    return grads, 0.0\n\ndef cosine_sim(grads_a, grads_b, param_names=None):\n    \"\"\"Cosine similarity between two gradient dictionaries.\"\"\"\n    keys = [n for n in (param_names or sorted(set(grads_a) & set(grads_b)))\n            if n in grads_a and n in grads_b]\n    if not keys: return 0.0\n    va = torch.cat([grads_a[n].flatten() for n in keys])\n    vb = torch.cat([grads_b[n].flatten() for n in keys])\n    return F.cosine_similarity(va.unsqueeze(0), vb.unsqueeze(0)).item()\n\ndef set_mixed_grad(model, g_struct_n, g_value_n, lam):\n    \"\"\"Set mixed gradient on model parameters.\"\"\"\n    for n, p in model.named_parameters():\n        if n in g_struct_n and n in g_value_n:\n            p.grad = (1 - lam) * g_struct_n[n] + lam * g_value_n[n]\n        elif n in g_struct_n:\n            p.grad = (1 - lam) * g_struct_n[n]\n        elif n in g_value_n:\n            p.grad = lam * g_value_n[n]\n\n@torch.no_grad()\ndef evaluate(model, test_loader, device):\n    \"\"\"Evaluate model on test set.\"\"\"\n    model.eval()\n    correct = total = 0\n    total_loss = 0.0\n    n = 0\n    for batch in test_loader:\n        ids = batch[\"input_ids\"].to(device)\n        mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n        out = model(input_ids=ids, attention_mask=mask, labels=labels)\n        preds = out.logits.argmax(dim=-1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n        total_loss += out.loss.item()\n        n += 1\n    model.train()\n    acc = correct / total\n    return acc, 1.0 - acc, total_loss / n\n\nprint(\"Core functions defined.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Single Run (Epoch-Level Logging)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def run_single_experiment(lam, seed, config):\n    \"\"\"\n    Execute one dual-gradient training run with epoch-level logging.\n    Returns per-epoch test error for survival curve construction.\n    \"\"\"\n    exp_id = f\"{config['experiment_id_prefix']}-{seed:03d}-lam{lam:.2f}\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    set_seed(seed)\n    t_start = time.time()\n\n    # Data preparation\n    noisy_labels, trusted_idx, noisy_idx = prepare_noisy_and_trusted(\n        train_labels, config[\"noise_rate\"], config[\"trusted_ratio\"],\n        config[\"num_classes\"], seed, config.get(\"max_train_samples\")\n    )\n\n    noisy_dataset = TextDataset(\n        {k: v[noisy_idx] for k, v in train_encodings.items()},\n        noisy_labels[noisy_idx]\n    )\n    trusted_dataset = TextDataset(\n        {k: v[trusted_idx] for k, v in train_encodings.items()},\n        noisy_labels[trusted_idx]\n    )\n\n    struct_loader = DataLoader(noisy_dataset, batch_size=config[\"batch_size\"],\n                               shuffle=True, drop_last=True)\n    value_loader = DataLoader(trusted_dataset,\n                              batch_size=min(config[\"batch_size\"], len(trusted_dataset)),\n                              shuffle=True, drop_last=False)\n\n    # Model\n    set_seed(seed)\n    model = DistilBertForSequenceClassification.from_pretrained(\n        config[\"model_name\"], num_labels=config[\"num_classes\"]\n    ).to(device)\n    model.train()\n    head_params = get_head_param_names(model)\n\n    optimizer = torch.optim.AdamW(\n        model.parameters(),\n        lr=config[\"learning_rate\"],\n        weight_decay=config[\"weight_decay\"]\n    )\n\n    use_amp = config.get(\"use_bf16\", False) and device.type == \"cuda\"\n    amp_dtype = torch.bfloat16 if use_amp else torch.float32\n    measure_every = config.get(\"csv_measure_every_n_steps\", 25)\n\n    # Training with epoch logging\n    epoch_logs = []\n    global_step = 0\n    best_error = 1.0\n    best_epoch = 0\n\n    for epoch in range(config[\"epochs\"]):\n        value_iter = iter(value_loader)\n        epoch_csv = []\n        epoch_loss_s = 0.0\n        epoch_loss_v = 0.0\n        n_batches = 0\n\n        for struct_batch in struct_loader:\n            # Struct gradient\n            optimizer.zero_grad()\n            s_ids = struct_batch[\"input_ids\"].to(device)\n            s_mask = struct_batch[\"attention_mask\"].to(device)\n            s_labels = struct_batch[\"labels\"].to(device)\n            with torch.amp.autocast(\"cuda\", dtype=amp_dtype, enabled=use_amp):\n                out_s = model(input_ids=s_ids, attention_mask=s_mask, labels=s_labels)\n            out_s.loss.backward()\n            g_struct = extract_grads(model)\n\n            # Value gradient\n            optimizer.zero_grad()\n            try:\n                vb = next(value_iter)\n            except StopIteration:\n                value_iter = iter(value_loader)\n                vb = next(value_iter)\n            v_ids = vb[\"input_ids\"].to(device)\n            v_mask = vb[\"attention_mask\"].to(device)\n            v_labels = vb[\"labels\"].to(device)\n            with torch.amp.autocast(\"cuda\", dtype=amp_dtype, enabled=use_amp):\n                out_v = model(input_ids=v_ids, attention_mask=v_mask, labels=v_labels)\n            out_v.loss.backward()\n            g_value = extract_grads(model)\n\n            # Mix and step\n            g_sn, _ = normalize_grad_dict(g_struct)\n            g_vn, _ = normalize_grad_dict(g_value)\n            set_mixed_grad(model, g_sn, g_vn, lam)\n            optimizer.step()\n\n            global_step += 1\n            n_batches += 1\n            epoch_loss_s += out_s.loss.item()\n            epoch_loss_v += out_v.loss.item()\n\n            if global_step % measure_every == 0:\n                c = cosine_sim(g_struct, g_value, head_params)\n                epoch_csv.append(c)\n\n        # End-of-epoch evaluation\n        test_acc, test_error, test_loss = evaluate(model, test_loader, device)\n\n        if test_error < best_error:\n            best_error = test_error\n            best_epoch = epoch + 1\n\n        epoch_log = {\n            \"epoch\": epoch + 1,\n            \"test_acc\": round(test_acc, 4),\n            \"test_error\": round(test_error, 4),\n            \"test_loss\": round(test_loss, 4),\n            \"loss_struct\": round(epoch_loss_s / max(n_batches, 1), 4),\n            \"loss_value\": round(epoch_loss_v / max(n_batches, 1), 4),\n            \"c_sv_head\": round(float(np.mean(epoch_csv)) if epoch_csv else 0.0, 4),\n        }\n        epoch_logs.append(epoch_log)\n\n    elapsed = time.time() - t_start\n    final_acc, final_error, _ = evaluate(model, test_loader, device)\n\n    result = {\n        \"experiment_id\": exp_id,\n        \"experiment\": config[\"experiment\"],\n        \"lambda\": lam,\n        \"seed\": seed,\n        \"test_acc\": round(final_acc, 4),\n        \"test_error\": round(final_error, 4),\n        \"best_error\": round(best_error, 4),\n        \"best_epoch\": best_epoch,\n        \"epoch_logs\": epoch_logs,\n        \"avg_cos_struct_value\": round(float(np.mean([l[\"c_sv_head\"] for l in epoch_logs])), 4),\n        \"total_steps\": global_step,\n        \"time_seconds\": round(elapsed, 1),\n    }\n\n    del model, optimizer\n    torch.cuda.empty_cache()\n    return result\n\nprint(\"Single experiment function defined.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Main Execution Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "all_results = []\nn_total = len(CONFIG[\"lambda_values\"]) * len(CONFIG[\"seeds\"])\nrun_count = 0\nt_total = time.time()\n\nprint(\"=\" * 70)\nprint(f\"X4: Metastability Test \u2014 {n_total} runs\")\nprint(f\"  \u03bb = {CONFIG['lambda_values']}\")\nprint(f\"  Seeds: {len(CONFIG['seeds'])}, Epochs: {CONFIG['epochs']}\")\nprint(\"=\" * 70)\n\nfor lam in CONFIG[\"lambda_values\"]:\n    lam_results = []\n    print(f\"\\n{'='*50}\")\n    print(f\"\u03bb = {lam:.2f} ({len(CONFIG['seeds'])} seeds \u00d7 {CONFIG['epochs']} epochs)\")\n    print(f\"{'='*50}\")\n\n    for seed in CONFIG[\"seeds\"]:\n        run_count += 1\n        print(f\"  [{run_count}/{n_total}] seed={seed}\", end=\" \u2192 \")\n        result = run_single_experiment(lam, seed, CONFIG)\n        all_results.append(result)\n        lam_results.append(result)\n\n        # Progress\n        elapsed = time.time() - t_total\n        eta = elapsed / run_count * (n_total - run_count)\n        print(f\"err={result['test_error']:.4f} best={result['best_error']:.4f}@ep{result['best_epoch']} \"\n              f\"({result['time_seconds']:.0f}s) [ETA: {eta/60:.0f}m]\")\n\n    # Save per-\u03bb results incrementally\n    save_to_drive(f\"X4_lambda{lam:.2f}_results.json\", lam_results)\n    \n    # Quick summary for this \u03bb\n    errors = [r[\"test_error\"] for r in lam_results]\n    bests = [r[\"best_error\"] for r in lam_results]\n    print(f\"  Summary \u03bb={lam:.2f}: final={np.mean(errors):.4f}\u00b1{np.std(errors):.4f} \"\n          f\"best={np.mean(bests):.4f}\u00b1{np.std(bests):.4f}\")\n\n# Save everything\ntotal_time = time.time() - t_total\nsave_to_drive(\"X4_results.json\", all_results)\nsave_to_drive(\"X4_config.json\", CONFIG)\n\nprint(f\"\\n{'='*70}\")\nprint(f\"X4 COMPLETE: {n_total} runs in {total_time/60:.1f} min ({total_time/3600:.1f} h)\")\nprint(f\"{'='*70}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Survival Curve Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Load results\npath = os.path.join(LOCAL_OUTPUT, \"X4_results.json\")\nif not os.path.exists(path):\n    path = os.path.join(DRIVE_OUTPUT, \"X4_results.json\")\nwith open(path) as f:\n    results = json.load(f)\n\n# Organize by \u03bb\nlambda_data = {}\nfor r in results:\n    lam = r[\"lambda\"]\n    if lam not in lambda_data:\n        lambda_data[lam] = []\n    lambda_data[lam].append(r)\n\nlambdas = sorted(lambda_data.keys())\nn_epochs = CONFIG[\"epochs\"]\n\n# ============================================================\n# Define \"good\" threshold: error < 0.50 (better than random)\n# ============================================================\nGOOD_THRESHOLD = 0.50\n\nprint(\"=\" * 60)\nprint(f\"X4: Survival Analysis (threshold: error < {GOOD_THRESHOLD})\")\nprint(\"=\" * 60)\n\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\n\n# --- (A) Survival curve ---\nax = axes[0, 0]\ncolors = {\"0.34\": \"#D62728\", \"0.36\": \"#FF7F0E\", \"0.42\": \"#2CA02C\"}\n\nfor lam in lambdas:\n    runs = lambda_data[lam]\n    n_runs = len(runs)\n    survival = []\n    \n    for epoch in range(1, n_epochs + 1):\n        n_good = sum(1 for r in runs if r[\"epoch_logs\"][epoch-1][\"test_error\"] < GOOD_THRESHOLD)\n        survival.append(n_good / n_runs)\n    \n    color = colors.get(f\"{lam:.2f}\", \"#333333\")\n    ax.plot(range(1, n_epochs+1), survival, 'o-', color=color,\n            linewidth=2.5, markersize=7, label=f\"\u03bb={lam:.2f}\")\n    ax.fill_between(range(1, n_epochs+1), survival, alpha=0.1, color=color)\n\nax.axhline(0.5, color=\"gray\", ls=\":\", alpha=0.4)\nax.set_xlabel(\"Epoch\", fontsize=13)\nax.set_ylabel(f\"Fraction with error < {GOOD_THRESHOLD}\", fontsize=13)\nax.set_title(f\"(A) Survival Curve (threshold = {GOOD_THRESHOLD})\", fontsize=13, fontweight=\"bold\")\nax.legend(fontsize=11)\nax.grid(True, alpha=0.3)\nax.set_ylim(-0.05, 1.05)\nax.set_xlim(0.5, n_epochs + 0.5)\n\n# --- (B) Epoch trajectories (mean \u00b1 std) ---\nax = axes[0, 1]\nfor lam in lambdas:\n    runs = lambda_data[lam]\n    trajs = np.array([[l[\"test_error\"] for l in r[\"epoch_logs\"]] for r in runs])\n    means = trajs.mean(axis=0)\n    stds = trajs.std(axis=0)\n    epochs = range(1, n_epochs+1)\n    color = colors.get(f\"{lam:.2f}\", \"#333333\")\n    ax.plot(epochs, means, 'o-', color=color, linewidth=2.5, markersize=6, label=f\"\u03bb={lam:.2f}\")\n    ax.fill_between(epochs, means-stds, means+stds, alpha=0.15, color=color)\n\nax.axhline(0.5, color=\"gray\", ls=\":\", alpha=0.4, label=\"Random chance\")\nax.set_xlabel(\"Epoch\", fontsize=13)\nax.set_ylabel(\"Test Error\", fontsize=13)\nax.set_title(\"(B) Error Trajectory (mean \u00b1 std)\", fontsize=13, fontweight=\"bold\")\nax.legend(fontsize=10)\nax.grid(True, alpha=0.3)\n\n# --- (C) Individual traces ---\nax = axes[1, 0]\nfor lam in lambdas:\n    runs = lambda_data[lam]\n    color = colors.get(f\"{lam:.2f}\", \"#333333\")\n    for r in runs:\n        errs = [l[\"test_error\"] for l in r[\"epoch_logs\"]]\n        ax.plot(range(1, n_epochs+1), errs, '-', color=color, alpha=0.15, linewidth=0.8)\n    # Mean overlay\n    trajs = np.array([[l[\"test_error\"] for l in r[\"epoch_logs\"]] for r in runs])\n    ax.plot(range(1, n_epochs+1), trajs.mean(axis=0), 'o-', color=color,\n            linewidth=3, markersize=6, label=f\"\u03bb={lam:.2f} mean\", zorder=5)\n\nax.axhline(GOOD_THRESHOLD, color=\"black\", ls=\"--\", alpha=0.5, label=f\"Threshold ({GOOD_THRESHOLD})\")\nax.set_xlabel(\"Epoch\", fontsize=13)\nax.set_ylabel(\"Test Error\", fontsize=13)\nax.set_title(\"(C) Individual Run Traces\", fontsize=13, fontweight=\"bold\")\nax.legend(fontsize=9)\nax.grid(True, alpha=0.3)\n\n# --- (D) Final vs Best error scatter ---\nax = axes[1, 1]\nfor lam in lambdas:\n    runs = lambda_data[lam]\n    finals = [r[\"test_error\"] for r in runs]\n    bests = [r[\"best_error\"] for r in runs]\n    color = colors.get(f\"{lam:.2f}\", \"#333333\")\n    ax.scatter(bests, finals, color=color, alpha=0.6, s=50, edgecolors=\"black\",\n               linewidth=0.5, label=f\"\u03bb={lam:.2f}\")\n\n# Diagonal\nlim = [0, 1]\nax.plot(lim, lim, 'k--', alpha=0.3, label=\"No degradation\")\nax.set_xlabel(\"Best Error (across epochs)\", fontsize=13)\nax.set_ylabel(\"Final Error (epoch 12)\", fontsize=13)\nax.set_title(\"(D) Degradation: Final vs Best\", fontsize=13, fontweight=\"bold\")\nax.legend(fontsize=10)\nax.grid(True, alpha=0.3)\nax.set_xlim(0, 1)\nax.set_ylim(0, 1)\nax.set_aspect(\"equal\")\n\nfig.suptitle(\"X4: Metastability \u2014 SST-2 / DistilBERT / \u03b7=0.8 / 12 epochs\",\n             fontsize=15, fontweight=\"bold\", y=1.02)\nplt.tight_layout()\nsave_figure_to_drive(fig, \"X4_metastability.png\")\nplt.show()\n\n# ============================================================\n# Summary statistics\n# ============================================================\nprint(\"\\n\" + \"=\" * 60)\nprint(\"X4 SUMMARY\")\nprint(\"=\" * 60)\nfor lam in lambdas:\n    runs = lambda_data[lam]\n    finals = [r[\"test_error\"] for r in runs]\n    bests = [r[\"best_error\"] for r in runs]\n    best_epochs = [r[\"best_epoch\"] for r in runs]\n    n_survive = sum(1 for e in finals if e < GOOD_THRESHOLD)\n    n_ever_good = sum(1 for b in bests if b < GOOD_THRESHOLD)\n    degradation = [f - b for f, b in zip(finals, bests)]\n    \n    print(f\"\\n\u03bb = {lam:.2f} ({len(runs)} runs):\")\n    print(f\"  Final error:  {np.mean(finals):.4f} \u00b1 {np.std(finals):.4f}\")\n    print(f\"  Best error:   {np.mean(bests):.4f} \u00b1 {np.std(bests):.4f}\")\n    print(f\"  Best epoch:   {np.mean(best_epochs):.1f} \u00b1 {np.std(best_epochs):.1f}\")\n    print(f\"  Degradation:  {np.mean(degradation):.4f} \u00b1 {np.std(degradation):.4f}\")\n    print(f\"  Ever good:    {n_ever_good}/{len(runs)} ({100*n_ever_good/len(runs):.0f}%)\")\n    print(f\"  Survive ep12: {n_survive}/{len(runs)} ({100*n_survive/len(runs):.0f}%)\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Download Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"All results saved to: {DRIVE_OUTPUT}\")\nfor f in sorted(os.listdir(DRIVE_OUTPUT)):\n    sz = os.path.getsize(os.path.join(DRIVE_OUTPUT, f))\n    print(f\"  {f} ({sz/1024:.1f} KB)\")\n\nimport shutil\nshutil.make_archive(\"X4_results\", \"zip\", LOCAL_OUTPUT)\ntry:\n    from google.colab import files\n    files.download(\"X4_results.zip\")\nexcept:\n    pass\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}