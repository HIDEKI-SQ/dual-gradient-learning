{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp T2-spinodal: Measuring Œª‚Üì (Collapse ‚Üí Ordered Recovery Threshold)\n",
    "\n",
    "## ÁõÆÁöÑ\n",
    "CollapseÁä∂ÊÖã„Åã„ÇâŒª„ÇíÊÆµÈöéÁöÑ„Å´‰∏ã„Åí„ÄÅÂõûÂæ©„ÅåËµ∑„Åç„ÇãÈñæÂÄ§ÔºàŒª‚ÜìÔºâ„ÇíÊ∏¨ÂÆö„ÄÇ\n",
    "\n",
    "## ÂÆüÈ®ìË®≠Ë®à\n",
    "- **Phase 1 (Init)**: Œª=0.70„Åß50ep ‚Üí CollapseÁä∂ÊÖãÁ¢∫Á´ã\n",
    "- **Phase 2 (Sweep)**: Œª„ÇíÊÆµÈöéÁöÑ„Å´‰∏ãÈôç\n",
    "  - 0.54 ‚Üí 0.52 ‚Üí 0.50 ‚Üí 0.48 ‚Üí 0.46 ‚Üí 0.44 ‚Üí 0.42 ‚Üí 0.40 ‚Üí 0.38\n",
    "  - ÂêÑŒª„Åß10ep‰øùÊåÅ\n",
    "- **ÂõûÂæ©Âà§ÂÆö**: error ‚â§ 0.20 „Åå3ÂõûÈÄ£Á∂ö ‚Üí ÂÅúÊ≠¢„ÄÅŒª‚Üì„ÇíË®òÈå≤\n",
    "- **Œ∑**: 0.4\n",
    "- **Seeds**: 10Ôºà„Çπ„ÇØ„É™„Éº„Éã„É≥„Ç∞Ôºâ\n",
    "\n",
    "## ÁßëÂ≠¶ÁöÑÁõÆÊ®ô\n",
    "- Œª‚Üì„ÅÆÂàÜÂ∏É„ÇíÊ∏¨ÂÆö\n",
    "- T1-spinodalÔºàŒª‚ÜëÔºâ„Å®ÁµÑ„ÅøÂêà„Çè„Åõ„Å¶„Éí„Çπ„ÉÜ„É™„Ç∑„ÇπÂπÖ„ÇíÂÆöÈáèÂåñ\n",
    "- **„Éí„Çπ„ÉÜ„É™„Ç∑„ÇπÂπÖ = Œª‚Üë ‚àí Œª‚Üì**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os, glob, json, time\n",
    "from datetime import datetime\n",
    "\n",
    "EXP_NAME = 'exp_T_spinodal'\n",
    "NOTEBOOK_ID = 'T2-spinodal'\n",
    "BASE_DIR = '/content/drive/MyDrive/dual-gradient-learning/Paper-A'\n",
    "\n",
    "existing = glob.glob(f'{BASE_DIR}/{EXP_NAME}_*')\n",
    "if existing:\n",
    "    SAVE_DIR = sorted(existing)[-1]\n",
    "    print(f'üîÑ Resuming: {SAVE_DIR}')\n",
    "else:\n",
    "    TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    SAVE_DIR = f'{BASE_DIR}/{EXP_NAME}_{TIMESTAMP}'\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    print(f'üÜï New: {SAVE_DIR}')\n",
    "\n",
    "os.makedirs(f'{SAVE_DIR}/figures', exist_ok=True)\n",
    "print(f'Notebook: {NOTEBOOK_ID} (Œª‚Üì measurement, Œ∑=0.4)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils import parameters_to_vector\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "BATCH_SIZE = 256\n",
    "NUM_WORKERS = 4\n",
    "LR = 0.1\n",
    "K = 16\n",
    "\n",
    "# Spinodal experiment specific (COLLAPSE init)\n",
    "NOISE_RATE = 0.4\n",
    "INIT_LAMBDA = 0.70           # High Œª for collapse init\n",
    "INIT_EPOCHS = 50\n",
    "INIT_COLLAPSE_THRESHOLD = 0.35  # Must be in collapsed state (error > 0.35)\n",
    "\n",
    "# Œª sweep parameters (downward for Œª‚Üì)\n",
    "LAMBDA_SWEEP = [0.54, 0.52, 0.50, 0.48, 0.46, 0.44, 0.42, 0.40, 0.38]\n",
    "EPOCHS_PER_LAMBDA = 10\n",
    "RECOVERY_THRESHOLD = 0.20  # error ‚â§ this = recovered/ordered\n",
    "RECOVERY_PERSIST = 3       # consecutive epochs to confirm\n",
    "\n",
    "N_SEEDS = 10  # Screening first\n",
    "\n",
    "print(f'Œ∑ = {NOISE_RATE}')\n",
    "print(f'Init: Œª={INIT_LAMBDA} for {INIT_EPOCHS}ep (collapse state)')\n",
    "print(f'Sweep: {LAMBDA_SWEEP}')\n",
    "print(f'Per Œª: {EPOCHS_PER_LAMBDA}ep')\n",
    "print(f'Recovery: error‚â§{RECOVERY_THRESHOLD} for {RECOVERY_PERSIST} consecutive')\n",
    "print(f'Seeds: {N_SEEDS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet18():\n",
    "    model = resnet18(weights=None, num_classes=10)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "    return model\n",
    "\n",
    "class IndexedDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        return img, label, idx\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def inject_label_noise(labels, noise_rate, seed):\n",
    "    np.random.seed(seed)\n",
    "    noisy = labels.copy()\n",
    "    n_noisy = int(noise_rate * len(labels))\n",
    "    idx = np.random.choice(len(labels), n_noisy, replace=False)\n",
    "    for i in idx:\n",
    "        noisy[i] = np.random.choice([l for l in range(10) if l != labels[i]])\n",
    "    return noisy\n",
    "\n",
    "def load_cifar10():\n",
    "    tr = transforms.Compose([transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(),\n",
    "                             transforms.ToTensor(), transforms.Normalize((0.4914,0.4822,0.4465),(0.2023,0.1994,0.2010))])\n",
    "    te = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914,0.4822,0.4465),(0.2023,0.1994,0.2010))])\n",
    "    return torchvision.datasets.CIFAR10('./data', True, tr, download=True), torchvision.datasets.CIFAR10('./data', False, te, download=True)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            correct += (model(x).argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, opt, sched, clean_t, noisy_t, lam, cached_gv_ref):\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    step = cached_gv_ref['step']\n",
    "    cached_gv = cached_gv_ref['gv']\n",
    "    \n",
    "    for x, _, idx in train_loader:\n",
    "        x, idx = x.to(device), idx.to(device)\n",
    "        bn, bc = noisy_t[idx], clean_t[idx]\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss_s = crit(model(x), bn)\n",
    "        loss_s.backward(retain_graph=True)\n",
    "        gs = parameters_to_vector([p.grad for p in model.parameters()]).clone()\n",
    "        \n",
    "        if step % K == 0 or cached_gv is None:\n",
    "            opt.zero_grad()\n",
    "            loss_v = crit(model(x), bc)\n",
    "            loss_v.backward()\n",
    "            cached_gv = parameters_to_vector([p.grad for p in model.parameters()]).clone()\n",
    "        \n",
    "        gs_n = gs / (gs.norm() + 1e-12)\n",
    "        gv_n = cached_gv / (cached_gv.norm() + 1e-12)\n",
    "        \n",
    "        g_mix = (1 - lam) * gs_n + lam * gv_n\n",
    "        opt.zero_grad()\n",
    "        i = 0\n",
    "        for p in model.parameters():\n",
    "            n = p.numel()\n",
    "            p.grad = g_mix[i:i+n].view(p.shape).clone()\n",
    "            i += n\n",
    "        opt.step()\n",
    "        step += 1\n",
    "    \n",
    "    if sched is not None:\n",
    "        sched.step()\n",
    "    \n",
    "    cached_gv_ref['step'] = step\n",
    "    cached_gv_ref['gv'] = cached_gv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_spinodal_down(seed, train_loader, test_loader, clean_labels, noisy_labels):\n",
    "    \"\"\"Run spinodal experiment: collapse init ‚Üí sweep Œª downward ‚Üí find recovery point\"\"\"\n",
    "    \n",
    "    clean_t = torch.tensor(clean_labels, device=device)\n",
    "    noisy_t = torch.tensor(noisy_labels, device=device)\n",
    "    \n",
    "    set_seed(seed)\n",
    "    model = get_resnet18().to(device)\n",
    "    opt = optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "    sched = optim.lr_scheduler.MultiStepLR(opt, [50, 75], 0.1)\n",
    "    \n",
    "    cached_gv_ref = {'step': 0, 'gv': None}\n",
    "    trajectory = []\n",
    "    \n",
    "    # Phase 1: Initialize to COLLAPSE state (high Œª)\n",
    "    print(f'    Phase 1: Init COLLAPSE (Œª={INIT_LAMBDA}, {INIT_EPOCHS}ep)...')\n",
    "    for ep in range(INIT_EPOCHS):\n",
    "        train_one_epoch(model, train_loader, opt, sched, clean_t, noisy_t, INIT_LAMBDA, cached_gv_ref)\n",
    "        if (ep + 1) % 10 == 0:\n",
    "            err = 1 - evaluate(model, test_loader)\n",
    "            trajectory.append({'phase': 'init', 'lambda': INIT_LAMBDA, 'epoch': ep+1, 'error': err})\n",
    "            print(f'      Ep {ep+1}: err={err:.4f}')\n",
    "    \n",
    "    init_error = 1 - evaluate(model, test_loader)\n",
    "    \n",
    "    # Check if in collapsed state (error should be HIGH)\n",
    "    if init_error < INIT_COLLAPSE_THRESHOLD:\n",
    "        print(f'    ‚ö†Ô∏è Init failed: {init_error:.4f} < {INIT_COLLAPSE_THRESHOLD} (not collapsed!)')\n",
    "        return {\n",
    "            'seed': seed, 'init_error': init_error, 'init_success': False,\n",
    "            'lambda_down': None, 'recovered': False, 'trajectory': trajectory\n",
    "        }\n",
    "    \n",
    "    print(f'    ‚úÖ Init OK (collapsed): {init_error:.4f}')\n",
    "    print(f'    Phase 2: Œª sweep downward...')\n",
    "    \n",
    "    # Phase 2: Sweep Œª downward\n",
    "    lambda_down = None\n",
    "    recovery_count = 0\n",
    "    total_ep = INIT_EPOCHS\n",
    "    \n",
    "    for lam in LAMBDA_SWEEP:\n",
    "        print(f'      Œª={lam:.2f}: ', end='')\n",
    "        \n",
    "        for ep in range(EPOCHS_PER_LAMBDA):\n",
    "            train_one_epoch(model, train_loader, opt, sched, clean_t, noisy_t, lam, cached_gv_ref)\n",
    "            total_ep += 1\n",
    "            err = 1 - evaluate(model, test_loader)\n",
    "            trajectory.append({'phase': 'sweep', 'lambda': lam, 'epoch': total_ep, 'error': err})\n",
    "            \n",
    "            # Check recovery\n",
    "            if err <= RECOVERY_THRESHOLD:\n",
    "                recovery_count += 1\n",
    "                if recovery_count >= RECOVERY_PERSIST:\n",
    "                    lambda_down = lam\n",
    "                    print(f'‚úÖ RECOVERED at ep {total_ep} (err={err:.4f})')\n",
    "                    break\n",
    "            else:\n",
    "                recovery_count = 0\n",
    "        \n",
    "        if lambda_down is not None:\n",
    "            break\n",
    "        \n",
    "        final_err = 1 - evaluate(model, test_loader)\n",
    "        print(f'err={final_err:.4f} (still collapsed)')\n",
    "    \n",
    "    if lambda_down is None:\n",
    "        print(f'    ‚è±Ô∏è No recovery down to Œª={LAMBDA_SWEEP[-1]}')\n",
    "    \n",
    "    return {\n",
    "        'seed': seed,\n",
    "        'init_error': init_error,\n",
    "        'init_success': True,\n",
    "        'lambda_down': lambda_down,\n",
    "        'recovered': lambda_down is not None,\n",
    "        'final_lambda': lam,\n",
    "        'final_error': 1 - evaluate(model, test_loader),\n",
    "        'trajectory': trajectory\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = load_cifar10()\n",
    "clean_labels = np.array(trainset.targets)\n",
    "train_loader = DataLoader(IndexedDataset(trainset), BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader = DataLoader(testset, BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "m = get_resnet18().to(device)\n",
    "for _ in range(10): _ = m(torch.randn(BATCH_SIZE,3,32,32,device=device))\n",
    "del m; torch.cuda.empty_cache()\n",
    "print('Ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "ckpt = f'{SAVE_DIR}/{NOTEBOOK_ID}_checkpoint.json'\n",
    "done_seeds = set()\n",
    "\n",
    "if os.path.exists(ckpt):\n",
    "    results = json.load(open(ckpt))\n",
    "    done_seeds = {r['seed'] for r in results}\n",
    "    print(f'Loaded: {len(done_seeds)} done')\n",
    "\n",
    "for seed in range(N_SEEDS):\n",
    "    if seed in done_seeds:\n",
    "        continue\n",
    "    \n",
    "    run_num = len(results) + 1\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'[{run_num}/{N_SEEDS}] Seed {seed} | {NOTEBOOK_ID}')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    noisy_labels = inject_label_noise(clean_labels, NOISE_RATE, seed)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    result = run_spinodal_down(seed, train_loader, test_loader, clean_labels, noisy_labels)\n",
    "    dt = time.time() - t0\n",
    "    \n",
    "    result['experiment_id'] = f'{NOTEBOOK_ID}-{seed:03d}'\n",
    "    result['eta'] = NOISE_RATE\n",
    "    result['time'] = dt\n",
    "    \n",
    "    results.append(result)\n",
    "    done_seeds.add(seed)\n",
    "    \n",
    "    # Summary\n",
    "    if result['recovered']:\n",
    "        status = f\"‚úÖ Œª‚Üì={result['lambda_down']:.2f}\"\n",
    "    else:\n",
    "        status = f\"‚è±Ô∏è No recovery (down to Œª={result['final_lambda']:.2f})\"\n",
    "    print(f'\\n  {status} | {dt/60:.1f}min')\n",
    "    \n",
    "    # Stats\n",
    "    recovered_results = [r for r in results if r.get('recovered', False)]\n",
    "    if recovered_results:\n",
    "        lambdas = [r['lambda_down'] for r in recovered_results]\n",
    "        print(f'  Œª‚Üì stats: n={len(lambdas)}, mean={np.mean(lambdas):.3f}, std={np.std(lambdas):.3f}')\n",
    "    \n",
    "    json.dump(results, open(ckpt, 'w'), indent=2)\n",
    "    \n",
    "    remaining = N_SEEDS - run_num\n",
    "    print(f'  ETA: {remaining*dt/3600:.1f}h')\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print('\\n' + '='*60 + f'\\n{NOTEBOOK_ID} DONE\\n' + '='*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "json.dump(results, open(f'{SAVE_DIR}/{NOTEBOOK_ID}_results.json', 'w'), indent=2)\n",
    "df = pd.DataFrame([{k: v for k, v in r.items() if k != 'trajectory'} for r in results])\n",
    "df.to_csv(f'{SAVE_DIR}/{NOTEBOOK_ID}_results.csv', index=False)\n",
    "\n",
    "print('='*60)\n",
    "print(f'{NOTEBOOK_ID} SUMMARY | Œ∑={NOISE_RATE}')\n",
    "print('='*60)\n",
    "\n",
    "n_total = len(df)\n",
    "n_recovered = df['recovered'].sum()\n",
    "print(f'\\nüìä Results: {n_recovered}/{n_total} recovered ({100*n_recovered/n_total:.0f}%)')\n",
    "\n",
    "if n_recovered > 0:\n",
    "    lambda_downs = df[df['recovered'] == True]['lambda_down'].values\n",
    "    print(f'\\nüìà Œª‚Üì Distribution:')\n",
    "    print(f'   Mean: {lambda_downs.mean():.3f}')\n",
    "    print(f'   Std:  {lambda_downs.std():.3f}')\n",
    "    print(f'   Min:  {lambda_downs.min():.2f}')\n",
    "    print(f'   Max:  {lambda_downs.max():.2f}')\n",
    "else:\n",
    "    print('\\n‚ö†Ô∏è No recovery events observed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# 1. Œª‚Üì histogram\n",
    "ax = axes[0]\n",
    "recovered_df = df[df['recovered'] == True]\n",
    "if len(recovered_df) > 0:\n",
    "    ax.hist(recovered_df['lambda_down'], bins=np.arange(0.36, 0.56, 0.02), \n",
    "            color='blue', alpha=0.7, edgecolor='black')\n",
    "    ax.axvline(recovered_df['lambda_down'].mean(), color='darkblue', linestyle='--', \n",
    "               linewidth=2, label=f\"Mean Œª‚Üì={recovered_df['lambda_down'].mean():.3f}\")\n",
    "ax.set_xlabel('Œª‚Üì (Recovery Threshold)', fontsize=12)\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.set_title('Œª‚Üì Distribution (Collapse ‚Üí Ordered)', fontsize=12)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Sample trajectories\n",
    "ax = axes[1]\n",
    "for r in results[:5]:\n",
    "    if r['trajectory']:\n",
    "        sweep_traj = [t for t in r['trajectory'] if t['phase'] == 'sweep']\n",
    "        if sweep_traj:\n",
    "            eps = [t['epoch'] for t in sweep_traj]\n",
    "            errs = [t['error'] for t in sweep_traj]\n",
    "            color = 'blue' if r['recovered'] else 'red'\n",
    "            ax.plot(eps, errs, alpha=0.7, color=color, linewidth=1.5,\n",
    "                   label=f\"seed {r['seed']}: Œª‚Üì={r.get('lambda_down', 'N/A')}\")\n",
    "\n",
    "ax.axhline(RECOVERY_THRESHOLD, color='green', linestyle='--', linewidth=2, label=f'Recovery ({RECOVERY_THRESHOLD})')\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Test Error', fontsize=12)\n",
    "ax.set_title('Error Trajectories During Œª Sweep', fontsize=12)\n",
    "ax.legend(loc='upper right', fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Error vs Œª\n",
    "ax = axes[2]\n",
    "for r in results[:5]:\n",
    "    if r['trajectory']:\n",
    "        sweep_traj = [t for t in r['trajectory'] if t['phase'] == 'sweep']\n",
    "        if sweep_traj:\n",
    "            lambdas = [t['lambda'] for t in sweep_traj]\n",
    "            errs = [t['error'] for t in sweep_traj]\n",
    "            color = 'blue' if r['recovered'] else 'red'\n",
    "            ax.scatter(lambdas, errs, alpha=0.5, color=color, s=20)\n",
    "\n",
    "ax.axhline(RECOVERY_THRESHOLD, color='green', linestyle='--', linewidth=2)\n",
    "ax.set_xlabel('Œª', fontsize=12)\n",
    "ax.set_ylabel('Test Error', fontsize=12)\n",
    "ax.set_title('Error vs Œª', fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR}/figures/{NOTEBOOK_ID}_analysis.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nüìà Figure saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hysteresis width calculation (if T1-spinodal results available)\n",
    "print('='*60)\n",
    "print('HYSTERESIS WIDTH CALCULATION')\n",
    "print('='*60)\n",
    "\n",
    "# Try to load T1-spinodal results\n",
    "t1_results_path = f'{SAVE_DIR}/T1-spinodal_results.json'\n",
    "if os.path.exists(t1_results_path):\n",
    "    t1_results = json.load(open(t1_results_path))\n",
    "    t1_collapsed = [r for r in t1_results if r.get('collapsed', False)]\n",
    "    \n",
    "    if t1_collapsed and n_recovered > 0:\n",
    "        lambda_up_mean = np.mean([r['lambda_up'] for r in t1_collapsed])\n",
    "        lambda_down_mean = lambda_downs.mean()\n",
    "        hysteresis_width = lambda_up_mean - lambda_down_mean\n",
    "        \n",
    "        print(f'\\nüìä Œª‚Üë (ordered‚Üícollapse): {lambda_up_mean:.3f}')\n",
    "        print(f'üìä Œª‚Üì (collapse‚Üíordered): {lambda_down_mean:.3f}')\n",
    "        print(f'\\nüîÑ HYSTERESIS WIDTH = {hysteresis_width:.3f}')\n",
    "    else:\n",
    "        print('\\n‚ö†Ô∏è Insufficient data for hysteresis calculation')\n",
    "else:\n",
    "    print(f'\\n‚ö†Ô∏è T1-spinodal results not found at {t1_results_path}')\n",
    "    print('   Run T1-spinodal first to calculate hysteresis width')"
   ]
  }
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}},
 "nbformat": 4,
 "nbformat_minor": 4
}
