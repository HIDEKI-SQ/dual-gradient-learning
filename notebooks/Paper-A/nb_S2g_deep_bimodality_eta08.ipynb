{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp S2g: High Noise Regime (Œ∑=0.8)\n",
    "\n",
    "## ÁõÆÁöÑ\n",
    "È´ò„Éé„Ç§„Ç∫Áí∞Â¢É(Œ∑=0.8)„Åß„ÅÆÁõ∏Ëª¢Áßª„ÇíÁ¢∫Ë™ç„ÄÇ\n",
    "\n",
    "## ÂÆüÈ®ìË®≠Ë®à\n",
    "- **Œ∑**: 0.8ÔºàÈ´ò„Éé„Ç§„Ç∫Ôºâ\n",
    "- **Œª**: 0.40, 0.44, 0.48ÔºàR3a„ÅßŒª_‚Üë‚âà0.44„Å®Âà§ÊòéÔºâ\n",
    "- **Seeds**: ÂêÑ34 (ÂêàË®à102 runs)\n",
    "\n",
    "## Êé®ÂÆöÊôÇÈñì\n",
    "~102 √ó 11 min ‚âà **19h**\n",
    "\n",
    "## ÁßëÂ≠¶ÁöÑÁõÆÊ®ô\n",
    "- R3a„ÅßŒ∑=0.8„Åß„ÅØŒª_‚Üë=0.44ÔºàÂÖ®seedÂêå‰∏ÄÔºâ„Å®Âà§Êòé\n",
    "- Œ∑=0.4(Œª_c‚âà0.50)„Å®„ÅÆÊØîËºÉ: È´ò„Éé„Ç§„Ç∫„ÅßÈÅ∑ÁßªÁÇπ„ÅåÂäáÁöÑ„Å´‰Ωé‰∏ã\n",
    "- PRX Figure: Œ∑‰æùÂ≠òÊÄß„ÅÆÁõ∏Âõ≥ÂÆåÊàê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os, glob, json, time\n",
    "from datetime import datetime\n",
    "\n",
    "EXP_NAME = 'exp_S2_deep_bimodality'\n",
    "NOTEBOOK_ID = 'S2g'\n",
    "BASE_DIR = '/content/drive/MyDrive/dual-gradient-learning/Paper-A'\n",
    "\n",
    "existing = glob.glob(f'{BASE_DIR}/{EXP_NAME}_*')\n",
    "if existing:\n",
    "    SAVE_DIR = sorted(existing)[-1]\n",
    "    print(f'üîÑ Resuming: {SAVE_DIR}')\n",
    "else:\n",
    "    TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    SAVE_DIR = f'{BASE_DIR}/{EXP_NAME}_{TIMESTAMP}'\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    print(f'üÜï New: {SAVE_DIR}')\n",
    "\n",
    "os.makedirs(f'{SAVE_DIR}/figures', exist_ok=True)\n",
    "print(f'Notebook: {NOTEBOOK_ID} (Œ∑=0.8, high noise regime)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils import parameters_to_vector\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet18():\n",
    "    model = resnet18(weights=None, num_classes=10)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "    return model\n",
    "\n",
    "class IndexedDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        return img, label, idx\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "NUM_WORKERS = 4\n",
    "EPOCHS = 100\n",
    "LR = 0.1\n",
    "K = 16\n",
    "NOISE_RATE = 0.8  # HIGH NOISE!\n",
    "LAMBDA_VALUES = [0.40, 0.44, 0.48]  # Based on R3a: Œª_‚Üë ‚âà 0.44 for Œ∑=0.8\n",
    "SEEDS_PER_LAMBDA = 34\n",
    "\n",
    "experiments = []\n",
    "for lam in LAMBDA_VALUES:\n",
    "    for s in range(SEEDS_PER_LAMBDA):\n",
    "        experiments.append({'lambda': lam, 'seed': s})\n",
    "\n",
    "print(f'Œ∑ = {NOISE_RATE} (HIGH NOISE)')\n",
    "print(f'Œª values: {LAMBDA_VALUES}')\n",
    "print(f'Seeds per Œª: {SEEDS_PER_LAMBDA}')\n",
    "print(f'Total: {len(experiments)} runs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def inject_label_noise(labels, noise_rate, seed):\n",
    "    np.random.seed(seed)\n",
    "    noisy = labels.copy()\n",
    "    n_noisy = int(noise_rate * len(labels))\n",
    "    idx = np.random.choice(len(labels), n_noisy, replace=False)\n",
    "    for i in idx:\n",
    "        noisy[i] = np.random.choice([l for l in range(10) if l != labels[i]])\n",
    "    return noisy\n",
    "\n",
    "def load_cifar10():\n",
    "    tr = transforms.Compose([transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(),\n",
    "                             transforms.ToTensor(), transforms.Normalize((0.4914,0.4822,0.4465),(0.2023,0.1994,0.2010))])\n",
    "    te = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914,0.4822,0.4465),(0.2023,0.1994,0.2010))])\n",
    "    return torchvision.datasets.CIFAR10('./data', True, tr, download=True), torchvision.datasets.CIFAR10('./data', False, te, download=True)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            correct += (model(x).argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dual_gradient(model, train_loader, test_loader, clean_labels, noisy_labels, lam):\n",
    "    opt = optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "    sched = optim.lr_scheduler.MultiStepLR(opt, [50, 75], 0.1)\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    clean_t = torch.tensor(clean_labels, device=device)\n",
    "    noisy_t = torch.tensor(noisy_labels, device=device)\n",
    "    cached_gv = None\n",
    "    step = 0\n",
    "    cos_hist = []\n",
    "    error_hist = []\n",
    "    \n",
    "    for ep in range(EPOCHS):\n",
    "        model.train()\n",
    "        ep_cos = []\n",
    "        for x, _, idx in train_loader:\n",
    "            x, idx = x.to(device), idx.to(device)\n",
    "            bn, bc = noisy_t[idx], clean_t[idx]\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            loss_s = crit(model(x), bn)\n",
    "            loss_s.backward(retain_graph=True)\n",
    "            gs = parameters_to_vector([p.grad for p in model.parameters()]).clone()\n",
    "            \n",
    "            if step % K == 0 or cached_gv is None:\n",
    "                opt.zero_grad()\n",
    "                loss_v = crit(model(x), bc)\n",
    "                loss_v.backward()\n",
    "                cached_gv = parameters_to_vector([p.grad for p in model.parameters()]).clone()\n",
    "            \n",
    "            gs_n = gs / (gs.norm() + 1e-12)\n",
    "            gv_n = cached_gv / (cached_gv.norm() + 1e-12)\n",
    "            ep_cos.append((gs_n @ gv_n).item())\n",
    "            \n",
    "            g_mix = (1 - lam) * gs_n + lam * gv_n\n",
    "            opt.zero_grad()\n",
    "            i = 0\n",
    "            for p in model.parameters():\n",
    "                n = p.numel()\n",
    "                p.grad = g_mix[i:i+n].view(p.shape).clone()\n",
    "                i += n\n",
    "            opt.step()\n",
    "            step += 1\n",
    "        sched.step()\n",
    "        cos_hist.append(np.mean(ep_cos))\n",
    "        \n",
    "        if (ep + 1) % 10 == 0:\n",
    "            error_hist.append({'epoch': ep + 1, 'error': 1 - evaluate(model, test_loader)})\n",
    "    \n",
    "    final_error = 1 - evaluate(model, test_loader)\n",
    "    return final_error, np.mean(cos_hist), error_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = load_cifar10()\n",
    "clean_labels = np.array(trainset.targets)\n",
    "train_loader = DataLoader(IndexedDataset(trainset), BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader = DataLoader(testset, BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "m = get_resnet18().to(device)\n",
    "for _ in range(20): _ = m(torch.randn(BATCH_SIZE,3,32,32,device=device))\n",
    "del m; torch.cuda.empty_cache()\n",
    "print('Ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "ckpt = f'{SAVE_DIR}/{NOTEBOOK_ID}_checkpoint.json'\n",
    "done = set()\n",
    "\n",
    "if os.path.exists(ckpt):\n",
    "    results = json.load(open(ckpt))\n",
    "    done = {(r['lambda'], r['seed']) for r in results}\n",
    "    print(f'Loaded: {len(done)} done')\n",
    "\n",
    "total = len(experiments)\n",
    "for i, exp in enumerate(experiments):\n",
    "    lam, seed = exp['lambda'], exp['seed']\n",
    "    if (lam, seed) in done: continue\n",
    "    \n",
    "    run = len(results) + 1\n",
    "    print(f'\\n[{run}/{total}] Œ∑={NOISE_RATE} Œª={lam} seed={seed}')\n",
    "    \n",
    "    set_seed(seed)\n",
    "    noisy = inject_label_noise(clean_labels, NOISE_RATE, seed)\n",
    "    model = get_resnet18().to(device)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    err, cos, error_hist = train_dual_gradient(model, train_loader, test_loader, clean_labels, noisy, lam)\n",
    "    dt = time.time() - t0\n",
    "    \n",
    "    phase = 'ordered' if err <= 0.20 else ('collapsed' if err >= 0.40 else 'intermediate')\n",
    "    results.append({\n",
    "        'experiment_id': f'{NOTEBOOK_ID}-{lam:.2f}-{seed:03d}',\n",
    "        'eta': NOISE_RATE,\n",
    "        'lambda': lam,\n",
    "        'seed': seed,\n",
    "        'final_error': err,\n",
    "        'avg_cos': cos,\n",
    "        'phase': phase,\n",
    "        'time': dt,\n",
    "        'error_history': error_hist\n",
    "    })\n",
    "    done.add((lam, seed))\n",
    "    \n",
    "    st = '‚úÖ' if phase == 'ordered' else ('‚ö†Ô∏è' if phase == 'collapsed' else 'üî∂')\n",
    "    print(f'  Error: {err:.4f} | {phase} {st} | {dt/60:.1f}min')\n",
    "    \n",
    "    json.dump(results, open(ckpt, 'w'), indent=2)\n",
    "    remaining = total - run\n",
    "    print(f'  ETA: {remaining*dt/3600:.1f}h')\n",
    "    \n",
    "    del model; torch.cuda.empty_cache()\n",
    "\n",
    "print('\\n' + '='*50 + f'\\n{NOTEBOOK_ID} DONE\\n' + '='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Save results\n",
    "json.dump(results, open(f'{SAVE_DIR}/{NOTEBOOK_ID}_results.json', 'w'), indent=2)\n",
    "df = pd.DataFrame([{k: v for k, v in r.items() if k != 'error_history'} for r in results])\n",
    "df.to_csv(f'{SAVE_DIR}/{NOTEBOOK_ID}_results.csv', index=False)\n",
    "\n",
    "print('='*60)\n",
    "print(f'{NOTEBOOK_ID} SUMMARY - HIGH NOISE REGIME (Œ∑={NOISE_RATE})')\n",
    "print('='*60)\n",
    "\n",
    "for lam in LAMBDA_VALUES:\n",
    "    sub = df[df['lambda'] == lam]\n",
    "    errors = sub['final_error'].values\n",
    "    n_ord = (sub['phase'] == 'ordered').sum()\n",
    "    n_col = (sub['phase'] == 'collapsed').sum()\n",
    "    n_int = (sub['phase'] == 'intermediate').sum()\n",
    "    \n",
    "    print(f'\\nŒª = {lam}:')\n",
    "    print(f'  N = {len(sub)}')\n",
    "    print(f'  Mean error: {errors.mean():.4f} ¬± {errors.std():.4f}')\n",
    "    print(f'  Ordered: {n_ord} ({100*n_ord/len(sub):.1f}%)')\n",
    "    print(f'  Intermediate: {n_int} ({100*n_int/len(sub):.1f}%)')\n",
    "    print(f'  Collapsed: {n_col} ({100*n_col/len(sub):.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization for high noise regime\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "colors = {0.40: 'green', 0.44: 'orange', 0.48: 'red'}\n",
    "\n",
    "# Histogram by Œª\n",
    "ax = axes[0]\n",
    "for lam in LAMBDA_VALUES:\n",
    "    sub = df[df['lambda'] == lam]\n",
    "    ax.hist(sub['final_error'], bins=15, alpha=0.5, label=f'Œª={lam}', color=colors[lam], edgecolor='black')\n",
    "ax.axvline(0.20, color='green', linestyle='--', linewidth=2)\n",
    "ax.axvline(0.40, color='red', linestyle='--', linewidth=2)\n",
    "ax.set_xlabel('Final Error', fontsize=12)\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.set_title(f'Error Distribution (Œ∑={NOISE_RATE})', fontsize=12)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Phase fraction by Œª\n",
    "ax = axes[1]\n",
    "x = np.arange(len(LAMBDA_VALUES))\n",
    "width = 0.25\n",
    "\n",
    "ord_fracs = []\n",
    "int_fracs = []\n",
    "col_fracs = []\n",
    "for lam in LAMBDA_VALUES:\n",
    "    sub = df[df['lambda'] == lam]\n",
    "    n = len(sub)\n",
    "    ord_fracs.append((sub['phase'] == 'ordered').sum() / n * 100 if n > 0 else 0)\n",
    "    int_fracs.append((sub['phase'] == 'intermediate').sum() / n * 100 if n > 0 else 0)\n",
    "    col_fracs.append((sub['phase'] == 'collapsed').sum() / n * 100 if n > 0 else 0)\n",
    "\n",
    "ax.bar(x - width, ord_fracs, width, label='Ordered', color='green', alpha=0.7)\n",
    "ax.bar(x, int_fracs, width, label='Intermediate', color='orange', alpha=0.7)\n",
    "ax.bar(x + width, col_fracs, width, label='Collapsed', color='red', alpha=0.7)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'Œª={l}' for l in LAMBDA_VALUES])\n",
    "ax.set_ylabel('Fraction (%)', fontsize=12)\n",
    "ax.set_title(f'Phase Fraction (Œ∑={NOISE_RATE})', fontsize=12)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Comparison with Œ∑=0.4 expectation\n",
    "ax = axes[2]\n",
    "mean_errors = [df[df['lambda'] == lam]['final_error'].mean() for lam in LAMBDA_VALUES]\n",
    "std_errors = [df[df['lambda'] == lam]['final_error'].std() for lam in LAMBDA_VALUES]\n",
    "ax.errorbar(LAMBDA_VALUES, mean_errors, yerr=std_errors, fmt='o-', capsize=5, \n",
    "            color='purple', linewidth=2, markersize=8, label=f'Œ∑={NOISE_RATE}')\n",
    "ax.axhline(0.20, color='green', linestyle='--', linewidth=2, label='ordered threshold')\n",
    "ax.axhline(0.40, color='red', linestyle='--', linewidth=2, label='collapse threshold')\n",
    "ax.axvline(0.44, color='orange', linestyle=':', linewidth=2, label='R3a Œª_‚Üë=0.44')\n",
    "ax.set_xlabel('Œª', fontsize=12)\n",
    "ax.set_ylabel('Mean Error', fontsize=12)\n",
    "ax.set_title(f'Transition Curve (Œ∑={NOISE_RATE})', fontsize=12)\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR}/figures/{NOTEBOOK_ID}_high_noise.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print('\\nüìä Key finding for PRX Intelligence:')\n",
    "print(f'  Œ∑=0.4: Œª_c ‚âà 0.50 (from S1/S2a-e)')\n",
    "print(f'  Œ∑=0.8: Œª_c ‚âà 0.44 (from R3a, confirmed here)')\n",
    "print(f'  ‚Üí Noise rate Œ∑ controls transition point Œª_c')"
   ]
  }
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}},
 "nbformat": 4,
 "nbformat_minor": 4
}
