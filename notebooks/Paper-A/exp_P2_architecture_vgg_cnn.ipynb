{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp P2: Architecture Universality - VGG11 & SimpleCNN (lr=0.01)\n",
    "\n",
    "## ÁõÆÁöÑ\n",
    "PÂÆüÈ®ì„ÅßVGG11/SimpleCNN„ÅåÊó©ÊúüÂ¥©Â£ä„Åó„ÅüÂéüÂõ†„ÇíÊ§úË®º„ÄÇ\n",
    "lr=0.1„ÅØResNetÂêë„Åë„Å´ÊúÄÈÅ©Âåñ„Åï„Çå„Å¶„ÅÑ„Åü„Åü„ÇÅ„ÄÅlr=0.01„ÅßÂÜçÂÆüÈ®ì„ÄÇ\n",
    "\n",
    "## ÂÆüÈ®ìË®≠Ë®à\n",
    "- **Models**: VGG11, SimpleCNN\n",
    "- **Œª**: [0.15, 0.25, 0.35, 0.45, 0.55, 0.65]\n",
    "- **Œ∑**: 0.4ÔºàÂõ∫ÂÆöÔºâ\n",
    "- **Seeds**: 0-2Ôºà3ÂÄãÔºâ\n",
    "- **lr**: 0.01ÔºàÂ§âÊõ¥ÁÇπÔºâ\n",
    "- **Total**: 2 √ó 6 √ó 3 = **36 runs**\n",
    "\n",
    "## Êé®ÂÆöÊôÇÈñì\n",
    "~36 runs √ó 9 min ‚âà **5.5 ÊôÇÈñì**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== „Çª„ÉÉ„Éà„Ç¢„ÉÉ„Éó =====\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "EXP_NAME = 'exp_P2_architecture_vgg_cnn'\n",
    "BASE_DIR = '/content/drive/MyDrive/dual-gradient-learning/Paper-A'\n",
    "\n",
    "existing = glob.glob(f'{BASE_DIR}/{EXP_NAME}_*')\n",
    "if existing:\n",
    "    SAVE_DIR = sorted(existing)[-1]\n",
    "    TIMESTAMP = SAVE_DIR.split('_')[-2] + '_' + SAVE_DIR.split('_')[-1]\n",
    "    print(f'üîÑ Resuming from: {SAVE_DIR}')\n",
    "else:\n",
    "    TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    SAVE_DIR = f'{BASE_DIR}/{EXP_NAME}_{TIMESTAMP}'\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    print(f'üÜï New experiment: {SAVE_DIR}')\n",
    "\n",
    "os.makedirs(f'{SAVE_DIR}/figures', exist_ok=True)\n",
    "\n",
    "print(f'Experiment: {EXP_NAME}')\n",
    "print(f'Save directory: {SAVE_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils import parameters_to_vector\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== „É¢„Éá„É´ÂÆöÁæ© =====\n",
    "class IndexedDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        return img, label, idx\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"Simple 4-layer CNN for baseline comparison\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 * 8 * 8, 256), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "class VGG11(nn.Module):\n",
    "    \"\"\"VGG11 for CIFAR-10\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # Block 2\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # Block 3\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # Block 4\n",
    "            nn.Conv2d(256, 512, 3, padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # Block 5\n",
    "            nn.Conv2d(512, 512, 3, padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 512), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "def get_model(name):\n",
    "    if name == 'VGG11':\n",
    "        return VGG11()\n",
    "    elif name == 'SimpleCNN':\n",
    "        return SimpleCNN()\n",
    "    else:\n",
    "        raise ValueError(f'Unknown model: {name}')\n",
    "\n",
    "# Test models\n",
    "for name in ['VGG11', 'SimpleCNN']:\n",
    "    m = get_model(name)\n",
    "    params = sum(p.numel() for p in m.parameters())\n",
    "    print(f'{name}: {params:,} parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ÂÆüÈ®ì„Éë„É©„É°„Éº„Çø =====\n",
    "BATCH_SIZE = 256\n",
    "NUM_WORKERS = 4\n",
    "EPOCHS = 100\n",
    "LR = 0.01  # ‚òÖ Â§âÊõ¥ÁÇπ: 0.1 ‚Üí 0.01\n",
    "K = 16\n",
    "\n",
    "MODELS = ['VGG11', 'SimpleCNN']\n",
    "LAMBDAS = [0.15, 0.25, 0.35, 0.45, 0.55, 0.65]\n",
    "NOISE_RATE = 0.4\n",
    "SEEDS = [0, 1, 2]\n",
    "\n",
    "experiments = []\n",
    "for model in MODELS:\n",
    "    for lam in LAMBDAS:\n",
    "        for seed in SEEDS:\n",
    "            experiments.append({'model': model, 'lambda': lam, 'seed': seed})\n",
    "\n",
    "print(f'Models: {MODELS}')\n",
    "print(f'Lambdas: {LAMBDAS}')\n",
    "print(f'Learning rate: {LR} (changed from 0.1)')\n",
    "print(f'Total experiments: {len(experiments)}')\n",
    "print(f'Estimated time: {len(experiments) * 9 / 60:.1f} hours')\n",
    "\n",
    "# Save config\n",
    "config = {\n",
    "    'experiment': EXP_NAME,\n",
    "    'timestamp': TIMESTAMP,\n",
    "    'description': 'VGG11/SimpleCNN with lr=0.01 (re-experiment)',\n",
    "    'parameters': {\n",
    "        'models': MODELS,\n",
    "        'lambdas': LAMBDAS,\n",
    "        'noise_rate': NOISE_RATE,\n",
    "        'seeds': SEEDS,\n",
    "        'epochs': EPOCHS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'lr': LR,\n",
    "        'K': K\n",
    "    },\n",
    "    'total_runs': len(experiments)\n",
    "}\n",
    "with open(f'{SAVE_DIR}/{EXP_NAME}_config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== „É¶„Éº„ÉÜ„Ç£„É™„ÉÜ„Ç£Èñ¢Êï∞ =====\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def load_cifar10():\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "    ])\n",
    "    trainset = torchvision.datasets.CIFAR10('./data', train=True, download=True, transform=transform_train)\n",
    "    testset = torchvision.datasets.CIFAR10('./data', train=False, download=True, transform=transform_test)\n",
    "    return trainset, testset\n",
    "\n",
    "def inject_label_noise(labels, noise_rate, seed):\n",
    "    np.random.seed(seed)\n",
    "    labels = np.array(labels)\n",
    "    n_samples = len(labels)\n",
    "    n_noisy = int(noise_rate * n_samples)\n",
    "    noisy_idx = np.random.choice(n_samples, n_noisy, replace=False)\n",
    "    noisy_labels = labels.copy()\n",
    "    for idx in noisy_idx:\n",
    "        choices = [l for l in range(10) if l != labels[idx]]\n",
    "        noisy_labels[idx] = np.random.choice(choices)\n",
    "    return noisy_labels\n",
    "\n",
    "def get_data_loaders(trainset, testset):\n",
    "    train_loader = DataLoader(IndexedDataset(trainset), batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    test_loader = DataLoader(testset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Dual-Gradient Learning =====\n",
    "def train_dual_gradient(model, train_loader, test_loader, clean_labels, noisy_labels, lam):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 75], gamma=0.1)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    clean_labels_tensor = torch.tensor(clean_labels, device=device)\n",
    "    noisy_labels_tensor = torch.tensor(noisy_labels, device=device)\n",
    "    \n",
    "    cached_g_value = None\n",
    "    global_step = 0\n",
    "    \n",
    "    best_acc = 0\n",
    "    cos_history = []\n",
    "    history = {'epoch': [], 'test_acc': [], 'test_error': []}\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        epoch_cos = []\n",
    "        \n",
    "        for inputs, _, indices in train_loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            indices = indices.to(device, non_blocking=True)\n",
    "            batch_noisy = noisy_labels_tensor[indices]\n",
    "            batch_clean = clean_labels_tensor[indices]\n",
    "            \n",
    "            # Structure gradient\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss_struct = criterion(outputs, batch_noisy)\n",
    "            loss_struct.backward(retain_graph=True)\n",
    "            g_struct = parameters_to_vector([p.grad for p in model.parameters()]).clone()\n",
    "            \n",
    "            # Value gradient\n",
    "            if global_step % K == 0 or cached_g_value is None:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss_value = criterion(outputs, batch_clean)\n",
    "                loss_value.backward()\n",
    "                cached_g_value = parameters_to_vector([p.grad for p in model.parameters()]).clone()\n",
    "            \n",
    "            # Mix\n",
    "            g_struct_norm = g_struct / (g_struct.norm() + 1e-12)\n",
    "            g_value_norm = cached_g_value / (cached_g_value.norm() + 1e-12)\n",
    "            \n",
    "            cos_sim = (g_struct_norm @ g_value_norm).item()\n",
    "            epoch_cos.append(cos_sim)\n",
    "            \n",
    "            g_mix = (1 - lam) * g_struct_norm + lam * g_value_norm\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            idx = 0\n",
    "            for p in model.parameters():\n",
    "                numel = p.numel()\n",
    "                p.grad = g_mix[idx:idx+numel].view(p.shape).clone()\n",
    "                idx += numel\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "        \n",
    "        scheduler.step()\n",
    "        cos_history.append(np.mean(epoch_cos))\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            acc = evaluate(model, test_loader)\n",
    "            best_acc = max(best_acc, acc)\n",
    "            history['epoch'].append(epoch + 1)\n",
    "            history['test_acc'].append(acc)\n",
    "            history['test_error'].append(1 - acc)\n",
    "    \n",
    "    final_acc = evaluate(model, test_loader)\n",
    "    avg_cos = np.mean(cos_history)\n",
    "    \n",
    "    return final_acc, max(best_acc, final_acc), avg_cos, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== „Éá„Éº„ÇøÊ∫ñÂÇô =====\n",
    "trainset, testset = load_cifar10()\n",
    "clean_labels = np.array(trainset.targets)\n",
    "train_loader, test_loader = get_data_loaders(trainset, testset)\n",
    "\n",
    "print('Data prepared')\n",
    "\n",
    "# GPU warmup\n",
    "warmup_model = SimpleCNN().to(device)\n",
    "for _ in range(20):\n",
    "    _ = warmup_model(torch.randn(BATCH_SIZE, 3, 32, 32, device=device))\n",
    "del warmup_model\n",
    "torch.cuda.empty_cache()\n",
    "print('GPU warmed up')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== „É°„Ç§„É≥ÂÆüÈ®ì„É´„Éº„Éó =====\n",
    "results = []\n",
    "checkpoint_file = f'{SAVE_DIR}/{EXP_NAME}_checkpoint.json'\n",
    "completed = set()\n",
    "\n",
    "if os.path.exists(checkpoint_file):\n",
    "    with open(checkpoint_file, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    for r in results:\n",
    "        completed.add((r['model'], r['lambda'], r['seed']))\n",
    "    print(f'Checkpoint loaded: {len(results)} runs completed')\n",
    "\n",
    "total = len(experiments)\n",
    "\n",
    "for exp in experiments:\n",
    "    model_name, lam, seed = exp['model'], exp['lambda'], exp['seed']\n",
    "    \n",
    "    if (model_name, lam, seed) in completed:\n",
    "        continue\n",
    "    \n",
    "    run_num = len(completed) + 1\n",
    "    print(f'\\n[{run_num}/{total}] {model_name} Œª={lam} seed={seed}')\n",
    "    \n",
    "    set_seed(seed)\n",
    "    noisy_labels = inject_label_noise(clean_labels, NOISE_RATE, seed)\n",
    "    \n",
    "    model = get_model(model_name).to(device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    final_acc, best_acc, avg_cos, history = train_dual_gradient(\n",
    "        model, train_loader, test_loader, clean_labels, noisy_labels, lam\n",
    "    )\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    result = {\n",
    "        'experiment_id': f'P2-{run_num:03d}',\n",
    "        'experiment': EXP_NAME,\n",
    "        'model': model_name,\n",
    "        'lambda': lam,\n",
    "        'noise_rate': NOISE_RATE,\n",
    "        'seed': seed,\n",
    "        'lr': LR,\n",
    "        'test_acc': final_acc,\n",
    "        'test_error': 1 - final_acc,\n",
    "        'best_acc': best_acc,\n",
    "        'best_error': 1 - best_acc,\n",
    "        'avg_cos_struct_value': avg_cos,\n",
    "        'time_seconds': elapsed,\n",
    "        'history': history\n",
    "    }\n",
    "    results.append(result)\n",
    "    completed.add((model_name, lam, seed))\n",
    "    \n",
    "    status = '‚úÖ EXCELLENT' if best_acc > 0.85 else ('‚ö†Ô∏è COLLAPSE' if best_acc < 0.5 else '')\n",
    "    print(f'  Error: {1-best_acc:.4f} | cos: {avg_cos:.4f} | Time: {elapsed/60:.1f} min {status}')\n",
    "    \n",
    "    with open(checkpoint_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    remaining = total - run_num\n",
    "    eta = remaining * elapsed / 3600\n",
    "    print(f'  Progress: {run_num}/{total} | ETA: {eta:.1f} hours')\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('ALL EXPERIMENTS COMPLETED!')\n",
    "print('='*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ÁµêÊûú‰øùÂ≠ò =====\n",
    "import pandas as pd\n",
    "\n",
    "with open(f'{SAVE_DIR}/{EXP_NAME}_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "results_flat = [{k: v for k, v in r.items() if k != 'history'} for r in results]\n",
    "df = pd.DataFrame(results_flat)\n",
    "df.to_csv(f'{SAVE_DIR}/{EXP_NAME}_results.csv', index=False)\n",
    "\n",
    "print(f'Results saved to {SAVE_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ÁµêÊûúÊØîËºÉ =====\n",
    "print('='*70)\n",
    "print('P2 RESULTS: VGG11 & SimpleCNN with lr=0.01')\n",
    "print('='*70)\n",
    "\n",
    "for model_name in MODELS:\n",
    "    print(f'\\n=== {model_name} ===')\n",
    "    print(f'{\"Œª\":>6} | {\"error\":>8} | {\"std\":>6} | {\"cos\":>8} | state')\n",
    "    print('-' * 55)\n",
    "    \n",
    "    model_df = df[df['model'] == model_name]\n",
    "    for lam in LAMBDAS:\n",
    "        subset = model_df[model_df['lambda'] == lam]\n",
    "        if len(subset) == 0:\n",
    "            continue\n",
    "        mean_err = subset['best_error'].mean()\n",
    "        std_err = subset['best_error'].std()\n",
    "        mean_cos = subset['avg_cos_struct_value'].mean()\n",
    "        \n",
    "        if mean_err < 0.15:\n",
    "            state = '‚úÖ Ordered'\n",
    "        elif mean_err < 0.30:\n",
    "            state = '‚ö° Critical'\n",
    "        else:\n",
    "            state = '‚ùå Collapse'\n",
    "        \n",
    "        print(f'{lam:>6.2f} | {mean_err:>7.1%} | {std_err:>5.1%} | {mean_cos:>+8.3f} | {state}')\n",
    "\n",
    "print('\\n--- Comparison with P (lr=0.1) ---')\n",
    "print('P (lr=0.1): VGG11 collapsed at Œª=0.45, SimpleCNN never reached Ordered')\n",
    "print('P2 (lr=0.01): Check if wider Ordered region exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ÂèØË¶ñÂåñ =====\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for i, model_name in enumerate(MODELS):\n",
    "    ax = axes[i]\n",
    "    model_df = df[df['model'] == model_name]\n",
    "    stats = model_df.groupby('lambda')['best_error'].agg(['mean', 'std'])\n",
    "    \n",
    "    ax.errorbar(stats.index, stats['mean'], yerr=stats['std'],\n",
    "                marker='o', capsize=4, linewidth=2, markersize=8)\n",
    "    ax.axhline(y=0.15, color='green', linestyle='--', alpha=0.5, label='Ordered threshold')\n",
    "    ax.axhline(y=0.30, color='red', linestyle='--', alpha=0.5, label='Collapse threshold')\n",
    "    ax.set_xlabel('Œª', fontsize=12)\n",
    "    ax.set_ylabel('Test Error', fontsize=12)\n",
    "    ax.set_title(f'{model_name} (lr=0.01)', fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR}/figures/{EXP_NAME}_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Figure saved!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
