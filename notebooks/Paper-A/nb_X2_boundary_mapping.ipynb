{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# X2: High-Resolution Boundary Mapping (NLP)\n\n**Paper-A (P0009) \u2014 X Series: Cross-Domain Generality**\n\n| Item | Value |\n|------|-------|\n| Dataset | SST-2 (binary sentiment) |\n| Model | DistilBERT (pretrained) |\n| Noise | Symmetric, **\u03b7 = 0.8** |\n| Trusted ratio | 5% |\n| \u03bb grid | **{0.26, 0.28, 0.30, 0.32, 0.34, 0.36, 0.38, 0.40, 0.42, 0.44}** |\n| Seeds | 0\u201319 (n=20 per \u03bb) |\n| Total runs | **200** |\n| Epochs | 3 |\n| Estimated time | ~2.5 hours (A100) |\n\n**Purpose**: Map the X1b cliff (\u03bb=0.30\u21920.40) at fine resolution.  \nIdentify variance peak and exact boundary location.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install -q transformers datasets accelerate\n\nimport os\nimport json\nimport time\nimport copy\nimport random\nimport warnings\nimport logging\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset, Subset\n\n# Suppress noisy warnings from transformers model loading\nlogging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\nlogging.getLogger(\"transformers.configuration_utils\").setLevel(logging.ERROR)\nwarnings.filterwarnings(\"ignore\", message=\".*does not exist in your Colab secrets.*\")\nwarnings.filterwarnings(\"ignore\", message=\".*unauthenticated requests.*\")\nos.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\n\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\nfrom datasets import load_dataset\n\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "CONFIG = {\n    \"experiment\": \"exp_X2_boundary_mapping\",\n    \"series\": \"X\",\n    \"dataset\": \"SST-2\",\n    \"num_classes\": 2,\n    \"model_name\": \"distilbert-base-uncased\",\n    \"noise_type\": \"symmetric\",\n    \"noise_rate\": 0.8,\n    \"trusted_ratio\": 0.05,\n    \"lambda_values\": [0.26, 0.28, 0.30, 0.32, 0.34, 0.36, 0.38, 0.40, 0.42, 0.44],\n    \"seeds\": list(range(20)),\n    \"epochs\": 3,\n    \"batch_size\": 64,\n    \"learning_rate\": 2e-5,\n    \"weight_decay\": 0.01,\n    \"warmup_ratio\": 0.1,\n    \"max_seq_length\": 128,\n    \"use_bf16\": True,\n    \"max_train_samples\": 16000,\n    \"csv_measure_every_n_steps\": 25,\n    \"output_dir\": \"X2_results\",\n}\n\nos.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n\nn_used = min(int(67349 * 0.95), CONFIG[\"max_train_samples\"])\nsteps_per_epoch = n_used // CONFIG[\"batch_size\"]\ntotal_steps = steps_per_epoch * CONFIG[\"epochs\"]\nn_runs = len(CONFIG[\"lambda_values\"]) * len(CONFIG[\"seeds\"])\nest_per_run = total_steps * 25 / 1000 + 15\n\nprint(f\"Total runs: {n_runs}\")\nprint(f\"\u03bb values: {CONFIG['lambda_values']}\")\nprint(f\"Steps/epoch: ~{steps_per_epoch}, Total steps/run: ~{total_steps}\")\nprint(f\"Est. time/run: ~{est_per_run:.0f}s\")\nprint(f\"Est. total: ~{n_runs * est_per_run / 3600:.1f}h\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Google Drive Mount & Output Directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n# Google Drive Mount\n# ============================================================\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# --- Save location on Drive ---\n# Change this path if needed\nDRIVE_BASE = \"/content/drive/MyDrive/Paper-A_X_series\"\nDRIVE_OUTPUT = f\"{DRIVE_BASE}/exp_X2_boundary_mapping\"\n\nos.makedirs(DRIVE_OUTPUT, exist_ok=True)\nprint(f\"Drive output directory: {DRIVE_OUTPUT}\")\n\n# Also keep local copy for speed\nLOCAL_OUTPUT = CONFIG[\"output_dir\"]  # \"X1_results\"\nos.makedirs(LOCAL_OUTPUT, exist_ok=True)\n\ndef save_to_drive(filename, data):\n    \"\"\"Save JSON to both local and Drive simultaneously.\"\"\"\n    # Local (fast)\n    local_path = os.path.join(LOCAL_OUTPUT, filename)\n    with open(local_path, \"w\") as f:\n        json.dump(data, f, indent=2)\n    # Drive (persistent)\n    drive_path = os.path.join(DRIVE_OUTPUT, filename)\n    with open(drive_path, \"w\") as f:\n        json.dump(data, f, indent=2)\n    return drive_path\n\ndef save_figure_to_drive(fig, filename, dpi=300):\n    \"\"\"Save figure to both local and Drive.\"\"\"\n    local_path = os.path.join(LOCAL_OUTPUT, filename)\n    fig.savefig(local_path, dpi=dpi, bbox_inches=\"tight\", facecolor=\"white\")\n    drive_path = os.path.join(DRIVE_OUTPUT, filename)\n    fig.savefig(drive_path, dpi=dpi, bbox_inches=\"tight\", facecolor=\"white\")\n    return drive_path\n\nprint(\"Save functions defined (local + Drive dual-write).\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Deterministic Seeding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def set_seed(seed):\n    \"\"\"Set all random seeds for full reproducibility.\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n# Load SST-2 and tokenize\n# ============================================================\n\ntokenizer = DistilBertTokenizer.from_pretrained(CONFIG[\"model_name\"])\n\nprint(\"Loading SST-2 dataset...\")\nraw_dataset = load_dataset(\"glue\", \"sst2\")\n\n# Use validation set as test (SST-2 test has no labels)\ntrain_texts = list(raw_dataset[\"train\"][\"sentence\"])\ntrain_labels = list(raw_dataset[\"train\"][\"label\"])\ntest_texts = list(raw_dataset[\"validation\"][\"sentence\"])\ntest_labels = list(raw_dataset[\"validation\"][\"label\"])\n\nprint(f\"Train: {len(train_texts)} samples\")\nprint(f\"Test:  {len(test_texts)} samples\")\nprint(f\"Label distribution (train): {np.bincount(train_labels)}\")\n\n# Tokenize\nprint(\"Tokenizing...\")\n\ndef tokenize_batch(texts, max_length):\n    return tokenizer(\n        texts,\n        padding=\"max_length\",\n        truncation=True,\n        max_length=max_length,\n        return_tensors=\"pt\"\n    )\n\ntrain_encodings = tokenize_batch(train_texts, CONFIG[\"max_seq_length\"])\ntest_encodings = tokenize_batch(test_texts, CONFIG[\"max_seq_length\"])\n\ntrain_labels_tensor = torch.tensor(train_labels)\ntest_labels_tensor = torch.tensor(test_labels)\n\nprint(\"Tokenization complete.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n# Custom Dataset\n# ============================================================\n\nclass TextDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        item = {\n            \"input_ids\": self.encodings[\"input_ids\"][idx],\n            \"attention_mask\": self.encodings[\"attention_mask\"][idx],\n            \"labels\": self.labels[idx]\n        }\n        return item\n\ntest_dataset = TextDataset(test_encodings, test_labels_tensor)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\nprint(f\"Test dataset: {len(test_dataset)} samples\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Noise Injection & Trusted Subset Separation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def prepare_noisy_and_trusted(train_labels, noise_rate, trusted_ratio,\n                              num_classes, seed, max_noisy_samples=None):\n    \"\"\"\n    1) Separate trusted subset (clean) BEFORE noise injection\n    2) Inject symmetric noise into remaining data\n    3) Optionally subsample the noisy pool for speed\n    Returns: noisy_labels, trusted_indices, noisy_indices\n    \"\"\"\n    rng = np.random.RandomState(seed + 10000)\n    n = len(train_labels)\n    labels = np.array(train_labels)\n    indices = np.arange(n)\n\n    # --- Step 1: Stratified trusted subset ---\n    trusted_indices = []\n    for c in range(num_classes):\n        class_idx = indices[labels == c]\n        n_class_trusted = max(1, int(len(class_idx) * trusted_ratio))\n        selected = rng.choice(class_idx, size=n_class_trusted, replace=False)\n        trusted_indices.extend(selected)\n    trusted_indices = np.array(sorted(trusted_indices))\n\n    # Remaining = noisy pool\n    noisy_mask = np.ones(n, dtype=bool)\n    noisy_mask[trusted_indices] = False\n    noisy_indices = indices[noisy_mask]\n\n    # --- Step 2: Inject symmetric noise ---\n    noisy_labels = labels.copy()\n    n_noisy = len(noisy_indices)\n    n_flip = int(n_noisy * noise_rate)\n    flip_idx = rng.choice(noisy_indices, size=n_flip, replace=False)\n    for idx in flip_idx:\n        original = noisy_labels[idx]\n        candidates = [c for c in range(num_classes) if c != original]\n        noisy_labels[idx] = rng.choice(candidates)\n\n    # --- Step 3: Subsample noisy pool (if requested) ---\n    if max_noisy_samples and len(noisy_indices) > max_noisy_samples:\n        noisy_indices = rng.choice(noisy_indices, size=max_noisy_samples, replace=False)\n        noisy_indices = np.sort(noisy_indices)\n\n    actual_noise = np.mean(noisy_labels[noisy_indices] != labels[noisy_indices])\n    print(f\"  Trusted: {len(trusted_indices)} ({len(trusted_indices)/n*100:.1f}%) | \"\n          f\"Noisy: {len(noisy_indices)} (\u03b7_actual={actual_noise:.3f})\")\n\n    return torch.tensor(noisy_labels), trusted_indices, noisy_indices\n\n# Quick test\nprint(\"Testing noise injection + subsampling (seed=0)...\")\n_, ti, ni = prepare_noisy_and_trusted(train_labels, 0.4, 0.05, 2, seed=0,\n                                       max_noisy_samples=16000)\nprint(f\"  \u2192 {len(ti)} trusted + {len(ni)} noisy\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Dual-Gradient Training Core"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_head_param_names(model):\n    \"\"\"Return parameter names of the classification head.\"\"\"\n    return [n for n, _ in model.named_parameters()\n            if \"classifier\" in n or \"pre_classifier\" in n]\n\n\ndef extract_grads(model):\n    \"\"\"Extract current gradients as a flat dict.\"\"\"\n    return {n: p.grad.clone() for n, p in model.named_parameters()\n            if p.grad is not None}\n\n\ndef normalize_grad_dict(grads):\n    \"\"\"L2-normalize gradient vector (across all params).\"\"\"\n    flat = torch.cat([g.flatten() for g in grads.values()])\n    norm = flat.norm()\n    if norm > 0:\n        return {n: g / norm for n, g in grads.items()}, norm.item()\n    return grads, 0.0\n\n\ndef cosine_sim(grads_a, grads_b, param_names=None):\n    \"\"\"Cosine similarity between two gradient dicts.\"\"\"\n    if param_names:\n        keys = [n for n in param_names if n in grads_a and n in grads_b]\n    else:\n        keys = sorted(set(grads_a) & set(grads_b))\n    if not keys:\n        return 0.0\n    va = torch.cat([grads_a[n].flatten() for n in keys])\n    vb = torch.cat([grads_b[n].flatten() for n in keys])\n    return F.cosine_similarity(va.unsqueeze(0), vb.unsqueeze(0)).item()\n\n\ndef set_mixed_grad(model, g_struct_norm, g_value_norm, lam):\n    \"\"\"Set param.grad = (1-\u03bb)\u011d_struct + \u03bb\u011d_value for optimizer.step().\"\"\"\n    for n, p in model.named_parameters():\n        if n in g_struct_norm and n in g_value_norm:\n            p.grad = (1 - lam) * g_struct_norm[n] + lam * g_value_norm[n]\n        elif n in g_struct_norm:\n            p.grad = (1 - lam) * g_struct_norm[n]\n        elif n in g_value_norm:\n            p.grad = lam * g_value_norm[n]\n\nprint(\"Dual-gradient core functions defined.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "@torch.no_grad()\ndef evaluate(model, test_loader, device):\n    \"\"\"Evaluate model on test set.\"\"\"\n    model.eval()\n    correct = 0\n    total = 0\n    total_loss = 0.0\n    n_batches = 0\n\n    for batch in test_loader:\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        preds = outputs.logits.argmax(dim=-1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n        total_loss += outputs.loss.item()\n        n_batches += 1\n\n    acc = correct / total\n    error = 1.0 - acc\n    avg_loss = total_loss / n_batches\n    model.train()\n    return acc, error, avg_loss\n\nprint(\"Evaluation function defined.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Single Run Execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def run_single_experiment(lam, seed, config):\n    \"\"\"\n    Execute one dual-gradient training run.\n    - bf16 mixed precision for speed\n    - Batch-level updates with AdamW + warmup scheduler\n    - Subsampled noisy pool for feasible runtime\n    \"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    set_seed(seed)\n    t_start = time.time()\n\n    # --- Prepare data ---\n    noisy_labels, trusted_idx, noisy_idx = prepare_noisy_and_trusted(\n        train_labels, config[\"noise_rate\"], config[\"trusted_ratio\"],\n        config[\"num_classes\"], seed,\n        max_noisy_samples=config.get(\"max_train_samples\")\n    )\n\n    noisy_dataset = TextDataset(\n        {k: v[noisy_idx] for k, v in train_encodings.items()},\n        noisy_labels[noisy_idx]\n    )\n    trusted_dataset = TextDataset(\n        {k: v[trusted_idx] for k, v in train_encodings.items()},\n        noisy_labels[trusted_idx]\n    )\n\n    struct_loader = DataLoader(noisy_dataset, batch_size=config[\"batch_size\"],\n                               shuffle=True, drop_last=True)\n    value_loader = DataLoader(trusted_dataset,\n                              batch_size=min(config[\"batch_size\"], len(trusted_dataset)),\n                              shuffle=True, drop_last=False)\n\n    # --- Initialize model ---\n    set_seed(seed)\n    model = DistilBertForSequenceClassification.from_pretrained(\n        config[\"model_name\"], num_labels=config[\"num_classes\"]\n    ).to(device)\n    model.train()\n\n    head_params = get_head_param_names(model)\n\n    # --- Optimizer + Scheduler ---\n    optimizer = torch.optim.AdamW(\n        model.parameters(),\n        lr=config[\"learning_rate\"],\n        weight_decay=config[\"weight_decay\"]\n    )\n\n    steps_per_epoch = len(struct_loader)\n    total_steps = steps_per_epoch * config[\"epochs\"]\n    warmup_steps = int(total_steps * config[\"warmup_ratio\"])\n\n    def lr_lambda(step):\n        if step < warmup_steps:\n            return step / max(1, warmup_steps)\n        return max(0.0, (total_steps - step) / max(1, total_steps - warmup_steps))\n\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n\n    # --- Mixed precision ---\n    use_amp = config.get(\"use_bf16\", False) and device.type == \"cuda\"\n    amp_dtype = torch.bfloat16 if use_amp else torch.float32\n\n    # --- Training loop ---\n    epoch_logs = []\n    all_csv = []\n    measure_every = config.get(\"csv_measure_every_n_steps\", 25)\n    global_step = 0\n\n    for epoch in range(config[\"epochs\"]):\n        epoch_loss_s = 0.0\n        epoch_loss_v = 0.0\n        epoch_csv = []\n        n_steps = 0\n        value_iter = iter(value_loader)\n\n        for struct_batch in struct_loader:\n            # === Struct gradient ===\n            optimizer.zero_grad()\n            s_ids = struct_batch[\"input_ids\"].to(device)\n            s_mask = struct_batch[\"attention_mask\"].to(device)\n            s_labels = struct_batch[\"labels\"].to(device)\n\n            with torch.amp.autocast(\"cuda\", dtype=amp_dtype, enabled=use_amp):\n                out_s = model(input_ids=s_ids, attention_mask=s_mask, labels=s_labels)\n            out_s.loss.backward()\n            g_struct = extract_grads(model)\n            loss_s = out_s.loss.item()\n\n            # === Value gradient ===\n            optimizer.zero_grad()\n            try:\n                value_batch = next(value_iter)\n            except StopIteration:\n                value_iter = iter(value_loader)\n                value_batch = next(value_iter)\n\n            v_ids = value_batch[\"input_ids\"].to(device)\n            v_mask = value_batch[\"attention_mask\"].to(device)\n            v_labels = value_batch[\"labels\"].to(device)\n\n            with torch.amp.autocast(\"cuda\", dtype=amp_dtype, enabled=use_amp):\n                out_v = model(input_ids=v_ids, attention_mask=v_mask, labels=v_labels)\n            out_v.loss.backward()\n            g_value = extract_grads(model)\n            loss_v = out_v.loss.item()\n\n            # === Normalize & Mix ===\n            g_struct_n, _ = normalize_grad_dict(g_struct)\n            g_value_n, _ = normalize_grad_dict(g_value)\n            set_mixed_grad(model, g_struct_n, g_value_n, lam)\n\n            # === Step ===\n            optimizer.step()\n            scheduler.step()\n            global_step += 1\n            n_steps += 1\n\n            epoch_loss_s += loss_s\n            epoch_loss_v += loss_v\n\n            # === c_sv (periodic) ===\n            if global_step % measure_every == 0:\n                c = cosine_sim(g_struct, g_value, head_params)\n                epoch_csv.append(c)\n                all_csv.append(c)\n\n        # --- Evaluate ---\n        test_acc, test_error, test_loss = evaluate(model, test_loader, device)\n        mean_csv = float(np.mean(epoch_csv)) if epoch_csv else 0.0\n\n        epoch_log = {\n            \"epoch\": epoch + 1,\n            \"steps\": n_steps,\n            \"test_acc\": round(test_acc, 4),\n            \"test_error\": round(test_error, 4),\n            \"test_loss\": round(test_loss, 4),\n            \"loss_struct\": round(epoch_loss_s / max(n_steps, 1), 4),\n            \"loss_value\": round(epoch_loss_v / max(n_steps, 1), 4),\n            \"c_sv_head\": round(mean_csv, 4),\n        }\n        epoch_logs.append(epoch_log)\n\n        print(f\"    Ep {epoch+1}/{config['epochs']} ({n_steps} steps): \"\n              f\"err={test_error:.4f}  acc={test_acc:.4f}  \"\n              f\"c_sv={mean_csv:.4f}\")\n\n    # --- Final ---\n    final_acc, final_error, _ = evaluate(model, test_loader, device)\n    elapsed = time.time() - t_start\n    mean_csv_all = float(np.mean(all_csv)) if all_csv else 0.0\n\n    result = {\n        \"experiment_id\": f\"X2-{seed:03d}-lam{lam:.2f}\",\n        \"experiment\": config[\"experiment\"],\n        \"dataset\": config[\"dataset\"],\n        \"model\": config[\"model_name\"],\n        \"noise_type\": config[\"noise_type\"],\n        \"noise_rate\": config[\"noise_rate\"],\n        \"trusted_ratio\": config[\"trusted_ratio\"],\n        \"lambda\": lam,\n        \"seed\": seed,\n        \"test_acc\": round(final_acc, 4),\n        \"test_error\": round(final_error, 4),\n        \"best_acc\": round(max(log[\"test_acc\"] for log in epoch_logs), 4),\n        \"best_error\": round(min(log[\"test_error\"] for log in epoch_logs), 4),\n        \"avg_cos_struct_value\": round(mean_csv_all, 4),\n        \"total_steps\": global_step,\n        \"epochs\": config[\"epochs\"],\n        \"epoch_logs\": epoch_logs,\n        \"time_seconds\": round(elapsed, 1),\n    }\n\n    del model, optimizer, scheduler\n    torch.cuda.empty_cache()\n    return result\n\nprint(\"Single run function defined (bf16 + subsampled + batch-level).\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Main Execution Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n# Main Loop: All \u03bb \u00d7 All Seeds\n# ============================================================\n\nall_results = []\ntotal_runs = len(CONFIG[\"lambda_values\"]) * len(CONFIG[\"seeds\"])\nrun_idx = 0\n\nprint(\"=\" * 70)\nprint(f\"X1: Coarse \u03bb Sweep \u2014 {total_runs} runs\")\nprint(f\"  \u03bb values: {CONFIG['lambda_values']}\")\nprint(f\"  Seeds: {CONFIG['seeds']}\")\nprint(f\"  \u03b7 = {CONFIG['noise_rate']}, trusted = {CONFIG['trusted_ratio']}\")\nprint(\"=\" * 70)\n\nt_total_start = time.time()\n\nfor lam in CONFIG[\"lambda_values\"]:\n    print(f\"\\n{'='*60}\")\n    print(f\"  \u03bb = {lam:.2f}\")\n    print(f\"{'='*60}\")\n\n    lambda_results = []\n\n    for seed in CONFIG[\"seeds\"]:\n        run_idx += 1\n        print(f\"\\n  --- Run {run_idx}/{total_runs}: \u03bb={lam:.2f}, seed={seed} ---\")\n\n        result = run_single_experiment(lam, seed, CONFIG)\n        lambda_results.append(result)\n        all_results.append(result)\n\n        # Progress\n        elapsed_total = time.time() - t_total_start\n        avg_per_run = elapsed_total / run_idx\n        remaining = avg_per_run * (total_runs - run_idx)\n        print(f\"    \u2192 err={result['test_error']:.4f}  c_sv={result['avg_cos_struct_value']:.4f}  \"\n              f\"time={result['time_seconds']:.0f}s  \"\n              f\"[{run_idx}/{total_runs}, ETA: {remaining/60:.0f} min]\")\n\n    # Summary for this \u03bb\n    errors = [r[\"test_error\"] for r in lambda_results]\n    cos_values = [r[\"avg_cos_struct_value\"] for r in lambda_results]\n    print(f\"\\n  \u03bb={lam:.2f} SUMMARY:\")\n    print(f\"    error: mean={np.mean(errors):.4f} \u00b1 {np.std(errors):.4f}\")\n    print(f\"    error: min={np.min(errors):.4f}  max={np.max(errors):.4f}\")\n    print(f\"    c_sv:  mean={np.mean(cos_values):.4f} \u00b1 {np.std(cos_values):.4f}\")\n\n    # Save intermediate results for this \u03bb (local + Drive)\n    fname = f\"X2_lambda{lam:.2f}_results.json\"\n    drive_path = save_to_drive(fname, lambda_results)\n    print(f\"    Saved: {fname} \u2192 Drive \u2713\")\n\n# ============================================================\n# Save all results\n# ============================================================\nt_total = time.time() - t_total_start\n\nsave_to_drive(\"X2_results.json\", all_results)\nsave_to_drive(\"X2_config.json\", CONFIG)\n\nprint(f\"\\n{'='*70}\")\nprint(f\"COMPLETE: {total_runs} runs in {t_total/60:.1f} minutes ({t_total/3600:.1f} hours)\")\nprint(f\"Results saved to Drive: {DRIVE_OUTPUT}\")\nprint(f\"{'='*70}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Analysis & Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n\n# Load results (prefer local, fallback to Drive)\nlocal_path = os.path.join(CONFIG[\"output_dir\"], \"X2_results.json\")\ndrive_path = os.path.join(DRIVE_OUTPUT, \"X2_results.json\")\n\nif os.path.exists(local_path):\n    with open(local_path) as f:\n        results = json.load(f)\n    print(f\"Loaded from local: {local_path}\")\nelif os.path.exists(drive_path):\n    with open(drive_path) as f:\n        results = json.load(f)\n    print(f\"Loaded from Drive: {drive_path}\")\nelse:\n    raise FileNotFoundError(\"No results found! Run main loop first.\")\n\n# Organize by \u03bb\nlambda_data = {}\nfor r in results:\n    lam = r[\"lambda\"]\n    if lam not in lambda_data:\n        lambda_data[lam] = {\"errors\": [], \"cos_values\": [], \"accs\": []}\n    lambda_data[lam][\"errors\"].append(r[\"test_error\"])\n    lambda_data[lam][\"cos_values\"].append(r[\"avg_cos_struct_value\"])\n    lambda_data[lam][\"accs\"].append(r[\"test_acc\"])\n\nlambdas = sorted(lambda_data.keys())\n\n# ============================================================\n# Figure 1: Phase Structure Overview (3 panels)\n# ============================================================\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\n# Panel (a): Error distribution by \u03bb\nax = axes[0]\npositions = range(len(lambdas))\nbp = ax.boxplot(\n    [lambda_data[l][\"errors\"] for l in lambdas],\n    positions=positions, widths=0.6, patch_artist=True\n)\nfor patch in bp[\"boxes\"]:\n    patch.set_facecolor(\"#1F77B4\")\n    patch.set_alpha(0.5)\n\n# Overlay individual points\nfor i, lam in enumerate(lambdas):\n    jitter = np.random.normal(0, 0.05, len(lambda_data[lam][\"errors\"]))\n    ax.scatter(\n        [i + j for j in jitter],\n        lambda_data[lam][\"errors\"],\n        color=\"#D62728\", alpha=0.6, s=20, zorder=3\n    )\n\nax.set_xticks(positions)\nax.set_xticklabels([f\"{l:.2f}\" for l in lambdas])\nax.set_xlabel(\"\u03bb\", fontsize=14)\nax.set_ylabel(\"Test Error\", fontsize=14)\nax.set_title(\"(a) Test Error Distribution by \u03bb\", fontsize=13, fontweight=\"bold\")\nax.grid(True, alpha=0.3)\n\n# Panel (b): Variance by \u03bb\nax = axes[1]\nvariances = [np.var(lambda_data[l][\"errors\"]) for l in lambdas]\nax.bar(positions, variances, color=\"#FF7F0E\", edgecolor=\"black\")\nfor i, v in enumerate(variances):\n    ax.text(i, v + max(variances)*0.02, f\"{v:.4f}\", ha=\"center\", fontsize=9)\nax.set_xticks(positions)\nax.set_xticklabels([f\"{l:.2f}\" for l in lambdas])\nax.set_xlabel(\"\u03bb\", fontsize=14)\nax.set_ylabel(\"Variance of Test Error\", fontsize=14)\nax.set_title(\"(b) Run-to-Run Variance (peak = critical region)\", fontsize=13, fontweight=\"bold\")\nax.grid(True, alpha=0.3)\n\n# Panel (c): c_sv distribution by \u03bb\nax = axes[2]\nbp2 = ax.boxplot(\n    [lambda_data[l][\"cos_values\"] for l in lambdas],\n    positions=positions, widths=0.6, patch_artist=True\n)\nfor patch in bp2[\"boxes\"]:\n    patch.set_facecolor(\"#2CA02C\")\n    patch.set_alpha(0.5)\n\nfor i, lam in enumerate(lambdas):\n    jitter = np.random.normal(0, 0.05, len(lambda_data[lam][\"cos_values\"]))\n    ax.scatter(\n        [i + j for j in jitter],\n        lambda_data[lam][\"cos_values\"],\n        color=\"#9467BD\", alpha=0.6, s=20, zorder=3\n    )\n\nax.set_xticks(positions)\nax.set_xticklabels([f\"{l:.2f}\" for l in lambdas])\nax.set_xlabel(\"\u03bb\", fontsize=14)\nax.set_ylabel(\"c_sv (cos similarity, head only)\", fontsize=14)\nax.set_title(\"(c) Gradient Geometry by \u03bb\", fontsize=13, fontweight=\"bold\")\nax.grid(True, alpha=0.3)\n\nfig.suptitle(\"X1: Phase Structure in NLP (SST-2, DistilBERT, \u03b7=0.4)\",\n             fontsize=14, fontweight=\"bold\", y=1.02)\nplt.tight_layout()\nsave_figure_to_drive(fig, \"X2_boundary_mapping.png\")\nplt.show()\nprint(f\"Saved: X2_boundary_mapping.png \u2192 Drive \u2713\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Boundary Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from scipy import stats\n\nprint(\"=\" * 60)\nprint(\"X2: Boundary Analysis\")\nprint(\"=\" * 60)\n\n# Compute derivative (slope between adjacent \u03bb)\nlambdas_sorted = sorted(lambda_data.keys())\nslopes = []\nfor i in range(len(lambdas_sorted)-1):\n    l1, l2 = lambdas_sorted[i], lambdas_sorted[i+1]\n    m1 = np.mean(lambda_data[l1])\n    m2 = np.mean(lambda_data[l2])\n    slope = (m2 - m1) / (l2 - l1)\n    slopes.append((l1, l2, slope))\n    print(f\"  \u03bb={l1:.2f}\u2192{l2:.2f}: slope = {slope:.2f} (\u0394err = {m2-m1:.4f})\")\n\n# Find steepest\nsteepest = min(slopes, key=lambda x: x[2])\nprint(f\"\\nSteepest transition: \u03bb={steepest[0]:.2f}\u2192{steepest[1]:.2f} (slope={steepest[2]:.2f})\")\n\n# Variance analysis\nprint(\"\\nVariance by \u03bb:\")\nvariances = {}\nfor lam in lambdas_sorted:\n    v = np.var(lambda_data[lam])\n    variances[lam] = v\n    print(f\"  \u03bb={lam:.2f}: var={v:.6f} ({v*10000:.2f} \u00d710\u2074)\")\n\npeak_lam = max(variances, key=variances.get)\nprint(f\"\\nPeak variance at \u03bb={peak_lam:.2f}\")\n\n# Separation tests\nprint(\"\\nAdjacent \u03bb separation (t-test):\")\nfor i in range(len(lambdas_sorted)-1):\n    l1, l2 = lambdas_sorted[i], lambdas_sorted[i+1]\n    e1 = lambda_data[l1]\n    e2 = lambda_data[l2]\n    t, p = stats.ttest_ind(e1, e2)\n    overlap = \"NONE\" if min(e1) > max(e2) or min(e2) > max(e1) else \"partial\"\n    print(f\"  \u03bb={l1:.2f} vs {l2:.2f}: t={t:.2f}, p={p:.2e}, overlap={overlap}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Summary Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n# Final Summary Table\n# ============================================================\n\nprint(f\"\\n{'='*80}\")\nprint(f\"{'X2 RESULTS SUMMARY':^80}\")\nprint(f\"{'='*80}\")\nprint(f\"{'\u03bb':>6} | {'mean_err':>10} | {'std_err':>10} | {'min_err':>10} | {'max_err':>10} | {'variance':>10} | {'mean_c_sv':>10}\")\nprint(\"-\" * 80)\n\nfor lam in lambdas:\n    errors = lambda_data[lam][\"errors\"]\n    cos_vals = lambda_data[lam][\"cos_values\"]\n    print(f\"{lam:>6.2f} | {np.mean(errors):>10.4f} | {np.std(errors):>10.4f} | \"\n          f\"{np.min(errors):>10.4f} | {np.max(errors):>10.4f} | \"\n          f\"{np.var(errors):>10.6f} | {np.mean(cos_vals):>10.4f}\")\n\nprint(\"-\" * 80)\nprint(f\"Total runs: {len(results)}\")\nprint(f\"Total time: {sum(r['time_seconds'] for r in results)/60:.1f} min\")\n\n# Identify critical region\npeak_var_lambda = lambdas[np.argmax(variances)]\nprint(f\"\\nPeak variance at \u03bb = {peak_var_lambda:.2f}\")\nprint(f\"  \u2192 Suggested critical region for X2: \u03bb \u2208 [{peak_var_lambda-0.04:.2f}, {peak_var_lambda+0.06:.2f}]\")\nprint(f\"  \u2192 Suggested \u03bb for X4 (metastability): {peak_var_lambda+0.04:.2f}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Download Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Results are already saved to Google Drive\nprint(f\"All results saved to: {DRIVE_OUTPUT}\")\nprint(f\"Contents:\")\nfor f in sorted(os.listdir(DRIVE_OUTPUT)):\n    size = os.path.getsize(os.path.join(DRIVE_OUTPUT, f))\n    print(f\"  {f}  ({size/1024:.1f} KB)\")\n\n# Optional: also zip for local download\nimport shutil\nshutil.make_archive(\"X1_results\", \"zip\", LOCAL_OUTPUT)\nprint(f\"\\nLocal zip also available: X1_results.zip\")\ntry:\n    from google.colab import files\n    files.download(\"X1_results.zip\")\nexcept:\n    pass\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}